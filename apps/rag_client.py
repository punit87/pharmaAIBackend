#!/usr/bin/env python3
"""
RAG-Anything Server - Optimized for ECS Tasks
- Persistent event loop for efficient async operations
- Lazy initialization to reduce cold start time
- Proper resource cleanup
- Memory-efficient caching
"""
import os
import time
import json
import boto3
import asyncio
import threading
import atexit
import logging
from functools import lru_cache
from flask import Flask, request, jsonify
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/tmp/rag_client.log')
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Global state management
_event_loop = None
_rag_instance = None
_rag_lock = threading.Lock()

# ============================================================================
# EVENT LOOP MANAGEMENT (Persistent loop for all async operations)
# ============================================================================

def get_event_loop():
    """Get or create persistent event loop for async operations"""
    global _event_loop
    start_time = time.time()
    logger.info("ğŸ”„ [EVENT_LOOP] Getting event loop...")
    
    if _event_loop is None or _event_loop.is_closed():
        logger.info("ğŸ”„ [EVENT_LOOP] Creating new event loop...")
        _event_loop = asyncio.new_event_loop()
        # Run loop in background thread
        loop_thread = threading.Thread(target=_event_loop.run_forever, daemon=True)
        loop_thread.start()
        logger.info(f"ğŸ”„ [EVENT_LOOP] Event loop created and started in {time.time() - start_time:.3f}s")
    else:
        logger.info(f"ğŸ”„ [EVENT_LOOP] Using existing event loop in {time.time() - start_time:.3f}s")
    
    return _event_loop

def run_async(coro):
    """Execute async coroutine in persistent event loop"""
    start_time = time.time()
    logger.info("ğŸ”„ [ASYNC] Executing async coroutine...")
    
    loop = get_event_loop()
    future = asyncio.run_coroutine_threadsafe(coro, loop)
    result = future.result()
    
    logger.info(f"ğŸ”„ [ASYNC] Async coroutine completed in {time.time() - start_time:.3f}s")
    return result

def cleanup_event_loop():
    """Cleanup event loop on shutdown"""
    global _event_loop
    if _event_loop and not _event_loop.is_closed():
        _event_loop.call_soon_threadsafe(_event_loop.stop)
        _event_loop.close()

atexit.register(cleanup_event_loop)

# ============================================================================
# ACTIVITY TRACKING (Simplified)
# ============================================================================

def update_activity():
    """Update activity timestamp for monitoring"""
    pass  # Simplified - no auto-stop logic

# ============================================================================
# RAG CONFIGURATION (Cached for reuse)
# ============================================================================

@lru_cache(maxsize=1)
def get_api_config():
    """Cache API configuration"""
    return {
        'api_key': os.environ.get('OPENAI_API_KEY'),
        'base_url': os.environ.get('OPENAI_BASE_URL'),
    }

@lru_cache(maxsize=1)
def get_rag_config():
    """Cache RAG configuration optimized for large documents"""
    start_time = time.time()
    logger.info("âš™ï¸ [CONFIG] Getting RAG configuration...")
    
    config = RAGAnythingConfig(
        working_dir=os.environ.get('OUTPUT_DIR', '/rag-output/'),
        parser=os.environ.get('PARSER', 'docling'),
        parse_method=os.environ.get('PARSE_METHOD', 'ocr'),  # Enable OCR for better text extraction
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
        # Large document handling optimizations
        chunk_size=int(os.environ.get('CHUNK_SIZE', '1000')),  # Smaller chunks for token limit
        chunk_overlap=int(os.environ.get('CHUNK_OVERLAP', '100')),  # Overlap between chunks
        max_context_length=int(os.environ.get('MAX_CONTEXT_LENGTH', '4000')),  # Reduced context length
        # OCR-specific optimizations
        ocr_options={
            'engine': 'easy_ocr',
            'force_full_page_ocr': True,
            'language': 'en'
        },
        # Neo4j-specific optimizations
        # RAG-Anything will use the 1536-dimension embeddings for Neo4j vector indexes
        # This ensures compatibility with Neo4j's vector search capabilities
    )
    
    logger.info(f"âš™ï¸ [CONFIG] RAG configuration loaded in {time.time() - start_time:.3f}s")
    logger.info(f"âš™ï¸ [CONFIG] Working dir: {config.working_dir}")
    logger.info(f"âš™ï¸ [CONFIG] Parser: {config.parser}")
    logger.info(f"âš™ï¸ [CONFIG] Parse method: {config.parse_method}")
    logger.info(f"âš™ï¸ [CONFIG] OCR enabled: {config.parse_method == 'ocr'}")
    logger.info(f"âš™ï¸ [CONFIG] OCR options: {getattr(config, 'ocr_options', 'Not set')}")
    logger.info(f"âš™ï¸ [CONFIG] Chunk size: {config.chunk_size}")
    logger.info(f"âš™ï¸ [CONFIG] Chunk overlap: {config.chunk_overlap}")
    logger.info(f"âš™ï¸ [CONFIG] Max context length: {config.max_context_length}")
    
    return config

def get_llm_model_func():
    """Create LLM model function with cached config"""
    start_time = time.time()
    logger.info("ğŸ¤– [LLM] Creating LLM model function...")
    
    config = get_api_config()
    
    def llm_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        llm_start_time = time.time()
        logger.info(f"ğŸ¤– [LLM] Starting LLM completion...")
        logger.info(f"ğŸ¤– [LLM] Prompt length: {len(prompt)} characters")
        logger.info(f"ğŸ¤– [LLM] System prompt length: {len(system_prompt) if system_prompt else 0} characters")
        logger.info(f"ğŸ¤– [LLM] History messages: {len(history_messages)} messages")
        
        # Handle token limits for large prompts
        max_tokens = int(os.environ.get('MAX_TOKENS', '4000'))
        logger.info(f"ğŸ¤– [LLM] Max tokens: {max_tokens}")
        
        result = openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=config['api_key'],
            base_url=config['base_url'],
            max_tokens=max_tokens,
            **kwargs,
        )
        
        logger.info(f"ğŸ¤– [LLM] LLM completion completed in {time.time() - llm_start_time:.3f}s")
        logger.info(f"ğŸ¤– [LLM] Response length: {len(result) if result else 0} characters")
        
        return result
    
    logger.info(f"ğŸ¤– [LLM] LLM model function created in {time.time() - start_time:.3f}s")
    return llm_func

def get_vision_model_func(llm_func):
    """Create vision model function with cached config"""
    config = get_api_config()
    
    def vision_func(prompt, system_prompt=None, history_messages=[], 
                   image_data=None, messages=None, **kwargs):
        # Multimodal VLM enhanced query format
        if messages:
            return openai_complete_if_cache(
                "gpt-4o", "", system_prompt=None, history_messages=[],
                messages=messages, api_key=config['api_key'], 
                base_url=config['base_url'], **kwargs
            )
        # Single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o", "", system_prompt=None, history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
                            }
                        ],
                    } if image_data else {"role": "user", "content": prompt}
                ],
                api_key=config['api_key'], base_url=config['base_url'], **kwargs
            )
        # Pure text fallback
        else:
            return llm_func(prompt, system_prompt, history_messages, **kwargs)
    
    return vision_func

def get_embedding_func():
    """
    Create embedding function optimized for Neo4j compatibility
    
    Using text-embedding-ada-002 for Neo4j compatibility:
    - Dimensions: 1536 (Neo4j standard)
    - Cost: $0.0001 per 1K tokens (most cost-effective)
    - Neo4j Support: Full compatibility with vector indexes
    - Rate Limits: Higher limits than newer models
    - Stability: Well-tested with Neo4j ecosystem
    """
    config = get_api_config()
    
    def safe_embed(texts):
        """Safe embedding function with error handling and retry logic"""
        try:
            return openai_embed(
                texts,
                model="text-embedding-ada-002",
                api_key=config['api_key'],
                base_url=config['base_url'],
            )
        except Exception as e:
            print(f"âš ï¸ [EMBEDDING] Error with text-embedding-ada-002: {e}")
            # For Neo4j compatibility, we stick with ada-002
            # If this fails, it's likely an API issue, not a model issue
            raise e
    
    return EmbeddingFunc(
        embedding_dim=1536,  # Neo4j-compatible dimensions
        max_token_size=8191,  # ada-002 token limit
        func=safe_embed,
    )

# ============================================================================
# LAZY RAG INITIALIZATION (Initialize once, reuse across requests)
# ============================================================================

def get_rag_instance():
    """Get or create singleton RAG instance (lazy initialization)"""
    global _rag_instance
    start_time = time.time()
    logger.info("ğŸš€ [RAG_INIT] Getting RAG instance...")
    
    # Thread-safe singleton pattern
    if _rag_instance is None:
        with _rag_lock:
            if _rag_instance is None:
                logger.info("ğŸš€ [RAG_INIT] Creating new RAG-Anything singleton...")
                init_start_time = time.time()
                
                try:
                    logger.info("ğŸš€ [RAG_INIT] Getting configuration...")
                    config = get_rag_config()
                    
                    logger.info("ğŸš€ [RAG_INIT] Getting LLM model function...")
                    llm_func = get_llm_model_func()
                    
                    logger.info("ğŸš€ [RAG_INIT] Getting vision model function...")
                    vision_func = get_vision_model_func(llm_func)
                    
                    logger.info("ğŸš€ [RAG_INIT] Getting embedding function...")
                    embedding_func = get_embedding_func()
                    
                    logger.info("ğŸš€ [RAG_INIT] Initializing RAGAnything...")
                    _rag_instance = RAGAnything(
                        config=config,
                        llm_model_func=llm_func,
                        vision_model_func=vision_func,
                        embedding_func=embedding_func,
                    )
                    
                    logger.info(f"ğŸš€ [RAG_INIT] RAG-Anything singleton initialized successfully in {time.time()-init_start_time:.3f}s")
                    
                except Exception as e:
                    logger.error(f"ğŸš€ [RAG_INIT] Failed to initialize RAG-Anything: {str(e)}")
                    raise
            else:
                logger.info(f"ğŸš€ [RAG_INIT] RAG instance already created by another thread in {time.time() - start_time:.3f}s")
    else:
        logger.info(f"ğŸš€ [RAG_INIT] Using existing RAG instance in {time.time() - start_time:.3f}s")
    
    return _rag_instance

# ============================================================================
# HEALTH CHECK
# ============================================================================

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    start_time = time.time()
    logger.info("ğŸ¥ [HEALTH] Health check requested...")
    
    try:
        rag_instance = get_rag_instance()
        rag_initialized = rag_instance is not None
        
        response = {
            "status": "healthy",
            "service": "raganything",
            "uptime": time.time() - start_time,
            "rag_initialized": rag_initialized
        }
        
        logger.info(f"ğŸ¥ [HEALTH] Health check completed in {time.time() - start_time:.3f}s")
        logger.info(f"ğŸ¥ [HEALTH] RAG initialized: {rag_initialized}")
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"ğŸ¥ [HEALTH] Health check failed: {str(e)}")
        return jsonify({
            "status": "unhealthy",
            "service": "raganything",
            "error": str(e),
            "uptime": time.time() - start_time
        }), 500

# ============================================================================
# DOCUMENT PROCESSING ENDPOINT
# ============================================================================

@app.route('/process', methods=['POST'])
def process_document():
    """Process document from S3 using RAG-Anything"""
    start_time = time.time()
    logger.info("ğŸ“„ [PROCESS] Document processing started...")
    
    temp_file_path = None
    
    try:
        # Validate request
        data = request.get_json()
        s3_bucket = data.get('bucket')
        s3_key = data.get('key')
        
        if not s3_bucket or not s3_key:
            logger.error("ğŸ“„ [PROCESS] Missing bucket or key in request")
            return jsonify({"error": "Missing bucket or key"}), 400
        
        logger.info(f"ğŸ“¦ [PROCESS] Processing document: s3://{s3_bucket}/{s3_key}")
        
        # Step 1: Download from S3
        download_start = time.time()
        logger.info("ğŸ“¥ [PROCESS] Starting S3 download...")
        s3_client = boto3.client('s3')
        temp_file_path = f"/tmp/{os.path.basename(s3_key)}"
        s3_client.download_file(s3_bucket, s3_key, temp_file_path)
        download_duration = time.time() - download_start
        
        file_size = os.path.getsize(temp_file_path) / (1024 * 1024)  # MB
        logger.info(f"ğŸ“¥ [PROCESS] Downloaded {file_size:.2f}MB in {download_duration:.3f}s")
        
        # Step 2: Get RAG instance (lazy init on first call)
        init_start = time.time()
        logger.info("ğŸš€ [PROCESS] Getting RAG instance...")
        rag = get_rag_instance()
        init_duration = time.time() - init_start
        logger.info(f"ğŸš€ [PROCESS] RAG instance ready in {init_duration:.3f}s")
        
        # Step 3: Process document
        process_start = time.time()
        parser = os.environ.get('PARSER', 'docling')
        parse_method = os.environ.get('PARSE_METHOD', 'auto')
        
        logger.info(f"ğŸ”§ [PROCESS] Using parser: {parser}, method: {parse_method}")
        
        process_kwargs = {
            'file_path': temp_file_path,
            'output_dir': os.environ.get('OUTPUT_DIR', '/rag-output/'),
            'doc_id': s3_key,
            'display_stats': True,
            'parse_method': parse_method,
        }
        
        # Add parser-specific parameters
        if parser == 'mineru':
            process_kwargs.update({
                'lang': os.environ.get('LANG', 'en'),
                'device': 'cpu',
                'formula': True,
                'table': True,
                'backend': 'pipeline',
                'source': 'local',
            })
        
        logger.info(f"ğŸ” [PROCESS] Processing with {parser} parser ({parse_method} mode)")
        result = run_async(rag.process_document_complete(**process_kwargs))
        process_duration = time.time() - process_start
        
        logger.info(f"ğŸ’¾ [PROCESS] Document processed in {process_duration:.3f}s")
        
        # Step 4: Cleanup
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
            logger.info(f"ğŸ§¹ [PROCESS] Temp file cleaned up")
        
        total_duration = time.time() - start_time
        logger.info(f"âœ… [PROCESS] Document processing completed successfully in {total_duration:.3f}s")
        
        return jsonify({
            "status": "success",
            "result": result,
            "document": {
                "bucket": s3_bucket,
                "key": s3_key,
                "size_mb": round(file_size, 2)
            },
            "parser": {
                "type": parser,
                "method": parse_method
            },
            "timing": {
                "total_duration": round(total_duration, 3),
                "download_duration": round(download_duration, 3),
                "rag_init_duration": round(init_duration, 3),
                "process_duration": round(process_duration, 3)
            }
        })
        
    except Exception as e:
        total_duration = time.time() - start_time
        logger.error(f"âŒ [PROCESS] Document processing failed after {total_duration:.3f}s: {str(e)}")
        
        import traceback
        traceback.print_exc()
        
        # Cleanup on error
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
            except:
                pass
        
        return jsonify({
            "error": str(e),
            "timing": {"total_duration": round(total_duration, 3)}
        }), 500

# ============================================================================
# QUERY ENDPOINT
# ============================================================================

@app.route('/query', methods=['POST'])
def process_query():
    """Process RAG query"""
    start_time = time.time()
    logger.info("ğŸ” [QUERY] RAG query started...")
    
    try:
        # Validate request
        data = request.get_json()
        query = data.get('query', '')
        mode = data.get('mode', 'hybrid')
        vlm_enhanced = data.get('vlm_enhanced')
        
        if not query:
            logger.error("ğŸ” [QUERY] Missing query in request")
            return jsonify({"error": "Missing query"}), 400
        
        logger.info(f"â“ [QUERY] Processing query: '{query[:80]}{'...' if len(query) > 80 else ''}'")
        logger.info(f"âš™ï¸ [QUERY] Mode: {mode}, VLM: {vlm_enhanced if vlm_enhanced is not None else 'auto'}")
        
        # Get RAG instance (reuses existing singleton)
        init_start = time.time()
        logger.info("ğŸš€ [QUERY] Getting RAG instance...")
        rag = get_rag_instance()
        init_duration = time.time() - init_start
        logger.info(f"ğŸš€ [QUERY] RAG instance ready in {init_duration:.3f}s")
        
        # Process query
        query_start = time.time()
        logger.info("ğŸ” [QUERY] Starting query processing...")
        query_kwargs = {'mode': mode}
        if vlm_enhanced is not None:
            query_kwargs['vlm_enhanced'] = vlm_enhanced
        
        result = run_async(rag.aquery(query, **query_kwargs))
        query_duration = time.time() - query_start
        
        logger.info(f"ğŸ“Š [QUERY] Query processed in {query_duration:.3f}s")
        
        # Parse result
        if isinstance(result, dict):
            answer = result.get('answer', str(result))
            sources = result.get('sources', [])
            confidence = result.get('confidence', 0.0)
        else:
            answer = str(result)
            sources = []
            confidence = 0.0
        
        total_duration = time.time() - start_time
        logger.info(f"âœ… [QUERY] Query completed successfully in {total_duration:.3f}s")
        
        return jsonify({
            "query": query,
            "answer": answer,
            "sources": sources,
            "confidence": confidence,
            "mode": mode,
            "vlm_enhanced": vlm_enhanced if vlm_enhanced is not None else "auto",
            "status": "completed",
            "timing": {
                "total_duration": round(total_duration, 3),
                "init_duration": round(init_duration, 3),
                "query_duration": round(query_duration, 3)
            }
        })
        
    except Exception as e:
        total_duration = time.time() - start_time
        logger.error(f"âŒ [QUERY] Query processing failed after {total_duration:.3f}s: {str(e)}")
        
        import traceback
        traceback.print_exc()
        
        return jsonify({
            "error": str(e),
            "query": query if 'query' in locals() else None,
            "answer": f"Error: {str(e)}",
            "sources": [],
            "confidence": 0.0,
            "status": "error",
            "timing": {"total_duration": round(total_duration, 3)}
        }), 500

# ============================================================================
# MULTIMODAL QUERY ENDPOINT
# ============================================================================

@app.route('/query_multimodal', methods=['POST'])
def process_multimodal_query():
    """Process multimodal RAG query with specific content (tables, equations, etc)"""
    start_time = time.time()
    update_activity()
    
    print(f"\n{'='*60}")
    print(f"ğŸ¨ [MULTIMODAL] Starting at {time.strftime('%H:%M:%S')}")
    
    try:
        # Validate request
        data = request.get_json()
        query = data.get('query', '')
        multimodal_content = data.get('multimodal_content', [])
        mode = data.get('mode', 'hybrid')
        
        if not query:
            return jsonify({"error": "Missing query"}), 400
        
        print(f"â“ [MULTIMODAL] Query: {query[:80]}{'...' if len(query) > 80 else ''}")
        print(f"ğŸ¨ [MULTIMODAL] Content items: {len(multimodal_content)}")
        
        # Get RAG instance
        init_start = time.time()
        rag = get_rag_instance()
        init_duration = time.time() - init_start
        
        # Process multimodal query
        query_start = time.time()
        result = run_async(rag.aquery_with_multimodal(
            query,
            multimodal_content=multimodal_content,
            mode=mode
        ))
        query_duration = time.time() - query_start
        
        print(f"ğŸ“Š [MULTIMODAL] Completed in {query_duration:.3f}s")
        
        # Parse result
        if isinstance(result, dict):
            answer = result.get('answer', str(result))
            sources = result.get('sources', [])
            confidence = result.get('confidence', 0.0)
        else:
            answer = str(result)
            sources = []
            confidence = 0.0
        
        total_duration = time.time() - start_time
        print(f"âœ… [MULTIMODAL] Total: {total_duration:.3f}s")
        print(f"{'='*60}\n")
        
        return jsonify({
            "query": query,
            "answer": answer,
            "sources": sources,
            "confidence": confidence,
            "mode": mode,
            "multimodal_items": len(multimodal_content),
            "status": "completed",
            "timing": {
                "total_duration": round(total_duration, 3),
                "init_duration": round(init_duration, 3),
                "query_duration": round(query_duration, 3)
            }
        })
        
    except Exception as e:
        total_duration = time.time() - start_time
        print(f"âŒ [MULTIMODAL] Error after {total_duration:.3f}s: {str(e)}")
        
        import traceback
        traceback.print_exc()
        
        return jsonify({
            "error": str(e),
            "query": query if 'query' in locals() else None,
            "answer": f"Error: {str(e)}",
            "sources": [],
            "confidence": 0.0,
            "status": "error",
            "timing": {"total_duration": round(total_duration, 3)}
        }), 500

@app.route('/analyze_efs', methods=['GET'])
def analyze_efs():
    """Analyze EFS contents - chunks, embeddings, metadata, graphs"""
    start_time = time.time()
    logger.info("ğŸ“Š [EFS_ANALYSIS] EFS analysis started...")
    
    try:
        efs_path = os.environ.get('EFS_MOUNT_PATH', '/mnt/efs')
        rag_output_dir = os.environ.get('RAG_OUTPUT_DIR', '/mnt/efs/rag_output')
        
        logger.info(f"ğŸ“Š [EFS_ANALYSIS] Analyzing EFS path: {efs_path}")
        logger.info(f"ğŸ“Š [EFS_ANALYSIS] RAG output dir: {rag_output_dir}")
        
        analysis = {
            'efs_path': efs_path,
            'rag_output_dir': rag_output_dir,
            'efs_exists': os.path.exists(efs_path),
            'rag_output_exists': os.path.exists(rag_output_dir),
            'files': [],
            'chunks': [],
            'embeddings': [],
            'metadata': [],
            'graphs': [],
            'total_files': 0,
            'total_size_bytes': 0
        }
        
        if not os.path.exists(efs_path):
            logger.error(f"ğŸ“Š [EFS_ANALYSIS] EFS path {efs_path} not found")
            return jsonify({
                'error': f'EFS path {efs_path} not found',
                'analysis': analysis
            }), 404
        
        logger.info("ğŸ“Š [EFS_ANALYSIS] Walking through EFS directory...")
        walk_start = time.time()
        
        # Walk through EFS directory and collect file information
        for root, dirs, files in os.walk(efs_path):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    file_size = os.path.getsize(file_path)
                    relative_path = os.path.relpath(file_path, efs_path)
                    
                    file_info = {
                        'path': file_path,
                        'relative_path': relative_path,
                        'name': file,
                        'size_bytes': file_size,
                        'directory': root
                    }
                    
                    analysis['files'].append(file_info)
                    analysis['total_files'] += 1
                    analysis['total_size_bytes'] += file_size
                    
                    # Categorize files by type
                    if file.endswith('.json'):
                        if 'chunk' in file.lower() or 'chunks' in root.lower():
                            analysis['chunks'].append(file_info)
                        elif 'embedding' in file.lower() or 'embeddings' in root.lower():
                            analysis['embeddings'].append(file_info)
                        elif 'metadata' in file.lower() or 'meta' in file.lower():
                            analysis['metadata'].append(file_info)
                        elif 'graph' in file.lower() or 'neo4j' in file.lower():
                            analysis['graphs'].append(file_info)
                    
                except Exception as e:
                    logger.warning(f"ğŸ“Š [EFS_ANALYSIS] Error processing file {file_path}: {str(e)}")
                    analysis['files'].append({
                        'path': file_path,
                        'name': file,
                        'error': str(e)
                    })
        
        walk_duration = time.time() - walk_start
        logger.info(f"ğŸ“Š [EFS_ANALYSIS] Directory walk completed in {walk_duration:.3f}s")
        logger.info(f"ğŸ“Š [EFS_ANALYSIS] Found {analysis['total_files']} files, {len(analysis['chunks'])} chunks, {len(analysis['embeddings'])} embeddings")
        
        # Try to read some sample chunk files to show content
        logger.info("ğŸ“Š [EFS_ANALYSIS] Reading sample chunk files...")
        sample_chunks = []
        for chunk_file in analysis['chunks'][:5]:  # First 5 chunk files
            try:
                with open(chunk_file['path'], 'r', encoding='utf-8') as f:
                    chunk_data = json.load(f)
                    
                    # For text chunks, show full content, for others show preview
                    if 'text_chunks' in chunk_file['name']:
                        sample_chunks.append({
                            'file': chunk_file['relative_path'],
                            'full_content': chunk_data,
                            'content_type': 'text_chunks'
                        })
                    else:
                        sample_chunks.append({
                            'file': chunk_file['relative_path'],
                            'content_preview': str(chunk_data)[:200] + '...' if len(str(chunk_data)) > 200 else str(chunk_data),
                            'content_type': 'other'
                        })
            except Exception as e:
                logger.warning(f"ğŸ“Š [EFS_ANALYSIS] Error reading chunk file {chunk_file['path']}: {str(e)}")
                sample_chunks.append({
                    'file': chunk_file['relative_path'],
                    'error': str(e)
                })
        
        analysis['sample_chunks'] = sample_chunks
        
        # Try to read some sample metadata files
        logger.info("ğŸ“Š [EFS_ANALYSIS] Reading sample metadata files...")
        sample_metadata = []
        for meta_file in analysis['metadata'][:3]:  # First 3 metadata files
            try:
                with open(meta_file['path'], 'r', encoding='utf-8') as f:
                    meta_data = json.load(f)
                    sample_metadata.append({
                        'file': meta_file['relative_path'],
                        'content_preview': str(meta_data)[:200] + '...' if len(str(meta_data)) > 200 else str(meta_data)
                    })
            except Exception as e:
                logger.warning(f"ğŸ“Š [EFS_ANALYSIS] Error reading metadata file {meta_file['path']}: {str(e)}")
                sample_metadata.append({
                    'file': meta_file['relative_path'],
                    'error': str(e)
                })
        
        analysis['sample_metadata'] = sample_metadata
        
        total_duration = time.time() - start_time
        logger.info(f"âœ… [EFS_ANALYSIS] EFS analysis completed successfully in {total_duration:.3f}s")
        
        return jsonify({
            'status': 'success',
            'analysis': analysis
        })
        
    except Exception as e:
        total_duration = time.time() - start_time
        logger.error(f"âŒ [EFS_ANALYSIS] EFS analysis failed after {total_duration:.3f}s: {str(e)}")
        
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 500

@app.route('/get_chunks', methods=['GET'])
def get_chunks():
    """Get full content of all chunks"""
    start_time = time.time()
    logger.info("ğŸ“„ [CHUNKS] Getting full chunk content...")
    
    try:
        efs_path = os.environ.get('EFS_MOUNT_PATH', '/mnt/efs')
        rag_output_dir = os.environ.get('RAG_OUTPUT_DIR', '/mnt/efs/rag_output')
        
        chunks_data = {
            'text_chunks': {},
            'entity_chunks': {},
            'relation_chunks': {},
            'vdb_chunks': {},
            'total_chunks': 0
        }
        
        # Read text chunks
        text_chunks_path = f"{rag_output_dir}/kv_store_text_chunks.json"
        if os.path.exists(text_chunks_path):
            logger.info("ğŸ“„ [CHUNKS] Reading text chunks...")
            with open(text_chunks_path, 'r', encoding='utf-8') as f:
                chunks_data['text_chunks'] = json.load(f)
                chunks_data['total_chunks'] += len(chunks_data['text_chunks'])
        
        # Read entity chunks
        entity_chunks_path = f"{rag_output_dir}/kv_store_entity_chunks.json"
        if os.path.exists(entity_chunks_path):
            logger.info("ğŸ“„ [CHUNKS] Reading entity chunks...")
            with open(entity_chunks_path, 'r', encoding='utf-8') as f:
                chunks_data['entity_chunks'] = json.load(f)
        
        # Read relation chunks
        relation_chunks_path = f"{rag_output_dir}/kv_store_relation_chunks.json"
        if os.path.exists(relation_chunks_path):
            logger.info("ğŸ“„ [CHUNKS] Reading relation chunks...")
            with open(relation_chunks_path, 'r', encoding='utf-8') as f:
                chunks_data['relation_chunks'] = json.load(f)
        
        # Read VDB chunks (vector database)
        vdb_chunks_path = f"{rag_output_dir}/vdb_chunks.json"
        if os.path.exists(vdb_chunks_path):
            logger.info("ğŸ“„ [CHUNKS] Reading VDB chunks...")
            with open(vdb_chunks_path, 'r', encoding='utf-8') as f:
                chunks_data['vdb_chunks'] = json.load(f)
        
        total_duration = time.time() - start_time
        logger.info(f"âœ… [CHUNKS] Retrieved {chunks_data['total_chunks']} chunks in {total_duration:.3f}s")
        
        return jsonify({
            'status': 'success',
            'chunks': chunks_data,
            'timing': {
                'total_duration': round(total_duration, 3)
            }
        })
        
    except Exception as e:
        total_duration = time.time() - start_time
        logger.error(f"âŒ [CHUNKS] Failed to get chunks after {total_duration:.3f}s: {str(e)}")
        
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 500

# ============================================================================
# SERVER STARTUP
# ============================================================================

def start_server():
    """Start the Flask server"""
    port = int(os.environ.get('PORT', 8000))
    
    logger.info("ğŸš€ [SERVER] Starting RAG-Anything Server...")
    logger.info(f"ğŸ“ [SERVER] Port: {port}")
    logger.info(f"ğŸ“‚ [SERVER] Working Dir: {os.environ.get('OUTPUT_DIR', '/rag-output/')}")
    logger.info(f"ğŸ”§ [SERVER] Parser: {os.environ.get('PARSER', 'docling')}")
    logger.info(f"ğŸ“ [SERVER] Parse Method: {os.environ.get('PARSE_METHOD', 'auto')}")
    logger.info(f"ğŸ”„ [SERVER] Lazy Init: RAG instance created on first request")
    logger.info("ğŸš€ [SERVER] Server starting...")
    
    # Use threaded mode for better request handling
    app.run(
        host='0.0.0.0',
        port=port,
        debug=False,
        threaded=True,
        use_reloader=False
    )

if __name__ == '__main__':
    start_server()