# -*- coding: utf-8 -*-
"""docx.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W_sNWAlXeEQYXqpjOr5Zo777rPopqry-
"""

# Install Python packages for document manipulation (python-docx), machine learning (faiss-cpu, sentence-transformers),
# and web app creation (fastapi, uvicorn), along with a tool for local server tunneling (pyngrok)
!pip install python-docx faiss-cpu sentence-transformers fastapi uvicorn pyngrok

# Update the list of software packages available on the system to ensure you get the latest versions
!apt-get update

# Install development tools needed for compiling Python extensions and handling compressed files
!apt-get install -y python3-dev zlib1g-dev

import os
import re
import faiss
import torch
import docx
import numpy as np
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from typing import Dict, List
from starlette.responses import JSONResponse
import uvicorn
import nest_asyncio
from google.colab import drive, userdata,files
import json
from pyngrok import ngrok

drive.mount('/content/drive')
nest_asyncio.apply()
# Define storage folder
storage_folder = "/content/drive/MyDrive/lifesciences/extracted_data"
os.makedirs(storage_folder, exist_ok=True)

# Extracts structured data from .docx files, organizing sections and table content into JSON format.

def extract_data_from_docx(docx_file):
    data = []
    doc = docx.Document(docx_file)
    current_section = None

    # Process paragraphs and tables together
    elements = doc.paragraphs + doc.tables
    elements.sort(key=lambda el: el._element.getparent().index(el._element))

    for el in elements:
        if isinstance(el, docx.text.paragraph.Paragraph):
            text = el.text.strip()
            # Detect section headers using regex
            if re.match(r'^\d+(\.\d+)*\s+[A-Za-z ]+', text):
                if current_section:
                    data.append(current_section)
                current_section = {"section_name": text, "table_data": []}
        elif isinstance(el, docx.table.Table):
            table_content = []
            header_row = True  # Flag to skip the header row

            for row in el.rows:
                row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]
                if header_row:
                    header_row = False  # Skip the first row (header row)
                    continue
                if row_text and not re.match(r'^\d+$', row_text[0]):  # Ignore serial number columns
                    table_content.append(row_text)  # Store row-wise table data

            # Assign table data to the latest detected section
            if current_section and table_content:
                current_section["table_data"].extend(table_content)

    # Ensure the last section is added
    if current_section:
        data.append(current_section)

    return data



# Function to process files
def process_uploaded_file(file_name):
    if file_name.endswith('.docx'):
        extracted_data = extract_data_from_docx(file_name)
        return extracted_data
    else:
        return "Unsupported file type."



filename = "/content/drive/My Drive/lifesciences/training_documents/ProtonGlow_URS.docx"
if not os.path.exists(filename):
  print(f"File not found: {filename}")

print(f"\nProcessing file: {filename}")
extracted_data = process_uploaded_file(filename)

if isinstance(extracted_data, list) and extracted_data:
    print("\nExtracted Data:")
    print(json.dumps(extracted_data, indent=4))  # Beautify JSON output

    # Save the extracted JSON data to the specified path
    output_path = "/content/drive/MyDrive/lifesciences/extracted_data/json/extracted_data.json"
    with open(output_path, "w") as json_file:
        json.dump(extracted_data, json_file, indent=4)
    print(f"\nExtracted data saved to: {output_path}")
else:
    print("No valid data found or unsupported file type.")

# Define the output folder
output_folder = "/content/drive/MyDrive/lifesciences/extracted_data/text"
os.makedirs(output_folder, exist_ok=True)

# Create all_sections.txt with section names (without numbers)
all_sections_path = os.path.join(output_folder, "all_sections.txt")
with open(all_sections_path, "w") as section_file:
    for section in extracted_data:
        section_name = re.sub(r'^\d+(\.\d+)*\s+', '', section["section_name"])  # Remove leading numbers
        section_file.write(section_name + "\n")
print(f"Section names saved to: {all_sections_path}")

# Create individual text files for each section's table data
for section in extracted_data:
    section_name = re.sub(r'^\d+(\.\d+)*\s+', '', section["section_name"])  # Clean section name
    section_filename = os.path.join(output_folder, f"{section_name}.txt")
    if section.get("table_data"):
        with open(section_filename, "w") as table_file:
            for row in section["table_data"]:
                table_file.write(f"{row[0]}: {row[1]}\n")  # Format as "1.1: Description"

    print(f"Table data saved to: {section_filename}")

# This code generates sentence embeddings for each text file in the specified folder and saves them as .npy files.
# It uses a pre-trained SentenceTransformer model to encode the text and stores the embeddings in a separate folder.

import os
import numpy as np
from sentence_transformers import SentenceTransformer

# Load model
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")

# Define paths
text_folder = "/content/drive/MyDrive/lifesciences/extracted_data/text"
embedding_folder = "/content/drive/MyDrive/lifesciences/extracted_data/embeddings"

# Ensure the embedding folder exists
os.makedirs(embedding_folder, exist_ok=True)

def generate_embeddings(documents):
    print("Generating embeddings...")
    embeddings = model.encode(documents, normalize_embeddings=True, show_progress_bar=True)
    print(f"Generated embeddings for {len(documents)} documents.")
    return embeddings

# Iterate over text files
for file_name in os.listdir(text_folder):
    if file_name.endswith(".txt"):
        text_file_path = os.path.join(text_folder, file_name)
        embedding_file_path = os.path.join(embedding_folder, file_name.replace(".txt", ".npy"))

        # Read text file line by line
        with open(text_file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        # Generate embedding for each line
        if lines:
            embeddings = generate_embeddings([line.strip() for line in lines if line.strip()])  # Strip empty lines
            # Save embeddings as .npy file
            np.save(embedding_file_path, embeddings)
            print(f"Saved: {embedding_file_path}")
        else:
            print(f"Skipping empty file: {file_name}")

# This code generates FAISS indices from pre-existing sentence embeddings and saves them as .faiss files.
# It loads each embedding, normalizes it for cosine similarity, and creates an index using FAISS's inner product approach.

def build_faiss_index(embeddings):
    print(embeddings.shape)
    print(embeddings)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)  # Inner Product for cosine similarity
    faiss.normalize_L2(embeddings)  # Normalize embeddings for cosine similarity
    index.add(embeddings)
    print(f"FAISS index built with {index.ntotal} vectors.")
    return index

# Define paths
embedding_folder = "/content/drive/MyDrive/lifesciences/extracted_data/embeddings"
index_folder = "/content/drive/MyDrive/lifesciences/extracted_data/indices"

# Ensure the index folder exists
os.makedirs(index_folder, exist_ok=True)

# Iterate over embedding files
for file_name in os.listdir(embedding_folder):
    if file_name.endswith(".npy"):
        embedding_file_path = os.path.join(embedding_folder, file_name)
        index_file_path = os.path.join(index_folder, file_name.replace(".npy", ".faiss"))

        # Load embedding
        embeddings = np.load(embedding_file_path)
        index = build_faiss_index(embeddings)

        # Save the FAISS index
        faiss.write_index(index, index_file_path)
        print(f"Saved index: {index_file_path}")

# This FastAPI application serves a query endpoint for searching relevant sections from a FAISS index.
# It uses a pre-trained sentence-transformers model to generate embeddings for user queries and searches
# both a general section index and section-specific indices to return relevant section data.
# The application is exposed via Ngrok for remote access.


# Define the FastAPI app
app = FastAPI()

# Paths for FAISS indices and section files
index_folder = "/content/drive/MyDrive/lifesciences/extracted_data/indices"
all_sections_index_path = os.path.join(index_folder, "all_sections.faiss")
section_file_path = "/content/drive/MyDrive/lifesciences/extracted_data/text/all_sections.txt"
section_folder_path="/content/drive/MyDrive/lifesciences/extracted_data/text/"

# Load the sentence-transformers model for query embeddings
try:
    model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
    print("✅ Sentence Transformer model loaded successfully.")
except Exception as e:
    print(f"❌ Error loading the sentence transformer model: {e}")
    raise RuntimeError("Model loading failed")

# Load the all_sections.faiss index
try:
    if os.path.exists(all_sections_index_path):
        all_sections_index = faiss.read_index(all_sections_index_path)
        print(f"✅ Loaded FAISS index from: {all_sections_index_path}")
        # Check the number of vectors and the dimensions
        print("Number of vectors:", all_sections_index.ntotal)
        print("Dimensions of each vector:", all_sections_index.d)

    else:
        print(f"❌ FAISS index file not found: {all_sections_index_path}")
        raise FileNotFoundError("FAISS index file missing")
except Exception as e:
    print(f"❌ Error loading FAISS index: {e}")
    raise RuntimeError("Failed to load FAISS index")

# Load section names from the text file
with open(section_file_path, "r", encoding="utf-8") as f:
    section_names = [line.strip() for line in f.readlines()]

# Define the structure of the input data (query)
class QueryInput(BaseModel):
    inputs: str

def encode_query(query):
    print(f"Encoding query: {query}")
    return model.encode([query], normalize_embeddings=True).astype('float32')

# Function to search for the most relevant section from the all_sections.faiss index
def search_index(query: str, index: faiss.Index, k, original_text_list):
    try:
        print(f"🔎 Generating embedding for the query: {query}")
        query_embedding =  encode_query(query)
        print(f"🔍 Searching FAISS index for the query...")
        distances, indices = index.search(query_embedding, k)  # Find the closest section (top 1 result)
        print(f"Distances: {distances}")
        print(f"Indices: {indices}")

        for idx, dist in zip(indices[0], distances[0]):
            print(f"Doc Index: {idx}, Similarity: {dist}")
            print(f"Document: {original_text_list[idx]}")
            print("-" * 50)

        results = [original_text_list[i] for i, d in zip(indices[0], distances[0])]
        print(f"Found {len(results)} matching documents.")
        return results if results else ["No relevant documents found."]
    except Exception as e:
        print(f"❌ Error during search: {e}")
        raise HTTPException(status_code=500, detail="Error during FAISS search")

# Function to get the relevant section file
def get_section_file(section_name: str):
    try:
        section_file_path = os.path.join(index_folder, f"{section_name}.faiss")
        print(f"📂 Looking for section file: {section_file_path}")

        if os.path.exists(section_file_path):
            print(f"✅ Section file found: {section_file_path}")
            return faiss.read_index(section_file_path)
        else:
            print(f"❌ Section file not found: {section_file_path}")
            return None
    except Exception as e:
        print(f"❌ Error accessing section file: {e}")
        raise HTTPException(status_code=500, detail="Error accessing section file")

# FastAPI Endpoints
@app.on_event("startup")
async def startup_event():
    ngrok.set_auth_token(userdata.get('ngrok_auth_token'))  # Replace with actual token if needed
    public_url = ngrok.connect(8000)
    print(f"API available at: {public_url}")

@app.post("/query")
async def query_section(query: QueryInput):
    try:
        print(f"📥 Received query: {query.inputs}")

        # Step 1: Search for the relevant section in the FAISS index
        section_name = search_index(query.inputs, all_sections_index, 1, section_names)
        print(f"✅ section_name {section_name}")
        # Step 2: Load the relevant section's FAISS index
        section_index_file = get_section_file(section_name[0])

        if not section_index_file:
            print(f"❌ Section '{section_name}' not found.")
            raise HTTPException(status_code=404, detail="Section not found.")


      # Load section names from the text file
        with open(os.path.join(section_folder_path, f"{section_name[0]}.txt"), "r", encoding="utf-8") as f:
            section_content = [line.strip() for line in f.readlines()]

        results = search_index(query.inputs, section_index_file, 10, section_content)
        print(f"✅ result {results}")

        # Step 5: Return the relevant information from the section index
        print(f"✅ Returning results: Section {section_name}, {results}")
        combined_results = "\n".join(results)
        response = [{"generated_text": combined_results}]
        return response

    except Exception as e:
        print(f"❌ Error handling query: {e}")
        response = [{"generated_text": "Sorry, no training data available for this query"}]
        return response

# Run the FastAPI server with Uvicorn and expose it via Ngrok
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)