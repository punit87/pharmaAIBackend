{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "6v14cmXPfed7",
        "outputId": "63c2ccfa-c08f-489d-8328-e34bbf5258d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nStart\\n |\\n |---> Install Required Libraries (pip install transformers, sentence-transformers, sklearn, pdfplumber, python-docx)\\n |\\n |---> Import Required Modules (os, glob, pdfplumber, docx, transformers, sentence-transformers, sklearn, numpy)\\n |\\n |---> Define Function: Extract Text from Documents\\n |      |---> If PDF: Use pdfplumber to extract text\\n |      |---> If DOCX: Use python-docx to extract text\\n |\\n |---> Define Function: Summarize Text using NLP\\n |      |---> Load Transformer Model (BART)\\n |      |---> Process Long Text in Chunks\\n |      |---> Generate Summary using Model\\n |\\n |---> Define Function: Compute Semantic Embeddings\\n |      |---> Load Sentence Transformer Model (all-MiniLM-L6-v2)\\n |      |---> Convert Summaries into Vector Embeddings\\n |\\n |---> Define Function: Cluster Documents into Labels\\n |      |---> Apply KMeans Clustering on Embeddings\\n |      |---> Assign Labels based on Cluster IDs\\n |\\n |---> Process Each Document in Folder\\n |      |---> Extract Text\\n |      |---> Summarize Content\\n |      |---> Generate Embeddings\\n |      |---> Store Data for Clustering\\n |\\n |---> Perform Clustering on All Document Embeddings\\n |      |---> Fit KMeans Model\\n |      |---> Assign Cluster Labels to Documents\\n |\\n |---> Output Results\\n |      |---> Print File Name, Summary, and Assigned Label\\n |\\nEnd\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "Start\n",
        " |\n",
        " |---> Install Required Libraries (pip install transformers, sentence-transformers, sklearn, pdfplumber, python-docx)\n",
        " |\n",
        " |---> Import Required Modules (os, glob, pdfplumber, docx, transformers, sentence-transformers, sklearn, numpy)\n",
        " |\n",
        " |---> Define Function: Extract Text from Documents\n",
        " |      |---> If PDF: Use pdfplumber to extract text\n",
        " |      |---> If DOCX: Use python-docx to extract text\n",
        " |\n",
        " |---> Define Function: Summarize Text using NLP\n",
        " |      |---> Load Transformer Model (BART)\n",
        " |      |---> Process Long Text in Chunks\n",
        " |      |---> Generate Summary using Model\n",
        " |\n",
        " |---> Define Function: Compute Semantic Embeddings\n",
        " |      |---> Load Sentence Transformer Model (all-MiniLM-L6-v2)\n",
        " |      |---> Convert Summaries into Vector Embeddings\n",
        " |\n",
        " |---> Define Function: Cluster Documents into Labels\n",
        " |      |---> Apply KMeans Clustering on Embeddings\n",
        " |      |---> Assign Labels based on Cluster IDs\n",
        " |\n",
        " |---> Process Each Document in Folder\n",
        " |      |---> Extract Text\n",
        " |      |---> Summarize Content\n",
        " |      |---> Generate Embeddings\n",
        " |      |---> Store Data for Clustering\n",
        " |\n",
        " |---> Perform Clustering on All Document Embeddings\n",
        " |      |---> Fit KMeans Model\n",
        " |      |---> Assign Cluster Labels to Documents\n",
        " |\n",
        " |---> Output Results\n",
        " |      |---> Print File Name, Summary, and Assigned Label\n",
        " |\n",
        "End\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools nltk jedi>=0.16 scikit-learn transformers sentence-transformers  pdfplumber python-docx"
      ],
      "metadata": {
        "id": "gH4Ycp0ufsTZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pdfplumber\n",
        "import docx\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pdfplumber\n",
        "import re\n",
        "import json\n",
        "\n",
        "nltk.download('punkt')\n",
        "drive.mount('/content/drive')\n",
        "training_folder = \"/content/drive/MyDrive/lifesciences/training_documents\"\n",
        "\n",
        "\n",
        "\n",
        "def extract_data_from_docx(docx_file):\n",
        "    data = []\n",
        "    doc = docx.Document(docx_file)\n",
        "    current_section = None\n",
        "\n",
        "    # Process paragraphs and tables together\n",
        "    elements = doc.paragraphs + doc.tables\n",
        "    elements.sort(key=lambda el: el._element.getparent().index(el._element))\n",
        "\n",
        "    for el in elements:\n",
        "        if isinstance(el, docx.text.paragraph.Paragraph):\n",
        "            text = el.text.strip()\n",
        "            # Detect section headers using regex\n",
        "            if re.match(r'^\\d+(\\.\\d+)*\\s+[A-Za-z ]+', text):\n",
        "                if current_section:\n",
        "                    data.append(current_section)\n",
        "                current_section = {\"section_name\": text, \"table_data\": []}\n",
        "        elif isinstance(el, docx.table.Table):\n",
        "            table_content = []\n",
        "            header_row = True  # Flag to skip the header row\n",
        "\n",
        "            for row in el.rows:\n",
        "                row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]\n",
        "                if header_row:\n",
        "                    header_row = False  # Skip the first row (header row)\n",
        "                    continue\n",
        "                if row_text and not re.match(r'^\\d+$', row_text[0]):  # Ignore serial number columns\n",
        "                    table_content.append(row_text)  # Store row-wise table data\n",
        "\n",
        "            # Assign table data to the latest detected section\n",
        "            if current_section and table_content:\n",
        "                current_section[\"table_data\"].extend(table_content)\n",
        "\n",
        "    # Ensure the last section is added\n",
        "    if current_section:\n",
        "        data.append(current_section)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def extract_data_from_pdf(pdf_file):\n",
        "    data = []\n",
        "    current_section = None\n",
        "\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        # Extract text and tables from all pages\n",
        "        all_text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            all_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        # Split text into paragraphs (assuming paragraphs are separated by newlines)\n",
        "        paragraphs = all_text.split(\"\\n\")\n",
        "\n",
        "        # Extract tables from the PDF\n",
        "        tables = []\n",
        "        for page in pdf.pages:\n",
        "            tables.append(page.extract_tables())\n",
        "\n",
        "        # Flatten tables (in case there are multiple tables across pages)\n",
        "        flattened_tables = [row for page_tables in tables for table in page_tables for row in table]\n",
        "\n",
        "        # Process paragraphs\n",
        "        for para in paragraphs:\n",
        "            text = para.strip()\n",
        "            if text:\n",
        "                # Detect section headers using regex (similar to DOCX section matching)\n",
        "                if re.match(r'^\\d+(\\.\\d+)*\\s+[A-Za-z ]+', text):\n",
        "                    # Add the current section if it exists before starting a new one\n",
        "                    if current_section:\n",
        "                        data.append(current_section)\n",
        "                    current_section = {\"section_name\": text, \"table_data\": []}\n",
        "\n",
        "        # Process tables\n",
        "        table_content = []\n",
        "        for table in flattened_tables:\n",
        "            for row in table:\n",
        "                row_text = [cell.strip() for cell in row if cell.strip()]\n",
        "                # Skip rows that just have serial numbers (assumes first column is serial numbers)\n",
        "                if row_text and not re.match(r'^\\d+$', row_text[0]):\n",
        "                    table_content.append(row_text)\n",
        "\n",
        "        # Assign table data to the latest detected section\n",
        "        if current_section and table_content:\n",
        "            current_section[\"table_data\"].extend(table_content)\n",
        "\n",
        "        # Ensure the last section is added\n",
        "        if current_section:\n",
        "            data.append(current_section)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# 1️⃣ Extract text from documents\n",
        "def extract_text_from_doc(file_path):\n",
        "    text = \"\"\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        data = extract_data_from_pdf(file_path)\n",
        "    elif file_path.endswith(\".docx\"):\n",
        "        data = extract_data_from_docx(file_path)\n",
        "        text = json.dumps(data, ensure_ascii=False, indent=4)\n",
        "    return text\n",
        "\n",
        "\n",
        "# 3️⃣ Compute embeddings for the entire document text\n",
        "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def compute_embedding(text):\n",
        "    return encoder.encode(text, convert_to_numpy=True)\n",
        "\n",
        "# 4️⃣ Function to assign meaningful labels based on summary\n",
        "def assign_meaningful_label(summary):\n",
        "    # Define keywords for labels\n",
        "    labels_dict = {\n",
        "        \"Technology\": [\"data science\", \"machine learning\", \"AI\", \"artificial intelligence\", \"deep learning\", \"algorithm\"],\n",
        "        \"Life Sciences\": [\"health\", \"biotechnology\", \"medicine\", \"pharmaceutical\", \"biology\", \"genomics\", \"life science\"],\n",
        "        \"Business\": [\"marketing\", \"business\", \"economy\", \"finance\", \"growth\", \"strategy\", \"management\"],\n",
        "        \"Environment\": [\"climate\", \"sustainability\", \"environment\", \"ecology\", \"pollution\", \"green\", \"conservation\"]\n",
        "    }\n",
        "\n",
        "    # Convert summary to lower case for comparison\n",
        "    summary_lower = summary.lower()\n",
        "\n",
        "    # Check for matching keywords and assign labels\n",
        "    for label, keywords in labels_dict.items():\n",
        "        if any(keyword in summary_lower for keyword in keywords):\n",
        "            return label\n",
        "    return \"Uncategorized\"  # Default if no keywords are found\n",
        "\n",
        "# Main processing function updated for similarity score based on entire document text\n",
        "def process_documents(folder_path):\n",
        "    file_paths = glob.glob(os.path.join(folder_path, \"*\"))\n",
        "\n",
        "    documents = []\n",
        "    summaries = []\n",
        "    embeddings = []\n",
        "\n",
        "    # Process each document\n",
        "    for file_path in file_paths:\n",
        "        text = extract_text_from_doc(file_path)\n",
        "        embedding = compute_embedding(text)  # Use entire document text for embedding\n",
        "        label = assign_meaningful_label(text)  # Assign meaningful label based on the document text\n",
        "\n",
        "        documents.append({\"file\": file_path, \"text\": text, \"embedding\": embedding, \"label\": label})\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    # Calculate cosine similarity between all document embeddings\n",
        "    similarity_matrix = cosine_similarity(np.array(embeddings))\n",
        "\n",
        "    # Print similarity scores between document pairs\n",
        "    print(\"\\nDocument Similarity Scores (based on entire text):\")\n",
        "    for i in range(len(documents)):\n",
        "        for j in range(i + 1, len(documents)):\n",
        "            similarity_score = similarity_matrix[i][j]\n",
        "            print(f\"Similarity between {documents[i]['file']} and {documents[j]['file']}: {similarity_score:.4f}\")\n",
        "\n",
        "    # Cluster based on embeddings (optional)\n",
        "    embeddings_matrix = np.array(embeddings)\n",
        "    labels = cluster_documents(embeddings_matrix, num_clusters=2)\n",
        "\n",
        "    # Assign labels to documents (if clustering is needed)\n",
        "    for i, doc in enumerate(documents):\n",
        "        doc[\"cluster_label\"] = f\"Cluster {labels[i]}\"\n",
        "\n",
        "    return documents\n",
        "\n",
        "# Run on a folder containing documents\n",
        "results = process_documents(training_folder)\n",
        "\n",
        "# Print results with meaningful labels\n",
        "for doc in results:\n",
        "    print(f\"File: {doc['file']}\")\n",
        "    print(f\"Meaningful Label: {doc['label']}\")\n",
        "    print(f\"Cluster Label: {doc['cluster_label']}\\n{'-'*40}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEk2E1j6fh0Z",
        "outputId": "8b9307f6-56dd-453f-e5ad-1ae73b574f75"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Document Similarity Scores (based on entire text):\n",
            "Similarity between /content/drive/MyDrive/lifesciences/training_documents/ProtonGlow_URS copy.docx and /content/drive/MyDrive/lifesciences/training_documents/ProtonGlow_URS copy.pdf: -0.0061\n",
            "File: /content/drive/MyDrive/lifesciences/training_documents/ProtonGlow_URS copy.docx\n",
            "Meaningful Label: Technology\n",
            "Cluster Label: Cluster 0\n",
            "----------------------------------------\n",
            "File: /content/drive/MyDrive/lifesciences/training_documents/ProtonGlow_URS copy.pdf\n",
            "Meaningful Label: Uncategorized\n",
            "Cluster Label: Cluster 1\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}