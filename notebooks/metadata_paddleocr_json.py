# -*- coding: utf-8 -*-
"""metadata_paddleocr_json.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UlEAtxQnVxXLqVAsmarUpv6wdFoPuZKv
"""

# -*- coding: utf-8 -*-
"""metadatadb_ppstructure.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11slh4u9PXCiykhkle_Jz58BHBKgH_iE_
"""

# Install Dependencies
!apt-get install ghostscript -y
!pip install paddlepaddle paddleocr>=2.7.0 faiss-cpu numpy PyPDF2 tk google-colab python-docx pdf2image psutil opencv-python
!pip install sentence-transformers
!apt-get install poppler-utils -y
!apt-get install -y libreoffice
!pip install pyuno

"""Setup and Initialization"""

import os
from google.colab import drive
import psutil
import logging
import subprocess
import time
import shutil
import json
import re
import gc
import numpy as np
import faiss
import pdf2image
from pdf2image import convert_from_path
from datetime import datetime
from sentence_transformers import SentenceTransformer
from paddleocr import PPStructure
import traceback
import cv2

# Set up logging with more detail
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger()

# Ensure debug logs are written to a file
log_file_handler = logging.FileHandler('/tmp/debug.log')
log_file_handler.setLevel(logging.DEBUG)
log_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(log_file_handler)

# Mount Google Drive
print("Mounting Google Drive...")
drive.mount('/content/drive', force_remount=True)
base_path = '/content/drive/My Drive/lifesciences'
training_docs_path = os.path.join(base_path, 'training_documents')
staging_images_path = os.path.join(base_path, 'staging/images')
staging_pdfs_path = os.path.join(base_path, 'staging/pdfs')
processed_faiss_path = os.path.join(base_path, 'processed/content_faiss.index')
metadata_json_path = os.path.join(base_path, 'processed/metadata.json')
local_metadata_json_path = '/tmp/metadata.json'  # Local path to avoid Google Drive sync issues

# Ensure directories exist with retry logic
def ensure_directory(directory):
    max_retries = 3
    for attempt in range(max_retries):
        try:
            os.makedirs(directory, exist_ok=True)
            if os.path.exists(directory):
                print(f"Directory created/exists: {directory}")
                return
            else:
                logger.warning(f"Directory {directory} not found after creation attempt {attempt+1}")
        except Exception as e:
            logger.warning(f"Failed to create directory {directory} (attempt {attempt+1}/{max_retries}): {str(e)}")
        time.sleep(1)  # Wait before retrying
    raise Exception(f"Failed to create directory {directory} after {max_retries} attempts")

ensure_directory(staging_images_path)
ensure_directory(staging_pdfs_path)
ensure_directory(os.path.dirname(processed_faiss_path))
ensure_directory(os.path.dirname(metadata_json_path))

# Initialize local metadata.json with an empty structure
print(f"Resetting metadata.json at {local_metadata_json_path}")
with open(local_metadata_json_path, 'w') as f:
    json.dump({"metadata": [], "last_id": 0}, f)

# Initialize Sentence Transformer model
print("Initializing Sentence Transformer...")
sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
EMBEDDING_DIM = 384

# Function to log memory usage
def log_memory(step=""):
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    mem_usage = mem_info.rss / 1024**2
    print(f"{step} - Memory usage: RSS={mem_usage:.2f} MB, VMS={mem_info.vms / 1024**2:.2f} MB")
    if mem_usage > 10000:
        logger.warning("Memory usage exceeds 10 GB, potential crash risk")

log_memory("After setup")

"""Convert DOCX to PDF"""

def docx_to_pdf(docx_path, staging_pdfs_path):
    try:
        pdf_filename = os.path.basename(docx_path).replace('.docx', '.pdf')
        pdf_path = os.path.join(staging_pdfs_path, pdf_filename)
        print(f"Attempting to convert {docx_path} to {pdf_path}")

        local_docx = f"/tmp/{os.path.basename(docx_path)}"
        shutil.copy(docx_path, local_docx)

        cmd = f"libreoffice --headless --convert-to pdf \"{local_docx}\" --outdir \"{staging_pdfs_path}\""
        print(f"Running command: {cmd}")
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        print(f"LibreOffice conversion stdout: {result.stdout}")
        print(f"LibreOffice conversion stderr: {result.stderr}")

        time.sleep(5)

        if not os.path.exists(pdf_path):
            logger.error(f"Conversion failed: {pdf_path} not found")
            raise FileNotFoundError(f"PDF not found after conversion: {pdf_path}")

        print(f"Conversion successful: {pdf_path} created")
        os.remove(local_docx)
        log_memory("After conversion")
        return pdf_path
    except Exception as e:
        logger.error(f"Error in DOCX to PDF conversion: {str(e)}")
        print(traceback.format_exc())
        raise

"""Initialize OCR and Process PDF"""

def load_metadata_json():
    try:
        if os.path.exists(local_metadata_json_path):
            with open(local_metadata_json_path, 'r') as f:
                return json.load(f)
        return {"metadata": [], "last_id": 0}
    except Exception as e:
        logger.error(f"Error loading metadata JSON: {str(e)}")
        raise

def deduplicate_metadata(metadata_store, new_entry):
    """Deduplicate entries based on section, type, and content."""
    for entry in metadata_store["metadata"]:
        if (entry["section"] == new_entry["section"] and
            entry["type"] == new_entry["type"] and
            entry["content"] == new_entry["content"]):
            print(f"Skipping duplicate entry: {new_entry}")
            return True
    return False

def save_to_json(metadata_store, id_seq, urs_name, section, item_type, content, coordinates, source_file):
    new_entry = {
        "id": id_seq,
        "urs": urs_name,
        "section": section,
        "type": item_type,
        "content": content,
        "coordinates": coordinates,
        "created_at": datetime.now().isoformat(),
        "source_file": source_file  # New key with Google Colab file path
    }

    # Skip if entry is a duplicate
    if deduplicate_metadata(metadata_store, new_entry):
        return id_seq

    metadata_store["metadata"].append(new_entry)
    metadata_store["last_id"] = id_seq + 1  # Update last_id to match the next id
    with open(local_metadata_json_path, 'w') as f:
        json.dump(metadata_store, f, indent=2)
    print(f"Saved {item_type} to JSON, ID: {id_seq}, Source: {source_file}")
    return id_seq + 1

def copy_metadata_to_drive():
    """Copy the local metadata.json to Google Drive at the end."""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            shutil.copy(local_metadata_json_path, metadata_json_path)
            print(f"Copied metadata.json to Google Drive: {metadata_json_path}")
            return
        except Exception as e:
            logger.warning(f"Failed to copy metadata.json to Google Drive (attempt {attempt+1}/{max_retries}): {str(e)}")
            time.sleep(1)
    raise Exception(f"Failed to copy metadata.json to Google Drive after {max_retries} attempts")

def extract_text_content(item):
    print("Extracting text content...")
    content = ""
    if isinstance(item.get('res'), list):
        content = '\n'.join([sub_item.get('text', '') for sub_item in item['res'] if isinstance(sub_item, dict) and 'text' in sub_item])
    elif isinstance(item.get('res'), dict) and 'text' in item['res']:
        content = item['res']['text']
    elif isinstance(item.get('res'), str):
        content = item['res']
    else:
        content = item.get('res', {}).get('text', '')
    return content.strip()

def extract_list_content(item):
    print("Extracting list content...")
    content = []
    if 'res' in item:
        if isinstance(item['res'], list):
            for sub_item in item['res']:
                text = sub_item.get('text', '') if isinstance(sub_item, dict) and 'text' in sub_item else sub_item if isinstance(sub_item, str) else ""
                cleaned_text = re.sub(r'^[^a-zA-Z0-9]+', '', text).strip()
                if cleaned_text:
                    content.append(cleaned_text)
        elif isinstance(item['res'], dict) and 'text' in item['res']:
            cleaned_text = re.sub(r'^[^a-zA-Z0-9]+', '', item['res']['text']).strip()
            content = [cleaned_text]
        elif isinstance(item['res'], str):
            cleaned_text = re.sub(r'^[^a-zA-Z0-9]+', '', item['res']).strip()
            content = [cleaned_text]
    return content if content else ["List content not found"]

def extract_table_content(item):
    print("Extracting table content...")
    if 'res' in item:
        if 'cells' in item['res']:
            table_data = []
            for row in item['res']['cells']:
                if row and isinstance(row[0], dict):
                    row_data = [cell.get('text', '').strip() for cell in row if cell.get('text', '').strip()]
                else:
                    row_data = [cell.strip() for cell in row if isinstance(cell, str) and cell.strip()]
                if row_data:
                    table_data.append(row_data)
            return table_data if table_data else "Table content not found"
        elif 'text' in item['res']:
            return [item['res']['text'].strip()]
    return "Table content not found"

def preprocess_image(image_path):
    print(f"Preprocessing image: {image_path}")
    img = cv2.imread(image_path)
    if img is None:
        logger.error(f"Failed to read image: {image_path}")
        return image_path
    return image_path

def should_skip_initial_table(item, page_height):
    coordinates = item.get('bbox', [0, 0, 0, 0])
    content = str(item.get('res', ''))
    if (item.get('type', '').lower() == 'table' and
        len(coordinates) == 4 and
        coordinates[3] - coordinates[1] >= page_height * 0.6 and
        ('<html>' in content.lower() or 'table content not found' in content.lower())):
        print(f"Skipping initial full-page table with coordinates {coordinates} and content: {content}")
        return True
    return False

def post_process_layout(layout_result, page_height):
    text_items = [item for item in layout_result if item.get('type', '').lower() == 'text']
    new_layout = [item for item in layout_result if item.get('type', '').lower() != 'text']
    table_rows = []
    current_row = []
    last_y = None
    y_threshold = 80
    x_threshold = 350

    sorted_text_items = sorted(text_items, key=lambda x: (x.get('bbox', [0, 0, 0, 0])[1], x.get('bbox', [0, 0, 0, 0])[0]))
    for item in sorted_text_items:
        bbox = item.get('bbox', [0, 0, 0, 0])
        y = bbox[1]
        x = bbox[0]
        if isinstance(item.get('res'), list):
            text = '\n'.join([sub_item.get('text', '') for sub_item in item['res'] if isinstance(sub_item, dict) and 'text' in sub_item]).strip()
        else:
            text = item.get('res', {}).get('text', '').strip()
        print(f"Post-processing text item: {text}, y={y}, x={x}, bbox={bbox}")
        if last_y is None or abs(y - last_y) < y_threshold:
            current_row.append((text, bbox))
        else:
            if len(current_row) >= 2:
                x_coords = [coord[0] for _, coord in current_row]
                if max(x_coords) - min(x_coords) > x_threshold:
                    table_rows.append([text for text, _ in current_row])
            else:
                new_layout.extend([{'type': 'text', 'res': {'text': text}, 'bbox': bbox} for text, bbox in current_row])
            current_row = [(text, bbox)]
        last_y = y

    if len(current_row) >= 2:
        x_coords = [coord[0] for _, coord in current_row]
        if max(x_coords) - min(x_coords) > x_threshold:
            table_rows.append([text for text, _ in current_row])
        else:
            new_layout.extend([{'type': 'text', 'res': {'text': text}, 'bbox': bbox} for text, bbox in current_row])
    elif current_row:
        new_layout.extend([{'type': 'text', 'res': {'text': text}, 'bbox': bbox} for text, bbox in current_row])

    for row in table_rows:
        if len(row) >= 2:
            bbox = [min([coord[0] for _, coord in current_row]), min([coord[1] for _, coord in current_row]),
                    max([coord[2] for _, coord in current_row]), max([coord[3] for _, coord in current_row])]
            new_layout.append({
                'type': 'table',
                'res': {'cells': [row]},
                'bbox': bbox
            })
        else:
            new_layout.extend([{'type': 'text', 'res': {'text': text}, 'bbox': bbox} for text, bbox in [(text, bbox) for text, bbox in current_row]])

    return sorted(new_layout, key=lambda x: x.get('bbox', [0, 0, 0, 0])[1])

def process_page(page_num, pdf_path, structure, staging_path, current_section,
                metadata_store, id_seq, urs_name, faiss_index, source_file):
    print(f"Processing page {page_num+1}")
    try:
        images = convert_from_path(pdf_path, dpi=800, first_page=page_num+1, last_page=page_num+1)
        image = images[0]
        page_height = image.height

        # Create unique filename using source filename and page number
        source_filename = os.path.splitext(os.path.basename(source_file))[0]
        image_path = os.path.join(staging_path, f"{source_filename}_page_{page_num+1}.png")
        print(f"Saving image to: {image_path}")

        ensure_directory(staging_path)
        image.save(image_path, 'PNG')

        if not os.path.exists(image_path):
            logger.error(f"Image save failed for page {page_num+1}")
            return current_section, id_seq

        image_path = preprocess_image(image_path)

        print(f"Analyzing page {page_num+1} layout with PP-StructureV2...")
        layout_result = structure(image_path, return_ocr_result_in_table=True)
        print(f"Raw layout_result: {layout_result}")
        for item in layout_result:
            print(f"Item type: {item.get('type')}, Content: {item.get('res')}, BBox: {item.get('bbox')}")
        print(f"Found {len(layout_result)} elements on page {page_num+1}")
        layout_result = sorted(layout_result, key=lambda x: x.get('bbox', [0, 0, 0, 0])[1] if isinstance(x, dict) else float('inf'))

        filtered_layout = [item for item in layout_result if not should_skip_initial_table(item, page_height)]
        if len(filtered_layout) < len(layout_result):
            print(f"Filtered out {len(layout_result) - len(filtered_layout)} initial table(s)")

        layout_result = post_process_layout(filtered_layout, page_height)
        print(f"Post-processed layout_result: {layout_result}")

        for item in layout_result:
            if isinstance(item, list):
                for sub_item in item:
                    if isinstance(sub_item, dict):
                        current_section, id_seq = process_item(sub_item, current_section,
                                                             metadata_store, id_seq, urs_name,
                                                             faiss_index, source_file)
                continue
            elif not isinstance(item, dict):
                logger.warning(f"Skipping invalid item (not a dict or list): {item}")
                continue

            current_section, id_seq = process_item(item, current_section, metadata_store,
                                                 id_seq, urs_name, faiss_index, source_file)

        # Clear memory after processing content
        del images, image, layout_result, filtered_layout
        gc.collect()
        log_memory(f"Completed page {page_num+1}")
        return current_section, id_seq

    except Exception as e:
        logger.error(f"ERROR processing page {page_num+1}: {str(e)}")
        print(traceback.format_exc())
        gc.collect()
        return current_section, id_seq

def process_item(item, current_section, metadata_store, id_seq, urs_name, faiss_index, source_file):
    """
    Process an individual item from the layout analysis.

    Args:
        item: The layout item to process
        current_section: Current document section
        metadata_store: Metadata storage dictionary
        id_seq: Current ID sequence number
        urs_name: User Requirement Specification name
        faiss_index: FAISS index for embeddings
        source_file: Source file path
    """
    item_type = item.get('type', '').lower()
    print(f"Processing {item_type} element: Full item data: {item}")
    coordinates = item.get('bbox', [0, 0, 0, 0])
    content = None

    # Process different item types dynamically
    content_extractors = {
        'title': extract_text_content,
        'text': extract_text_content,
        'list': extract_list_content,
        'table': extract_table_content,
        'figure': lambda x: "Figure detected"
    }

    if item_type in content_extractors:
        content = content_extractors[item_type](item)

    # Handle title as section marker
    if item_type == 'title' and content:
        current_section = content if content.strip() else current_section
        print(f"New section: {current_section}")

    # Skip empty content
    if not content or (isinstance(content, str) and not content.strip()) or \
       (isinstance(content, list) and not any(c.strip() for c in content if isinstance(c, str))):
        print("Skipping empty content")
        return current_section, id_seq

    # Save to JSON and generate embedding
    id_seq = save_to_json(metadata_store, id_seq, urs_name, current_section, item_type,
                         content, coordinates, source_file)

    # Generate embedding content
    embedding_content = '\n'.join([str(c) for c in content]) if isinstance(content, list) else str(content)
    embedding = sentence_model.encode(embedding_content, convert_to_numpy=True)
    faiss_index.add(np.array([embedding]))

    # Clear memory
    del embedding, embedding_content, content
    gc.collect()

    return current_section, id_seq

def process_pdf(pdf_path, urs_name, staging_images_path, processed_faiss_path, source_file):
    print(f"Starting to process PDF: {pdf_path}")

    try:
        print("Initializing PP-StructureV2...")
        structure = PPStructure(
            structure_version="PP-StructureV2",
            table=True,
            ocr=True,
            use_gpu=True,
            layout=True,
            layout_model_dir="picodet_lcnet_x1_0_fgd_layout",
            table_model_dir="en_ppstructure_mobile_v2.0_SLANet",
            layout_score_threshold=0.2,
            layout_nms_threshold=0.5,
            table_score_threshold=0.2,
            table_nms_threshold=0.2,
            lang='en'
        )
        log_memory("PP-StructureV2 initialized")
    except Exception as e:
        logger.error(f"Failed to initialize PP-StructureV2: {str(e)}")
        print(traceback.format_exc())
        raise

    print("Initializing FAISS index...")
    faiss_index = faiss.IndexFlatL2(EMBEDDING_DIM)

    if not os.path.exists(staging_images_path):
        ensure_directory(staging_images_path)

    pdf_info = pdf2image.pdfinfo_from_path(pdf_path)
    total_pages = pdf_info["Pages"]
    print(f"PDF has {total_pages} pages")

    # Process all pages
    pages_to_process = total_pages
    print(f"Will process all {pages_to_process} pages")

    metadata_store = load_metadata_json()
    id_seq = metadata_store["last_id"]
    current_section = ''

    for page in range(pages_to_process):
        current_section, id_seq = process_page(
            page, pdf_path, structure, staging_images_path,
            current_section, metadata_store, id_seq, urs_name, faiss_index,
            source_file
        )

    # Clear memory after all pages are processed
    del structure
    gc.collect()

    faiss_dir = os.path.dirname(processed_faiss_path)
    if not os.path.exists(faiss_dir):
        os.makedirs(faiss_dir)

    print(f"Saving FAISS index to: {processed_faiss_path}")
    faiss.write_index(faiss_index, processed_faiss_path)
    print("FAISS index saved successfully")

    copy_metadata_to_drive()

    # Clear remaining memory
    del faiss_index, metadata_store
    gc.collect()

    print(f"PDF processing complete for {urs_name}")

def process_all_files(training_docs_path, staging_pdfs_path, staging_images_path, processed_faiss_path):
    # Process both DOCX and PDF files
    docx_files = [f for f in os.listdir(training_docs_path) if f.endswith('.docx')]
    pdf_files = [f for f in os.listdir(training_docs_path) if f.endswith('.pdf')]

    if not docx_files and not pdf_files:
        print(f"No .docx or .pdf files found in {training_docs_path}")
        return

    # Process DOCX files
    for filename in docx_files:
        try:
            file_path = os.path.join(training_docs_path, filename)
            urs_name = input(f"Enter URS name for {filename} (converted to PDF): ")
            staged_pdf_path = docx_to_pdf(file_path, staging_pdfs_path)
            process_pdf(staged_pdf_path, urs_name, staging_images_path, processed_faiss_path, source_file=file_path)
        except Exception as e:
            logger.error(f"Failed to process {filename}: {str(e)}")
            print(traceback.format_exc())
            continue

    # Process PDF files
    for filename in pdf_files:
        try:
            file_path = os.path.join(training_docs_path, filename)
            urs_name = input(f"Enter URS name for {filename}: ")
            process_pdf(file_path, urs_name, staging_images_path, processed_faiss_path, source_file=file_path)
        except Exception as e:
            logger.error(f"Failed to process {filename}: {str(e)}")
            print(traceback.format_exc())
            continue

if __name__ == "__main__":
    try:
        print("Processing all files in training_documents folder...")
        process_all_files(training_docs_path, staging_pdfs_path, staging_images_path, processed_faiss_path)
    except Exception as e:
        logger.error(f"Main execution failed: {str(e)}")
        print(traceback.format_exc())
        try:
            copy_metadata_to_drive()
        except Exception as e2:
            logger.error(f"Failed to copy metadata.json on failure: {str(e2)}")
        raise

"""Visualization"""

def visualize_faiss():
    import matplotlib.pyplot as plt
    from sklearn.decomposition import PCA
    faiss_index = faiss.read_index(processed_faiss_path)
    if faiss_index.ntotal == 0:
        logger.warning("FAISS index is empty, nothing to visualize")
        return
    elif faiss_index.ntotal == 1:
        logger.warning("FAISS index has only 1 entry, visualizing as a single point")
        embeddings = faiss_index.reconstruct_n(0, 1)
        plt.scatter(embeddings[0, 0], embeddings[0, 1], s=100)
        plt.title("FAISS Index Visualization (Single Point)")
        plt.show()
        return

    embeddings = faiss_index.reconstruct_n(0, faiss_index.ntotal)
    pca = PCA(n_components=min(2, faiss_index.ntotal))
    reduced = pca.fit_transform(embeddings)
    plt.scatter(reduced[:, 0], reduced[:, 1] if reduced.shape[1] > 1 else np.zeros(reduced.shape[0]))
    plt.title("FAISS Index Visualization (PCA)")
    plt.show()

print("Visualizing FAISS index")
visualize_faiss()

def query_faiss(urs_name, section_name):
    faiss_index = faiss.read_index(processed_faiss_path)
    if faiss_index.ntotal == 0:
        logger.warning("FAISS index is empty, no query possible")
        return None

    query_text = f"{urs_name} {section_name}"
    query_embedding = sentence_model.encode(query_text, convert_to_numpy=True)
    D, I = faiss_index.search(np.array([query_embedding]), 1)
    index = int(I[0][0])

    with open(metadata_json_path, 'r') as f:
        metadata_store = json.load(f)

    for entry in metadata_store["metadata"]:
        if entry["id"] == index + 1:
            return entry
    return None

urs = input("Enter URS name to query: ")
section = input("Enter section name to query: ")
result = query_faiss(urs, section)
print(f"Query Result: {result}")

print("Script completed")