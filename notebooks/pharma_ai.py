# -*- coding: utf-8 -*-
"""pharmaAI (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10kgzNgk5Pg04087uTJM1aWkZ14m2l835
"""

!pip install -U torch torchvision --no-cache-dir
!pip install numpy google-colab huggingface_hub python-docx fastapi pyngrok uvicorn nest_asyncio sentence-transformers faiss-cpu datasets --no-cache-dir
!pip install -U bitsandbytes transformers

import os
import re
import gc
import torch
import numpy as np
from google.colab import drive
from huggingface_hub import login, snapshot_download
from docx import Document
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from fastapi import FastAPI
from pyngrok import ngrok
import uvicorn
import nest_asyncio
from sentence_transformers import SentenceTransformer
import faiss
from typing import List
from google.colab import userdata
import json

# Mount Google Drive
drive.mount('/content/drive')
nest_asyncio.apply()

# Configuration Paths
KB_PATH = '/content/drive/MyDrive/lifesciences/training_documents/'
DRIVE_MODEL_PATH = '/content/drive/MyDrive/lifesciences/models/'
FAISS_INDEX_PATH = "/content/drive/MyDrive/lifesciences/faiss_index/faiss_index.index"
MODEL_REPO_ID = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

# Constants
CHUNK_SIZE = 512
CHUNK_OVERLAP = 50
SIMILARITY_THRESHOLD = 0.65
MAX_CONTEXT_LENGTH = 3000
DEFAULT_CLUSTERS = 256  # Default number of clusters

# ----------------------------
# RAG System Class
# ----------------------------
class RAGSystem:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.embedding_model = None
        self.index = None
        self.chunks = []

    def initialize(self):
        """One-time initialization of all components"""
        print("Initializing RAG System...")
        try:
            self._auth_huggingface()
            model_path = self._download_model()
            self._load_llm(model_path)
            self._load_embedding_model()

            if os.path.exists(FAISS_INDEX_PATH):
                self.load_faiss_index(FAISS_INDEX_PATH)
                if self.index and self.index.ntotal == 0:
                    print("FAISS index is empty. Recreating index.")
                    self._process_knowledge_base()
            else:
                print("No existing FAISS index found. Processing knowledge base.")
                self._process_knowledge_base()
        except Exception as e:
            print(f"Error during initialization: {str(e)}")

    def _auth_huggingface(self):
        """Authenticate with Hugging Face"""
        try:
            login(token=userdata.get("HF_TOKEN"))
            print("Hugging Face authentication successful.")
        except Exception as e:
            print(f"Authentication failed: {str(e)}")

    def _download_model(self):
        """Download model from Hugging Face"""
        model_folder = os.path.join(DRIVE_MODEL_PATH, MODEL_REPO_ID.split('/')[-1])
        os.makedirs(model_folder, exist_ok=True)
        model_path = snapshot_download(
            repo_id=MODEL_REPO_ID,
            cache_dir=model_folder,
            local_dir=model_folder,
            local_dir_use_symlinks=False
        )
        return model_path

    def _load_llm(self, model_path):
        """Load LLM with quantization"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True, quantization_config=bnb_config
        )

    def _load_embedding_model(self):
        """Load sentence transformer model"""
        self.embedding_model = SentenceTransformer('thenlper/gte-base')

    def _process_knowledge_base(self):
        """Process documents and create FAISS index"""
        print("🔍 Processing knowledge base...")

        # Get all document files
        doc_files = self._get_docx_files()
        print(f"📂 Found {len(doc_files)} DOCX files in {KB_PATH}")

        if not doc_files:
            print("⚠️ No documents found! Please check the knowledge base directory.")
            return

        # Chunk documents and verify
        self.chunks = self._chunk_documents()
        print(f"📄 Created {len(self.chunks)} document chunks.")

        if len(self.chunks) == 0:
            print("❌ No chunks were generated! Something is wrong with chunking.")
            return

        # Generate embeddings and create FAISS index
        embeddings = self._create_embeddings(self.chunks)

        if embeddings is None or embeddings.shape[0] == 0:
            print("❌ No embeddings were created. Check document chunking!")
            return

        print(f"✅ Generated {embeddings.shape[0]} embeddings.")
        self._create_faiss_index(embeddings)



    def _chunk_documents(self) -> List[dict]:
        """Split documents into meaningful chunks while preserving section headings."""
        print("📄 Chunking documents...")
        chunks = []
        doc_paths = self._get_docx_files()

        for doc_path in doc_paths:
            section_data = self._read_docx(doc_path)
            print(f"📖 Processing {len(section_data)} sections from {doc_path}.")

            for section in section_data:
                section_name = section["section"]
                text = section["content"]

                if not text.strip():
                    continue  # Skip empty sections

                # Create larger chunks (e.g., 1000 characters) to preserve context
                section_chunks = [text[i:i + 1000] for i in range(0, len(text), 1000)]

                for chunk in section_chunks:
                    chunk_text = f"Section: {section_name}\n{chunk}"
                    chunks.append({"section": section_name, "content": chunk_text})

        print(f"📄 Total chunks created: {len(chunks)}")
        return chunks


    def _get_docx_files(self):
        """Retrieve DOCX files"""
        return [os.path.join(KB_PATH, f) for f in os.listdir(KB_PATH) if f.endswith('.docx')]

    def _read_docx(self, file_path: str) -> List[dict]:
        """Extract sections, paragraphs, and tables from a DOCX file."""
        print(f"📄 Processing file: {file_path}")

        try:
            doc = Document(file_path)
            content = []
            current_section = "General"  # Default section name
            section_data = []

            for para in doc.paragraphs:
                if para.style and para.style.name.startswith("Heading"):
                    current_section = para.text.strip()  # Store new section heading
                    continue  # Skip adding heading as a chunk

                if para.text.strip():
                    section_data.append({"section": current_section, "content": para.text.strip()})

            # Extract tables and associate with closest section
            for table in doc.tables:
                table_text = []
                for row in table.rows:
                    row_text = "\t".join(cell.text.strip() for cell in row.cells if cell.text.strip())
                    if row_text:
                        table_text.append(row_text)

                if table_text:
                    table_content = "\n".join(table_text)
                    section_data.append({"section": current_section, "content": table_content})

            print(f"📖 Extracted {len(section_data)} sections from {file_path}.")
            return section_data

        except Exception as e:
            print(f"❌ Error reading {file_path}: {str(e)}")
            return []




    def _clean_text(self, text: str) -> str:
        """Clean text"""
        return re.sub(r'\s+', ' ', text).strip()

    def _token_based_chunking(self, text: str) -> List[str]:
        """Token-based chunking"""
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        chunks = [self.tokenizer.decode(tokens[i: i + CHUNK_SIZE], skip_special_tokens=True)
                  for i in range(0, len(tokens), CHUNK_SIZE - CHUNK_OVERLAP)]
        return chunks

    def _create_embeddings(self, chunks: List[str]):
        """Create embeddings"""
        batch_size = 64
        return np.vstack([self.embedding_model.encode(chunks[i:i+batch_size], show_progress_bar=False)
                          for i in range(0, len(chunks), batch_size)])

    def _create_faiss_index(self, embeddings: np.ndarray):
        """Create FAISS index with a fallback to a flat index if too few vectors exist."""
        num_vectors = embeddings.shape[0]
        d = embeddings.shape[1]

        print(f"📊 Creating FAISS index with {num_vectors} vectors.")

        if num_vectors < 10:
            print(f"⚠️ Too few vectors for clustering ({num_vectors}). Using a flat index instead.")
            self.index = faiss.IndexFlatIP(d)  # Use a flat index for small datasets
        else:
            num_clusters = min(3, num_vectors // 2)  # Prevent excessive clustering
            print(f"✅ Using FAISS IVF index with {num_clusters} clusters.")
            self.index = faiss.IndexIVFFlat(faiss.IndexFlatIP(d), d, num_clusters, faiss.METRIC_INNER_PRODUCT)
            self.index.train(embeddings)

        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        self.save_faiss_index(FAISS_INDEX_PATH)



    def save_faiss_index(self, index_path: str):
        """Save FAISS index"""
        faiss.write_index(self.index, index_path)

    def load_faiss_index(self, index_path: str):
        """Load FAISS index"""
        self.index = faiss.read_index(index_path)
        print(f"Loaded FAISS index with {self.index.ntotal} vectors.")

    def _find_similar_documents(self, query: str) -> str:
        """Retrieve the most relevant section and return its corresponding table or text."""
        print("🔍 Searching for relevant section...")

        try:
            if not self.index or self.index.ntotal == 0:
                print("⚠️ FAISS index is empty! No documents found.")
                return "No relevant documents found."

            print(f"📄 Total chunks available: {len(self.chunks)}")

            # Encode query
            query_embedding = self.embedding_model.encode([query])
            query_embedding = np.array(query_embedding).astype(np.float32).reshape(1, -1)

            # Normalize Query Embedding
            faiss.normalize_L2(query_embedding)
            print(f"🔄 Normalized query embedding shape: {query_embedding.shape}")

            # Search FAISS
            distances, indices = self.index.search(query_embedding, k=5)

            print(f"🔍 FAISS search distances: {distances}")
            print(f"📌 FAISS returned indices: {indices[0]}")

            # Filter out invalid results and low-confidence matches
            valid_results = [(i, d) for i, d in zip(indices[0], distances[0]) if i != -1 and d > SIMILARITY_THRESHOLD]

            if not valid_results:
                print("❌ FAISS returned no valid results.")
                return "No relevant documents found."

            # Sort by highest similarity score
            valid_results.sort(key=lambda x: x[1], reverse=True)

            # Retrieve content from the top-scoring chunks
            retrieved_chunks = [self.chunks[i]["content"] for i, _ in valid_results]
            context = "\n\n".join(retrieved_chunks)

            print(f"📄 Retrieved context length: {len(context)} characters.")
            return context if context.strip() else "No relevant documents found."

        except Exception as e:
            print(f"❌ Error in _find_similar_documents: {str(e)}")
            return "Error while searching for documents."




    def generate_response(self, query: str, context: str) -> str:
        """Generate response using LLM with controlled input length."""
        print("📝 Generating response...")

        if "No relevant documents found" in context or len(context) < 50:
            return "⚠️ Sorry, no relevant documents were found in the knowledge base."

        try:
            full_input = f"Query: {query}\nContext: {context}\nResponse:"
            tokenized_input = self.tokenizer(full_input, return_tensors="pt", truncation=True, padding=True)

            # Truncate input if it's too long
            if tokenized_input["input_ids"].shape[1] > 512:
                print(f"⚠️ Input too long ({tokenized_input['input_ids'].shape[1]} tokens). Truncating...")
                full_input = full_input[:3000]  # Trim the context
                tokenized_input = self.tokenizer(full_input, return_tensors="pt", truncation=True, padding=True)

            inputs = tokenized_input.to(self.model.device)

            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,  # Generate up to 256 new tokens
                num_beams=5,  # Beam search for accuracy
                temperature=0.7,  # Slight randomness for better responses
                top_k=50,
                top_p=0.95
            )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return response

        except Exception as e:
            print(f"❌ Error generating response: {str(e)}")
            return "⚠️ Unable to generate response."



app = FastAPI()

@app.on_event("startup")
async def startup_event():
    rag_system.initialize()
    ngrok.set_auth_token(userdata.get("ngrok_auth_token"))
    public_url = ngrok.connect(8000)
    print(f"API available at: {public_url}")

@app.post("/query")
async def handle_query(query: str):
    context = rag_system._find_similar_documents(query)
    response = rag_system.generate_response(query, context) if context else "No relevant documents found."
    return {"response": response}

if __name__ == "__main__":
    rag_system = RAGSystem()
    uvicorn.run(app, host="0.0.0.0", port=8000)