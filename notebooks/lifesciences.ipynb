{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3bfcbc5857cb4e0da339a013107c71d6",
            "f2077101c2b0474e8d9854bd8b8dbd16",
            "fb7d89410e0e4a9f810204908d5b9619",
            "91e4ab718094463cbbf541cca6158733",
            "723d284fd6654efa88b0e998273b68c3",
            "97a0d9b7964940198d97f53b9e8067b6",
            "94e45cffa93442a3a16ab63c3ec7c3c8",
            "24e2f48865ad44be8a8fea39e1fb8af0",
            "b6f04e482f8745529fa8309d4af37b0d",
            "f6853571a65d43d0bc2ab9cff9ac81e9",
            "05683d56b4d14994ae279a0148c85ae7"
          ]
        },
        "id": "eZcEuKbIo8TV",
        "outputId": "098971ea-1f9d-4590-e437-505e2a3ab1ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-74ef45f30837>:184: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n",
            "INFO:     Started server process [61004]\n",
            "INFO:     Waiting for application startup.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastAPI app is running at: NgrokTunnel: \"https://f824-34-105-12-8.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:832: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bfcbc5857cb4e0da339a013107c71d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table of Contents\n",
            "1 General Requirements\n",
            "1.1 Utilites\n",
            "1.2 Facility Requirements\n",
            "1.3 Safety Requirements\n",
            "1.4 Environmental Requirements\n",
            "1.5 Calibration Requirements\n",
            "1.6 Documentation\n",
            "1.7 Training Requirements\n",
            "1.8 Vendor and Warranty Specifications\n",
            "2 Automation\n",
            "2.1 Platform and Networking Requirements\n",
            "2.2 Hardware Requirements\n",
            "2.3 Software Requirements\n",
            "2.4 System Performance Requirements\n",
            "2.5 Historical Data Trending\n",
            "2.6 Alarm and Events\n",
            "2.7 Operational Requirements\n",
            "2.8 User Roles and Access Requirements\n",
            "2.9 Password Rules\n",
            "2.10 Time Synchronization\n",
            "2.11 Security Requirements\n",
            "2.12 Anti Virus and Patching\n",
            "2.13 Electronic Signatures (21 CFR 11)\n",
            "2.14 Electronic Records (21 CFR 11) - Audit Trail \n",
            "2.15 Electronic Records (21 CFR 11) - Data Retention\n",
            "2.16 Electronic Records (21 CFR 11) - Backup and Disaster Recovery\n",
            "2.17 Interface with Third Party Systems\n",
            "2.18 Version Control\n",
            "2.19 Environmental Requirements\n",
            "2.20 Documentation Requirements\n",
            "2.21 Training Requirements\n",
            "2.22 Test System Requirements\n",
            "3 Process Control\n",
            "3.1 Temperature Control\n",
            "3.2 Pressure Control\n",
            "3.3 pH Control\n",
            "4 Vessels\n",
            "4.1 General Requirements\n",
            "4.2 Design and Dimensional Requirements\n",
            "4.3 Material of Construction\n",
            "4.4 Construction Compliance\n",
            "4.5 Temperature and Pressure Controls\n",
            "4.6 Agitation and Mixing Requirements\n",
            "4.7 Sealing and Closure Mechanisms\n",
            "4.8 Safety Features and Protocols\n",
            "4.9 Instrumentation and Monitoring\n",
            "4.10 Cleaning and Maintenance\n",
            "4.11 Operational Requirements\n",
            "4.12 Regulatory and Compliance Standards\n",
            "4.13 Documentation and Record-Keeping\n",
            "4.14 Training Requirements\n",
            "4.15 Vendor and Warranty Specifications\n",
            "4.16 Surface Finish Requirements\n",
            "4.17 Structural Integrity\n",
            "5 Machine Safety\n",
            "5.1 Emergency Stop Systems\n",
            "5.2 Safety Doors and Guards\n",
            "5.3 Light Gates and Safety Sensors\n",
            "5.4 Safety Control Systems\n",
            "5.5 Operator Training and Signage\n",
            "5.6 Regulatory Compliance\n",
            "6 Process Safety\n",
            "6.1 Reactor Design and Safety Features\n",
            "6.2 Chemical Hazard Management\n",
            "6.3 Process Monitoring and Control\n",
            "6.4 Safety in Chemical Reactions\n",
            "6.5 Compliance and Standards\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     95.223.75.30:0 - \"POST /query?query=%22The%20SCADA%20system%20should%20support%20quality%20control%20%22 HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-01-29T19:53:28+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-ab7c844a-2466-42a9-8eff-6bd3e356119e acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-01-29T19:53:28+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8000-ab7c844a-2466-42a9-8eff-6bd3e356119e err=\"failed to start tunnel: session closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-01-29T19:53:28+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-2d635bf9-c05e-4795-8ba5-70ed85e33526 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [61004]\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login, snapshot_download\n",
        "from docx import Document\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from fastapi import FastAPI\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from typing import List\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Configuration\n",
        "KB_PATH = '/content/drive/MyDrive/lifesciences/training_documents/'\n",
        "DRIVE_MODEL_PATH = '/content/drive/MyDrive/lifesciences/models/'\n",
        "MODEL_REPO_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Constants\n",
        "CHUNK_SIZE = 512  # In tokens\n",
        "CHUNK_OVERLAP = 50\n",
        "SIMILARITY_THRESHOLD = 0.65\n",
        "MAX_CONTEXT_LENGTH = 3000\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# ----------------------------\n",
        "# Initialization (Run once)\n",
        "# ----------------------------\n",
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.embedding_model = None\n",
        "        self.index = None\n",
        "        self.chunks = []\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"One-time initialization of all components\"\"\"\n",
        "        self._auth_huggingface()\n",
        "        model_path = self._download_model()\n",
        "        self._load_llm(model_path)\n",
        "        self._load_embedding_model()\n",
        "        self._process_knowledge_base()\n",
        "\n",
        "    def _auth_huggingface(self):\n",
        "        \"\"\"Authenticate with Hugging Face\"\"\"\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            login(token=userdata.get(\"HF_TOKEN\"))\n",
        "            logging.info(\"Hugging Face authentication successful.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Authentication failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _download_model(self):\n",
        "        \"\"\"Download model from Hugging Face Hub\"\"\"\n",
        "        try:\n",
        "            model_path = snapshot_download(\n",
        "                repo_id=MODEL_REPO_ID,\n",
        "                cache_dir=DRIVE_MODEL_PATH,\n",
        "                revision=\"main\",\n",
        "                ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\"],\n",
        "                local_dir=DRIVE_MODEL_PATH,\n",
        "                local_dir_use_symlinks=False\n",
        "            )\n",
        "            logging.info(f\"Model downloaded to {model_path}\")\n",
        "            return model_path\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Model download failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _load_llm(self, model_path):\n",
        "        \"\"\"Load LLM with quantization\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "            bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.float16,\n",
        "                trust_remote_code=True,\n",
        "                quantization_config=bnb_config\n",
        "            )\n",
        "            logging.info(\"LLM loaded successfully\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Model loading failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _load_embedding_model(self):\n",
        "        \"\"\"Load sentence transformer model\"\"\"\n",
        "        self.embedding_model = SentenceTransformer('thenlper/gte-base')\n",
        "        logging.info(\"Embedding model loaded\")\n",
        "\n",
        "    def _process_knowledge_base(self):\n",
        "        \"\"\"Process documents and create FAISS index\"\"\"\n",
        "        self.chunks = self._chunk_documents()\n",
        "        embeddings = self._create_embeddings(self.chunks)\n",
        "        self._create_faiss_index(embeddings)\n",
        "        logging.info(f\"Processed {len(self.chunks)} knowledge chunks\")\n",
        "\n",
        "    def _chunk_documents(self) -> List[str]:\n",
        "        \"\"\"Improved document chunking with text cleaning\"\"\"\n",
        "        chunks = []\n",
        "        for doc_path in self._get_docx_files():\n",
        "            content = self._read_docx(doc_path)\n",
        "            print(content)\n",
        "            content = self._clean_text(content)\n",
        "            chunks.extend(self._token_based_chunking(content))\n",
        "        return chunks\n",
        "\n",
        "    def _get_docx_files(self):\n",
        "        \"\"\"Get all DOCX files from knowledge base path\"\"\"\n",
        "        return [os.path.join(KB_PATH, f) for f in os.listdir(KB_PATH) if f.endswith('.docx')]\n",
        "\n",
        "    def _read_docx(self, file_path: str) -> str:\n",
        "        \"\"\"Read DOCX file with error handling\"\"\"\n",
        "        try:\n",
        "            doc = Document(file_path)\n",
        "            return \"\\n\".join(p.text for p in doc.paragraphs if p.text.strip())\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error reading {file_path}: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace\n",
        "        text = re.sub(r'\\u200b', '', text)  # Remove zero-width spaces\n",
        "        return text.strip()\n",
        "\n",
        "    def _token_based_chunking(self, text: str) -> List[str]:\n",
        "        \"\"\"Token-aware text chunking with overlap\"\"\"\n",
        "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(tokens):\n",
        "            end = min(start + CHUNK_SIZE, len(tokens))\n",
        "            chunk = tokens[start:end]\n",
        "            chunks.append(self.tokenizer.decode(chunk, skip_special_tokens=True))\n",
        "            start = end - CHUNK_OVERLAP if end - CHUNK_OVERLAP > start else end\n",
        "        return chunks\n",
        "\n",
        "    def _create_embeddings(self, chunks: List[str]):\n",
        "        \"\"\"Create embeddings with batching and memory management\"\"\"\n",
        "        embeddings = []\n",
        "        batch_size = 64\n",
        "        for i in range(0, len(chunks), batch_size):\n",
        "            batch = chunks[i:i+batch_size]\n",
        "            emb = self.embedding_model.encode(batch, show_progress_bar=False)\n",
        "            embeddings.append(emb)\n",
        "            del batch\n",
        "            gc.collect()\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def _create_faiss_index(self, embeddings: np.ndarray):\n",
        "        \"\"\"Create optimized FAISS index\"\"\"\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)  # Normalize for cosine similarity\n",
        "        self.index.add(embeddings)\n",
        "        logging.info(f\"FAISS index created with {self.index.ntotal} vectors\")\n",
        "\n",
        "# ----------------------------\n",
        "# FastAPI Application\n",
        "# ----------------------------\n",
        "app = FastAPI()\n",
        "rag_system = RAGSystem()\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    rag_system.initialize()\n",
        "    ngrok.set_auth_token(userdata.get(\"ngrok_auth_token\"))\n",
        "    public_url = ngrok.connect(8000)\n",
        "    logging.info(f\"API available at: {public_url}\")\n",
        "\n",
        "@app.post(\"/query\")\n",
        "async def handle_query(query: str):\n",
        "    try:\n",
        "        if len(query) < 3:\n",
        "            return {\"error\": \"Query too short\"}\n",
        "\n",
        "        context = retrieve_context(query)\n",
        "        response = generate_response(query, context)\n",
        "        return {\"response\": response}\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing query: {str(e)}\")\n",
        "        return {\"error\": \"Processing failed\"}\n",
        "\n",
        "def retrieve_context(query: str) -> str:\n",
        "    \"\"\"Retrieve relevant context from knowledge base\"\"\"\n",
        "    query_embed = rag_system.embedding_model.encode([query], show_progress_bar=False)\n",
        "    faiss.normalize_L2(query_embed)\n",
        "\n",
        "    # Search with score threshold\n",
        "    distances, indices = rag_system.index.search(query_embed, 5)\n",
        "    relevant_chunks = [\n",
        "        rag_system.chunks[i]\n",
        "        for i, score in zip(indices[0], distances[0])\n",
        "        if score > SIMILARITY_THRESHOLD\n",
        "    ]\n",
        "\n",
        "    if not relevant_chunks:\n",
        "        logging.warning(\"No relevant context found\")\n",
        "        return \"\"\n",
        "\n",
        "    return \"\\n\".join(relevant_chunks)[:MAX_CONTEXT_LENGTH]\n",
        "\n",
        "def generate_response(query: str, context: str) -> str:\n",
        "    \"\"\"Generate response using LLM without showing model's 'thinking'\"\"\"\n",
        "    if not context:\n",
        "        return \"I don't have sufficient information to answer that question.\"\n",
        "\n",
        "    # Modified prompt to enforce direct answers\n",
        "    prompt = f\"\"\"Answer the question directly using only the provided context.\n",
        "Do not explain your reasoning or thought process.\n",
        "If unsure, say \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = rag_system.tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=4096,\n",
        "        truncation=True\n",
        "    ).to(rag_system.model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = rag_system.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,  # Reduced to prevent verbose output\n",
        "            temperature=0.2,     # More deterministic\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            do_sample=False,     # Disable creative sampling\n",
        "            eos_token_id=rag_system.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    full_response = rag_system.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Strict answer extraction\n",
        "    return extract_answer(full_response)\n",
        "\n",
        "def extract_answer(full_response: str) -> str:\n",
        "    \"\"\"Forcefully extract only the answer portion\"\"\"\n",
        "    # Split on the last \"Answer:\" occurrence\n",
        "    if \"Answer:\" in full_response:\n",
        "        return full_response.rsplit(\"Answer:\", 1)[-1].strip()\n",
        "\n",
        "    # Fallback patterns\n",
        "    patterns = [\n",
        "        r\"(?i)(?:final answer|response):\\s*(.*)\",\n",
        "        r\"(?i)here(?:'s| is) (?:the|my) (?:answer|response):\\s*(.*)\",\n",
        "        r\"^(.*?)(?:Note:|Explanation:|Reasoning:)\",  # Stop at unwanted sections\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, full_response, re.DOTALL)\n",
        "        if match:\n",
        "            answer = match.group(1).strip()\n",
        "            # Remove any markdown formatting\n",
        "            answer = re.sub(r\"\\*\\*|__|```\", \"\", answer)\n",
        "            return re.split(r\"[\\.\\?!]\\s\", answer)[0] + \".\"  # Take first sentence\n",
        "\n",
        "    # Final fallback - take first 2 sentences\n",
        "    sentences = re.split(r\"(?<=[.!?])\\s+\", full_response.strip())\n",
        "    return \" \".join(sentences[:2]).strip()\n",
        "\n",
        "def start_ngrok():\n",
        "    ngrok.set_auth_token(userdata.get(\"ngrok_auth_token\"))\n",
        "    public_url = ngrok.connect(8000)\n",
        "    logging.info(f\"FastAPI app is running at: {public_url}\")\n",
        "    print(f\"FastAPI app is running at: {public_url}\")\n",
        "    nest_asyncio.apply()\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# ----------------------------\n",
        "# Main Execution\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    start_ngrok()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zTvv3Jix36hn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05683d56b4d14994ae279a0148c85ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24e2f48865ad44be8a8fea39e1fb8af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bfcbc5857cb4e0da339a013107c71d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2077101c2b0474e8d9854bd8b8dbd16",
              "IPY_MODEL_fb7d89410e0e4a9f810204908d5b9619",
              "IPY_MODEL_91e4ab718094463cbbf541cca6158733"
            ],
            "layout": "IPY_MODEL_723d284fd6654efa88b0e998273b68c3"
          }
        },
        "723d284fd6654efa88b0e998273b68c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e4ab718094463cbbf541cca6158733": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6853571a65d43d0bc2ab9cff9ac81e9",
            "placeholder": "​",
            "style": "IPY_MODEL_05683d56b4d14994ae279a0148c85ae7",
            "value": " 9/9 [00:00&lt;00:00,  9.40it/s]"
          }
        },
        "94e45cffa93442a3a16ab63c3ec7c3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97a0d9b7964940198d97f53b9e8067b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6f04e482f8745529fa8309d4af37b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2077101c2b0474e8d9854bd8b8dbd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97a0d9b7964940198d97f53b9e8067b6",
            "placeholder": "​",
            "style": "IPY_MODEL_94e45cffa93442a3a16ab63c3ec7c3c8",
            "value": "Fetching 9 files: 100%"
          }
        },
        "f6853571a65d43d0bc2ab9cff9ac81e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb7d89410e0e4a9f810204908d5b9619": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24e2f48865ad44be8a8fea39e1fb8af0",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6f04e482f8745529fa8309d4af37b0d",
            "value": 9
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
