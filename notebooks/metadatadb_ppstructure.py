# -*- coding: utf-8 -*-
"""metadatadb_ppstructure.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11slh4u9PXCiykhkle_Jz58BHBKgH_iE_

Install Dependencies
"""

!apt-get install ghostscript
!pip install camelot-py[cv]
!pip install google-colab python-docx pdf2image paddlepaddle paddleocr psycopg2-binary faiss-cpu numpy PyPDF2 tk
!apt-get install poppler-utils -y
!pip install psutil
!apt-get install -y libreoffice
!pip install pyuno

"""Setup and Initialization"""

import os
from google.colab import drive
import psutil
import logging
import subprocess
import time
import shutil

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger()

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)
base_path = '/content/drive/My Drive/lifesciences'
training_docs_path = os.path.join(base_path, 'training_documents')
staging_images_path = os.path.join(base_path, 'staging/images')
staging_pdfs_path = os.path.join(base_path, 'staging/pdfs')
processed_faiss_path = os.path.join(base_path, 'processed/content_faiss.index')

# Ensure directories exist
os.makedirs(staging_images_path, exist_ok=True)
os.makedirs(staging_pdfs_path, exist_ok=True)
os.makedirs(os.path.dirname(processed_faiss_path), exist_ok=True)

# Function to log memory usage
def log_memory(step=""):
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    mem_usage = mem_info.rss / 1024**2
    print(f"{step} - Memory usage: RSS (Resident Set Size= curreny RAM usage)={mem_usage:.2f} MB, VMS(Virtual Memory Size = RAM+Disk)={mem_info.vms / 1024**2:.2f} MB")
    if mem_usage > 10000:
        logger.warning("Memory usage exceeds 10 GB, potential crash risk")

log_memory("After setup")

"""Convert DOCX to PDF"""

def docx_to_pdf(docx_path, staging_pdfs_path):
    pdf_filename = os.path.basename(docx_path).replace('.docx', '.pdf')
    pdf_path = os.path.join(staging_pdfs_path, pdf_filename)
    print(f"Attempting to convert {docx_path} to {pdf_path}")

    local_docx = f"/tmp/{os.path.basename(docx_path)}"
    shutil.copy(docx_path, local_docx)

    cmd = f"libreoffice --headless --convert-to pdf \"{local_docx}\" --outdir \"{staging_pdfs_path}\""
    print(f"Running command: {cmd}")
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    print(f"LibreOffice conversion stdout: {result.stdout}")
    print(f"LibreOffice conversion stderr: {result.stderr}")

    time.sleep(5)

    if not os.path.exists(pdf_path):
        logger.error(f"Conversion failed: {pdf_path} not found")
        raise FileNotFoundError(f"PDF not found after conversion: {pdf_path}")

    print(f"Conversion successful: {pdf_path} created")
    os.remove(local_docx)
    log_memory("After conversion")
    return pdf_path

# Process one file
filename = "Text_Only_URS_Analytical_Instrument_Data_Management_System.docx"
file_path = os.path.join(training_docs_path, filename)
urs_name = input(f"Enter URS name for {filename} (converted to PDF): ")
staged_pdf_path = docx_to_pdf(file_path, staging_pdfs_path)

"""Initialize OCR and Process PDF"""

import os
import gc
import numpy as np
import psutil
import psycopg2
import faiss
import pdf2image
from pdf2image import convert_from_path
from paddleocr import PPStructure
import camelot
from google.colab import userdata

def log_memory(step=""):
    """Track and print memory usage at different steps."""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    mem_usage = mem_info.rss / 1024**2
    print(f"{step} - Memory usage: RSS={mem_usage:.2f} MB, VMS={mem_info.vms / 1024**2:.2f} MB")
    if mem_usage > 10000:
        print("WARNING: Memory usage exceeds 10 GB, potential crash risk")

def setup_database(db_params):
    """Setup database connection and tables."""
    print("Connecting to database...")
    conn = psycopg2.connect(**db_params)
    cursor = conn.cursor()

    print("Creating metadata table if needed...")
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS metadata (
            id SERIAL PRIMARY KEY,
            urs VARCHAR(255) NOT NULL,
            section VARCHAR(255),
            type VARCHAR(50) NOT NULL,
            content TEXT NOT NULL,
            coordinates VARCHAR(100),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            CONSTRAINT valid_type CHECK (type IN ('title', 'text', 'list', 'table', 'figure'))
        );
    """)
    conn.commit()
    print("Database setup complete")

    # Get current max ID
    cursor.execute("SELECT COALESCE(MAX(id), 0) FROM metadata")
    id_seq = cursor.fetchone()[0]
    print(f"Starting with database ID: {id_seq}")

    return conn, cursor, id_seq

def extract_text_content(item):
    """Extract content from text items."""
    print("Extracting text content...")
    content = ""

    if isinstance(item['res'], list):
        content_list = []
        for sub_item in item['res']:
            try:
                if isinstance(sub_item, dict) and 'text' in sub_item:
                    content_list.append(sub_item['text'])
                elif isinstance(sub_item, str):
                    content_list.append(sub_item)
            except Exception as e:
                print(f"Error with text item: {e}")
        content = '\n'.join(content_list)
    elif isinstance(item['res'], dict) and 'text' in item['res']:
        content = item['res']['text']
    elif isinstance(item['res'], str):
        content = item['res']
    else:
        content = item.get('res', {}).get('text', '')

    return content

def extract_list_content(item):
    """Extract content from list items."""
    print("Extracting list content...")
    content = ""

    if 'res' in item:
        if isinstance(item['res'], list):
            content_list = []
            for sub_item in item['res']:
                try:
                    if isinstance(sub_item, dict) and 'text' in sub_item:
                        content_list.append(sub_item['text'])
                    elif isinstance(sub_item, str):
                        content_list.append(sub_item)
                except Exception as e:
                    print(f"Error with list item: {e}")
            content = '\n'.join(content_list)
        elif isinstance(item['res'], dict) and 'text' in item['res']:
            content = item['res']['text']
        elif isinstance(item['res'], str):
            content = item['res']
        else:
            content = "List content could not be extracted."
    else:
        content = "List content not found."

    return content

def extract_table_content(item):
    """Extract content from table items."""
    print("Extracting table content...")

    if 'res' in item:
        if 'html' in item['res']:
            return item['res']['html']
        elif 'cells' in item['res']:
            return '\n'.join([' | '.join(cell.get('text', '') for cell in row)
                             for row in item['res']['cells']])
        else:
            return item['res'].get('text', 'Table detected')
    return "Table content not found"

def save_to_database(cursor, conn, id_seq, urs_name, section, item_type, content, coordinates):
    """Save content to database."""
    cursor.execute("""
        INSERT INTO metadata (id, urs, section, type, content, coordinates)
        VALUES (%s, %s, %s, %s, %s, %s)
    """, (id_seq, urs_name, section, item_type, content, str(coordinates)))
    conn.commit()
    print(f"Saved {item_type} to database, ID: {id_seq}")
    return id_seq + 1

def process_page(page_num, pdf_path, structure, staging_path, current_section,
                cursor, conn, id_seq, urs_name, faiss_index):
    """Process a single PDF page."""
    print(f"Processing page {page_num+1}")
    try:
        # Convert PDF to image
        images = convert_from_path(pdf_path, dpi=100, first_page=page_num+1, last_page=page_num+1)
        image = images[0]
        image_path = os.path.join(staging_path, f"page_{page_num}.png")
        print(f"Saving image to: {image_path}")
        image.save(image_path, 'PNG')

        if not os.path.exists(image_path):
            print(f"ERROR: Image save failed for page {page_num+1}")
            return current_section, id_seq

        # Run layout analysis
        print(f"Analyzing page {page_num+1} layout...")
        layout_result = structure(image_path)
        print(f"Found {len(layout_result)} elements on page {page_num+1}")
        layout_result = sorted(layout_result, key=lambda x: x['bbox'][1])
        # Process each element
        for item in layout_result:
            item_type = item['type'].lower()
            print(f"Processing {item_type} element")

            coordinates = item.get('bbox', [0, 0, 0, 0])
            content = ""

            # Extract content based on type
            if item_type == 'title':
                content = item['res'][0].get('text', 'Unknown Title')
                current_section = content
                print(f"New section: {current_section}")

            elif item_type == 'text':
                content = extract_text_content(item)

            elif item_type == 'list':
                content = extract_list_content(item)

            elif item_type == 'table':
                content = extract_table_content(item)

            elif item_type == 'figure':
                content = item.get('res', {}).get('text', 'Figure detected')

            # Skip empty content
            if not content.strip():
                print("Skipping empty content")
                continue

            # Save to database
            id_seq = save_to_database(cursor, conn, id_seq, urs_name,
                                      current_section, item_type, content, coordinates)

            # Add to FAISS index
            embedding = np.random.rand(64).astype('float32')
            faiss_index.add(np.array([embedding]))

        # Clean up
        del images, image, layout_result
        gc.collect()
        log_memory(f"Completed page {page_num+1}")

        return current_section, id_seq

    except Exception as e:
        print(f"ERROR processing page {page_num+1}: {e}")
        return current_section, id_seq

def process_pdf(pdf_path, urs_name, staging_images_path, processed_faiss_path, db_params, max_pages=10):
    """Main PDF processing function."""
    print(f"Starting to process PDF: {pdf_path}")

    # Initialize components
    print("Initializing PP-Structure...")
    structure = PPStructure(table=True, ocr=True, use_gpu=True, layout=True,
                           layout_score_threshold=0.1, layout_nms_threshold=0.3, lang='en')
    log_memory("PP-Structure initialized")

    # Setup database
    conn, cursor, id_seq = setup_database(db_params)

    # Initialize FAISS
    print("Initializing FAISS index...")
    faiss_index = faiss.IndexFlatL2(64)

    # Create staging directory
    if not os.path.exists(staging_images_path):
        print(f"Creating directory: {staging_images_path}")
        os.makedirs(staging_images_path)


    print(f"Camelot attempting to detect tables at the path...{pdf_path}")

    # First, try extracting with the 'stream' flavor
    tables_stream = camelot.read_pdf(pdf_path, flavor="stream", strip_text="\n", row_tol=10)
    valid_tables_stream = [table for table in tables_stream if table.df.shape[1] >= 2]

    # Then, try extracting with the 'lattice' flavor
    tables_lattice = camelot.read_pdf(pdf_path, flavor="lattice", line_scale=40)


    # Iterate over the combined tables and print the page number where a table is detected
    for idx, table in enumerate(valid_tables_stream):
        print(f"Camlot Table Stream detected on page {table.page}")
        print(table.df.to_string(index=False, header=True))
        camelot.plot(table, kind="contour").show()
        print("\n")

    for idx, table in enumerate(tables_lattice):
      print(f"Camlot Table Lattice detected on page {table.page}")
      camelot.plot(table, kind="contour").show()
      print("\n")


    # Get PDF info
    print("Getting PDF information...")
    pdf_info = pdf2image.pdfinfo_from_path(pdf_path)
    total_pages = pdf_info["Pages"]
    print(f"PDF has {total_pages} pages")

    pages_to_process = min(total_pages, max_pages) if max_pages else total_pages
    print(f"Will process {pages_to_process} pages")

    # Process pages
    current_section = ''
    for page in range(pages_to_process):
        current_section, id_seq = process_page(
            page, pdf_path, structure, staging_images_path,
            current_section, cursor, conn, id_seq, urs_name, faiss_index
        )

    # Save FAISS index
    faiss_dir = os.path.dirname(processed_faiss_path)
    if not os.path.exists(faiss_dir):
        print(f"Creating directory: {faiss_dir}")
        os.makedirs(faiss_dir)

    print(f"Saving FAISS index to: {processed_faiss_path}")
    faiss.write_index(faiss_index, processed_faiss_path)
    print("FAISS index saved successfully")

    # Close database
    cursor.close()
    conn.close()
    print("Database connection closed")

    print(f"PDF processing complete for {urs_name}")

# Main execution
if __name__ == "__main__":
    # Setup configuration
    db_params = {
        'dbname': os.environ.get('DB_NAME', userdata.get('DB_NAME')),
        'user': os.environ.get('DB_USER', userdata.get('DB_USER')),
        'password': os.environ.get('DB_PASSWORD',  userdata.get('DB_PASSWORD')),
        'host': os.environ.get('DB_HOST', userdata.get('DB_HOST')),
        'port': os.environ.get('DB_PORT', userdata.get('DB_PORT'))
    }

    # Get variable values - these were assumed to be defined in the original code


    processed_faiss_path = os.environ.get('PROCESSED_FAISS_PATH',
                                          '/content/drive/My Drive/lifesciences/processed/faiss_index.bin')

    # Print configuration
    print("Configuration:")
    print(f"- PDF path: {staging_pdfs_path}")
    print(f"- URS name: {urs_name}")
    print(f"- Images path: {staging_images_path}")
    print(f"- FAISS index path: {processed_faiss_path}")

    # Run processing
    process_pdf(staged_pdf_path, urs_name, staging_images_path, processed_faiss_path, db_params)

"""Visualization"""

def visualize_faiss():
    import matplotlib.pyplot as plt
    from sklearn.decomposition import PCA
    faiss_index = faiss.read_index(processed_faiss_path)
    if faiss_index.ntotal == 0:
        logger.warning("FAISS index is empty, nothing to visualize")
        return
    elif faiss_index.ntotal == 1:
        logger.warning("FAISS index has only 1 entry, visualizing as a single point")
        embeddings = faiss_index.reconstruct_n(0, 1)
        plt.scatter(embeddings[0, 0], embeddings[0, 1], s=100)
        plt.title("FAISS Index Visualization (Single Point)")
        plt.show()
        return

    embeddings = faiss_index.reconstruct_n(0, faiss_index.ntotal)
    pca = PCA(n_components=min(2, faiss_index.ntotal))
    reduced = pca.fit_transform(embeddings)

    plt.scatter(reduced[:, 0], reduced[:, 1] if reduced.shape[1] > 1 else np.zeros(reduced.shape[0]))
    plt.title("FAISS Index Visualization (PCA)")
    plt.show()

print("Visualizing FAISS index")
visualize_faiss()

def query_faiss(urs_name, section_name):
    faiss_index = faiss.read_index(processed_faiss_path)
    if faiss_index.ntotal == 0:
        logger.warning("FAISS index is empty, no query possible")
        return None
    query_embedding = np.random.rand(64).astype('float32')
    D, I = faiss_index.search(np.array([query_embedding]), 1)
    index = int(I[0][0])
    conn, cursor, id_seq = setup_database(db_params)
    cursor.execute("SELECT * FROM metadata WHERE id = %s", (index + 1,))
    result = cursor.fetchone()
    cursor.close()
    conn.close()
    return result

urs = input("Enter URS name to query: ")
section = input("Enter section name to query: ")
result = query_faiss(urs, section)
print(f"Query Result: {result}")

# Cleanup

print("Script completed")