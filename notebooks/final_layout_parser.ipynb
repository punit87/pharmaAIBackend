{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Install System and Python Dependencies"
      ],
      "metadata": {
        "id": "PwAtdX511E_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_dIXLvf0-ke",
        "outputId": "7c72ab33-f194-4f94-a938-131d119feda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.11/dist-packages (3.20.3)\n",
            "Requirement already satisfied: google-api-core==1.34.1 in /usr/local/lib/python3.11/dist-packages (1.34.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core==1.34.1) (1.69.2)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core==1.34.1) (2.38.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core==1.34.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core==1.34.1) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core==1.34.1) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core==1.34.1) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.34.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.34.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.34.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.34.1) (2025.1.31)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core==1.34.1) (0.6.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.4.1\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Torch version: 2.6.0+cu124\n",
            "Layoutparser version: 0.3.4\n",
            "Detectron2 version: 0.6\n",
            "FAISS version: 1.10.0\n",
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install System and Python Dependencies\n",
        "!apt-get update -qq && apt-get install -y libreoffice poppler-utils tesseract-ocr tesseract-ocr-eng -qq\n",
        "\n",
        "!pip install --quiet --no-cache-dir \\\n",
        "    torch torchvision \\\n",
        "    pdf2image opencv-python-headless \\\n",
        "    pytesseract PyPDF2 \\\n",
        "    python-docx \\\n",
        "    pillow \\\n",
        "    cython pycocotools fvcore \\\n",
        "    sentence-transformers \\\n",
        "    fastapi uvicorn pyngrok \\\n",
        "    psycopg2-binary\n",
        "\n",
        "!pip install protobuf==3.20.3 google-api-core==1.34.1\n",
        "!pip install faiss-cpu --upgrade\n",
        "!pip install --quiet -U 'git+https://github.com/facebookresearch/detectron2.git@ff53992b1985b63bd3262b5a36167098e3dada02'\n",
        "!pip install --quiet \"layoutparser[ocr]\"\n",
        "!pip install tensorflow==2.4.1  # Add this line\n",
        "!pip install spacy\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import layoutparser\n",
        "import detectron2\n",
        "import faiss\n",
        "import tensorflow as tf\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Layoutparser version: {layoutparser.__version__}\")\n",
        "print(f\"Detectron2 version: {detectron2.__version__}\")\n",
        "print(f\"FAISS version: {faiss.__version__}\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and Initial Setup"
      ],
      "metadata": {
        "id": "56nPhlVk1JNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and Initial Setup\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import layoutparser as lp\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.model_zoo import model_zoo\n",
        "from pdf2image import convert_from_path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import psycopg2\n",
        "from psycopg2.extras import execute_values\n",
        "from psycopg2.extras import execute_batch\n",
        "from google.colab import drive\n",
        "import hashlib\n",
        "import re\n",
        "import traceback\n",
        "import spacy\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory setup\n",
        "BASE_DIR = Path(\"/content/drive/My Drive/lifesciences\")\n",
        "SOURCE_DIR = BASE_DIR / \"training_documents\"\n",
        "PDF_STAGING_DIR = BASE_DIR / \"staging/pdf\"\n",
        "IMG_STAGING_DIR = BASE_DIR / \"post_label/images\"\n",
        "ANNOTATED_DIR = BASE_DIR / \"staging/annotated\"\n",
        "MODELS_DIR = BASE_DIR / \"models\"\n",
        "RETRAINED_STAGING_DIR = BASE_DIR / \"retrained_staging\"\n",
        "RETRAINED_ANNOTATED_DIR = RETRAINED_STAGING_DIR / \"annotated\"\n",
        "FAISS_BASE_DIR = BASE_DIR / \"faiss_indexes\"\n",
        "COCO_JSON_PATH = BASE_DIR / \"post_label/result.json\"\n",
        "RETRAINED_MODELS_DIR = BASE_DIR / \"retrained_staging/models\"\n",
        "\n",
        "for directory in [SOURCE_DIR, PDF_STAGING_DIR, IMG_STAGING_DIR, ANNOTATED_DIR, MODELS_DIR, RETRAINED_STAGING_DIR, RETRAINED_ANNOTATED_DIR, FAISS_BASE_DIR]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "Image.MAX_IMAGE_PIXELS = 100000000  # Increase max image pixel limit\n",
        "\n",
        "# Database configuration\n",
        "DB_CONFIG = {\n",
        "    \"dbname\": \"neondb\",\n",
        "    \"user\": \"neondb_owner\",\n",
        "    \"password\": \"npg_DU3Vxoi6cCIu\",\n",
        "    \"host\": \"ep-shy-recipe-a406b4wg-pooler.us-east-1.aws.neon.tech\",\n",
        "    \"port\": \"5432\",\n",
        "    \"sslmode\": \"require\"\n",
        "}\n",
        "\n",
        "# Global variables (initialized lazily)\n",
        "sentence_model = None\n",
        "model = None\n",
        "ocr_agent = None\n",
        "COLORS = {\"title\": (255, 0, 0), \"text\": (0, 255, 0), \"list\": (0, 0, 255), \"table\": (255, 255, 0), \"figure\": (255, 0, 255)}\n",
        "LABEL_MAP = {0: \"figure\", 1: \"list\", 2: \"table\", 3: \"text\", 4: \"title\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAxL-FaR1PQ7",
        "outputId": "9c9882dd-006c-45cd-e923-2cd5d9b02e44"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Utility Functions"
      ],
      "metadata": {
        "id": "P6X2vk5q1TKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Utility Functions\n",
        "def setup_db_connection():\n",
        "    conn = psycopg2.connect(**DB_CONFIG)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        INSERT INTO batch_runs (started_at, processed_files, batch_status)\n",
        "        VALUES (%s, 0, 'RUNNING')\n",
        "        RETURNING id\n",
        "    \"\"\", (datetime.now().isoformat(),))\n",
        "    batch_id = cursor.fetchone()[0]\n",
        "    conn.commit()\n",
        "    print(f\"Started batch run: batch_run_id={batch_id}\")\n",
        "    return conn, cursor, batch_id\n",
        "\n",
        "def update_run_status(conn, cursor, batch_id, status, processed_files=0):\n",
        "    try:\n",
        "        cursor.execute(\"\"\"\n",
        "            UPDATE batch_runs\n",
        "            SET completed_at = %s, processed_files = %s, batch_status = %s\n",
        "            WHERE id = %s\n",
        "        \"\"\", (datetime.now().isoformat(), processed_files, status, batch_id))\n",
        "        conn.commit()\n",
        "        print(f\"Updated batch run {batch_id} to {status} with {processed_files} files processed\")\n",
        "    except psycopg2.Error as e:\n",
        "        print(f\"Database error during status update: {e}\")\n",
        "        conn.rollback()\n",
        "        cursor.execute(\"\"\"\n",
        "            UPDATE batch_runs\n",
        "            SET completed_at = %s, processed_files = %s, batch_status = %s\n",
        "            WHERE id = %s\n",
        "        \"\"\", (datetime.now().isoformat(), processed_files, status, batch_id))\n",
        "        conn.commit()\n",
        "        print(f\"Retried and updated batch run {batch_id} to {status} after rollback\")\n",
        "\n",
        "def preprocess_name_for_hash(name):\n",
        "    name = re.sub(r'^\\d+\\s*', '', name).strip()\n",
        "    return re.sub(r'[^a-z0-9]', '', name.lower())\n",
        "\n",
        "def preprocess_name_for_insert(name):\n",
        "    return re.sub(r'^\\d+\\s*', '', name).strip()\n",
        "\n",
        "def get_faiss_index_path(urs_name):\n",
        "    hashed_name = hashlib.md5(urs_name.encode()).hexdigest()\n",
        "    return FAISS_BASE_DIR / f\"{hashed_name}.index\""
      ],
      "metadata": {
        "id": "lIs1FM111XZA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Model Training Function"
      ],
      "metadata": {
        "id": "vyxEtvrz1aW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Model Training Function\n",
        "def train_layoutparser_model(json_path, image_dir):\n",
        "    dataset_name = \"custom_layout_dataset\"\n",
        "    if dataset_name in DatasetCatalog:\n",
        "        DatasetCatalog.remove(dataset_name)\n",
        "        MetadataCatalog.remove(dataset_name)\n",
        "    register_coco_instances(dataset_name, {}, str(json_path), image_dir)\n",
        "    MetadataCatalog.get(dataset_name).thing_classes = list(LABEL_MAP.values())\n",
        "\n",
        "    cfg = get_cfg()\n",
        "    pretrained_model_path = MODELS_DIR / \"model_final.pth\"\n",
        "    if pretrained_model_path.exists():\n",
        "        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "        cfg.MODEL.WEIGHTS = str(pretrained_model_path)\n",
        "    else:\n",
        "        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
        "\n",
        "    cfg.MODEL.DEVICE = str(device)\n",
        "    cfg.DATASETS.TRAIN = (dataset_name,)\n",
        "    cfg.DATASETS.TEST = ()\n",
        "    cfg.DATALOADER.NUM_WORKERS = 2\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "    cfg.SOLVER.BASE_LR = 0.00025\n",
        "    cfg.SOLVER.MAX_ITER = 300\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(LABEL_MAP)\n",
        "    cfg.OUTPUT_DIR = str(RETRAINED_MODELS_DIR)\n",
        "\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "    trainer = DefaultTrainer(cfg)\n",
        "    trainer.resume_or_load(resume=False)\n",
        "    trainer.train()\n",
        "\n",
        "    model_path = Path(cfg.OUTPUT_DIR) / \"model_final.pth\"\n",
        "    config_path = Path(cfg.OUTPUT_DIR) / \"config.yaml\"\n",
        "    with open(config_path, \"w\") as f:\n",
        "        f.write(cfg.dump())\n",
        "\n",
        "    global model\n",
        "    model = lp.Detectron2LayoutModel(\n",
        "        config_path=str(config_path),\n",
        "        model_path=str(model_path),\n",
        "        label_map=LABEL_MAP,\n",
        "        extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.2],\n",
        "        device=str(device)\n",
        "    )\n",
        "    print(f\"Model trained and saved to {model_path}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "eWp3PSc61hd5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: Layout Processing Functions"
      ],
      "metadata": {
        "id": "uqedE9uf1kPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Layout Processing Functions\n",
        "\n",
        "import spacy\n",
        "spacy.cli.download(\"en_core_web_lg\")  # Downloads the model\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")  # You may need to run: python -m spacy download en_core_web_sm\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Insert a period and space before numbers that likely start new sentences\n",
        "    text = re.sub(r'(\\d+)\\s+([A-Z])', r'. \\1 \\2', text)\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "def clean_and_split_sentences(text):\n",
        "    # Preprocess the text to help spaCy detect sentence boundaries\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(preprocessed_text)\n",
        "\n",
        "    # Initialize list for cleaned sentences\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    # Define special characters to remove from the start\n",
        "    special_chars = r'[|!@#$%^&*()_+=[\\]{}:;\"\\'<>,/~`\\\\-]'\n",
        "\n",
        "    # Iterate through detected sentences\n",
        "    for sent in doc.sents:\n",
        "        # Get the sentence text and clean it\n",
        "        sentence = sent.text.strip()\n",
        "\n",
        "        # Replace all '-' with a single space\n",
        "        sentence = sentence.replace('-', ' ')\n",
        "\n",
        "        # Remove leading numbers and special characters\n",
        "        sentence = re.sub(f'^{special_chars}+', '', sentence.strip())\n",
        "        sentence = re.sub(r'^\\d+', '', sentence.strip())\n",
        "\n",
        "        # Remove extra spaces\n",
        "        sentence = ' '.join(sentence.split())\n",
        "\n",
        "        # Skip if sentence is empty\n",
        "        if not sentence:\n",
        "            continue\n",
        "\n",
        "        sentence = sentence.strip()\n",
        "        # Append a full stop if not already present\n",
        "        if sentence[-1] not in '.!?':\n",
        "            sentence = sentence.strip()+'.'\n",
        "\n",
        "        # Filter for meaningful sentences\n",
        "        tokens = nlp(sentence)\n",
        "        has_verb = any(token.pos_ == \"VERB\" for token in tokens)\n",
        "        is_long_enough = len(sentence) > 15\n",
        "\n",
        "        if has_verb and is_long_enough:\n",
        "            cleaned_sentences.append(sentence)\n",
        "\n",
        "    return cleaned_sentences\n",
        "\n",
        "\n",
        "def lazy_load_models():\n",
        "    global model, ocr_agent\n",
        "    if ocr_agent is None:\n",
        "        ocr_agent = lp.TesseractAgent(languages='eng', config='--psm 6 --oem 1')\n",
        "        print(\"OCR agent loaded\")\n",
        "    if model is None:\n",
        "        model_path = RETRAINED_MODELS_DIR / \"model_final.pth\"\n",
        "        config_path = RETRAINED_MODELS_DIR / \"config.yaml\"\n",
        "        if model_path.exists() and config_path.exists():\n",
        "            model = lp.Detectron2LayoutModel(\n",
        "                config_path=str(config_path),\n",
        "                model_path=str(model_path),\n",
        "                label_map=LABEL_MAP,\n",
        "                extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.2],\n",
        "                device=str(device)\n",
        "            )\n",
        "            print(f\"Loaded model from {model_path}\")\n",
        "        else:\n",
        "            raise ValueError(\"Model not found. Train it first.\")\n",
        "\n",
        "def is_overlapping(rect1, rect2):\n",
        "    return not (rect1[2] <= rect2[0] or rect2[2] <= rect1[0] or\n",
        "                rect1[3] <= rect2[1] or rect2[3] <= rect1[1])\n",
        "\n",
        "def process_layout(image_path, urs_name, conn, cursor, batch_id):\n",
        "    global model\n",
        "    lazy_load_models()\n",
        "    if model is None:\n",
        "        raise ValueError(\"Model not loaded.\")\n",
        "\n",
        "    image = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "\n",
        "    layout = model.detect(image)\n",
        "    sorted_layout = sorted(layout, key=lambda x: x.coordinates[1])\n",
        "    skipped_blocks = []\n",
        "    list_table_regions = [(b.type.lower(), list(map(int, b.coordinates))) for b in sorted_layout if b.type.lower() in [\"list\", \"table\"]]\n",
        "    current_section = \"notitle\"\n",
        "    block_number = 1\n",
        "\n",
        "    # Precompute list and table contents\n",
        "    list_table_contents = {}\n",
        "    for block_type, coords in list_table_regions:\n",
        "        cropped = image[coords[1]:coords[3], coords[0]:coords[2]]\n",
        "        content = ocr_agent.detect(cropped) or \"\"\n",
        "        if block_type == \"list\":\n",
        "            content = re.sub(r'[^a-zA-Z0-9\\s]', '', content)\n",
        "        elif block_type == \"table\":\n",
        "            content = '\\n'.join('-'.join(col.strip() for col in row.split() if col.strip()) for row in content.split('\\n') if row.strip()) or \"Table Content\"\n",
        "        list_table_contents[tuple(coords)] = content.strip()\n",
        "        del cropped\n",
        "\n",
        "    # Precompute figure contents\n",
        "    figure_regions = [(b.type.lower(), list(map(int, b.coordinates))) for b in sorted_layout if b.type.lower() == \"figure\"]\n",
        "    for block_type, coords in figure_regions:\n",
        "        if tuple(coords) not in list_table_contents:  # Avoid overwriting if already processed\n",
        "            cropped = image[coords[1]:coords[3], coords[0]:coords[2]]\n",
        "            content = ocr_agent.detect(cropped) or \"Figure Content\"\n",
        "            list_table_contents[tuple(coords)] = content.strip()\n",
        "            del cropped\n",
        "\n",
        "    # Calculate urs_hash\n",
        "    urs_hash = hashlib.md5(preprocess_name_for_hash(urs_name).encode()).hexdigest()\n",
        "\n",
        "    # Insert or get URS ID with urs_hash\n",
        "    cursor.execute(\"INSERT INTO urs (name, urs_hash) VALUES (%s, %s) ON CONFLICT (name) DO NOTHING RETURNING id\", (urs_name, urs_hash))\n",
        "    row = cursor.fetchone()\n",
        "    urs_id = row[0] if row else (cursor.execute(\"SELECT id FROM urs WHERE name = %s\", (urs_name,)) or cursor.fetchone()[0])\n",
        "    conn.commit()\n",
        "\n",
        "    # Initialize default section \"notitle\"\n",
        "    section_hash = hashlib.md5(preprocess_name_for_hash(current_section).encode()).hexdigest()\n",
        "    cursor.execute(\"SELECT id FROM sections WHERE section_hash = %s\", (section_hash,))\n",
        "    row = cursor.fetchone()\n",
        "    if not row:\n",
        "        cursor.execute(\"INSERT INTO sections (name, section_hash) VALUES (%s, %s) RETURNING id\", (current_section, section_hash))\n",
        "        section_id = cursor.fetchone()[0]\n",
        "    else:\n",
        "        section_id = row[0]\n",
        "    cursor.execute(\"INSERT INTO urs_section_mapping (urs_id, section_id) VALUES (%s, %s) ON CONFLICT DO NOTHING RETURNING urs_section_id\", (urs_id, section_id))\n",
        "    row = cursor.fetchone()\n",
        "    urs_section_id = row[0] if row else (cursor.execute(\"SELECT urs_section_id FROM urs_section_mapping WHERE urs_id = %s AND section_id = %s\", (urs_id, section_id)) or cursor.fetchone()[0])\n",
        "    conn.commit()\n",
        "\n",
        "    # Process blocks and write to DB immediately\n",
        "    for block in sorted_layout:\n",
        "        coords = list(map(int, block.coordinates))\n",
        "        block_type = block.type.lower()\n",
        "        overlapping_region = next((r for r in list_table_regions if is_overlapping(coords, r[1])), None)\n",
        "        if block_type in [\"title\", \"text\"] and overlapping_region:\n",
        "            parent_type, parent_coords = overlapping_region\n",
        "            cropped = image[coords[1]:coords[3], coords[0]:coords[2]]\n",
        "            skipped_content = ocr_agent.detect(cropped) or \"\"\n",
        "            if block_type == \"title\":\n",
        "                skipped_content = skipped_content or \"Untitled\"\n",
        "            skipped_blocks.append((\n",
        "                batch_id, parent_type, block_type, current_section,\n",
        "                list_table_contents[tuple(parent_coords)][:997] + \"...\" if len(list_table_contents[tuple(parent_coords)]) > 1000 else list_table_contents[tuple(parent_coords)],\n",
        "                skipped_content.strip()[:997] + \"...\" if len(skipped_content.strip()) > 1000 else skipped_content.strip(),\n",
        "                parent_coords, coords\n",
        "            ))\n",
        "            del cropped\n",
        "            continue\n",
        "\n",
        "        cropped = image[coords[1]:coords[3], coords[0]:coords[2]]\n",
        "        coords_tuple = tuple(coords)\n",
        "        if block_type in [\"list\", \"table\", \"figure\"]:\n",
        "            content = list_table_contents.get(coords_tuple, \"Unknown Content\")  # Safe access with fallback\n",
        "            content_lines = clean_and_split_sentences(content)\n",
        "        elif block_type == \"title\":\n",
        "            content = ocr_agent.detect(cropped) or \"Untitled\"\n",
        "            current_section = preprocess_name_for_insert(content)\n",
        "            section_hash = hashlib.md5(preprocess_name_for_hash(content).encode()).hexdigest()\n",
        "            cursor.execute(\"SELECT id FROM sections WHERE section_hash = %s\", (section_hash,))\n",
        "            row = cursor.fetchone()\n",
        "            if not row:\n",
        "                cursor.execute(\"INSERT INTO sections (name, section_hash) VALUES (%s, %s) RETURNING id\", (current_section, section_hash))\n",
        "                section_id = cursor.fetchone()[0]\n",
        "            else:\n",
        "                section_id = row[0]\n",
        "            cursor.execute(\"INSERT INTO urs_section_mapping (urs_id, section_id) VALUES (%s, %s) ON CONFLICT DO NOTHING RETURNING urs_section_id\", (urs_id, section_id))\n",
        "            row = cursor.fetchone()\n",
        "            urs_section_id = row[0] if row else (cursor.execute(\"SELECT urs_section_id FROM urs_section_mapping WHERE urs_id = %s AND section_id = %s\", (urs_id, section_id)) or cursor.fetchone()[0])\n",
        "            del cropped\n",
        "            continue\n",
        "        else:\n",
        "            content = ocr_agent.detect(cropped) or \"\"\n",
        "            content_lines = [content]\n",
        "        del cropped\n",
        "\n",
        "        # Write content blocks immediately\n",
        "        content_values = []\n",
        "        for line in content_lines:\n",
        "            if line.strip():\n",
        "                content_values.append((\n",
        "                    batch_id, urs_section_id, block_number, block_type, line.strip(),\n",
        "                    coords[0], coords[1], coords[2], coords[3], datetime.now().isoformat(), None\n",
        "                ))\n",
        "                block_number += 1\n",
        "        if content_values:\n",
        "            execute_values(cursor, \"\"\"\n",
        "                INSERT INTO content_blocks (batch_run_id, urs_section_id, block_number, content_type, content,\n",
        "                                           coord_x1, coord_y1, coord_x2, coord_y2, created_at, faiss_index_id)\n",
        "                VALUES %s\n",
        "                ON CONFLICT (batch_run_id, urs_section_id, block_number) DO UPDATE SET\n",
        "                    content_type = EXCLUDED.content_type, content = EXCLUDED.content,\n",
        "                    coord_x1 = EXCLUDED.coord_x1, coord_y1 = EXCLUDED.coord_y1,\n",
        "                    coord_x2 = EXCLUDED.coord_x2, coord_y2 = EXCLUDED.coord_y2,\n",
        "                    created_at = EXCLUDED.created_at, faiss_index_id = EXCLUDED.faiss_index_id\n",
        "            \"\"\", content_values)\n",
        "            conn.commit()\n",
        "\n",
        "        x1, y1, x2, y2 = coords\n",
        "        color = COLORS.get(block_type, (255, 255, 255))\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(image, f\"{block_type} #{block_number-1}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "\n",
        "    annotated_path = RETRAINED_ANNOTATED_DIR / f\"{Path(image_path).stem}_reannotated.png\"\n",
        "    cv2.imwrite(annotated_path, image, [cv2.IMWRITE_PNG_COMPRESSION, 9])\n",
        "    del image\n",
        "\n",
        "    # Write skipped blocks\n",
        "    if skipped_blocks:\n",
        "        execute_values(cursor, \"\"\"\n",
        "            INSERT INTO skipped_block_items (batch_run_id, parent_block_type, skipped_block_type, section_name,\n",
        "                                             parent_block_content, skipped_block_content, parent_block_coordinates, skipped_block_coordinates)\n",
        "            VALUES %s\n",
        "        \"\"\", skipped_blocks)\n",
        "        conn.commit()\n",
        "\n",
        "    return urs_section_id, block_number - 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww57aKh51mQs",
        "outputId": "e543942e-d592-45ad-d5b2-3ad25cdf1058"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Embedding and Storage Functions"
      ],
      "metadata": {
        "id": "T00qUeNd1pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Embedding and Storage Functions\n",
        "def initialize_embedding_tools(urs_name):\n",
        "    global sentence_model\n",
        "    if sentence_model is None:\n",
        "        sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
        "        print(\"SentenceTransformer loaded\")\n",
        "\n",
        "    faiss_index_path = get_faiss_index_path(urs_name)\n",
        "    embedding_dim = sentence_model.encode(\"test\").shape[0]\n",
        "    if faiss_index_path.exists():\n",
        "        faiss_index = faiss.read_index(str(faiss_index_path))\n",
        "        print(f\"Loaded FAISS index for {urs_name} from {faiss_index_path}\")\n",
        "    else:\n",
        "        faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "        print(f\"Initialized new FAISS index for {urs_name}\")\n",
        "    return faiss_index, faiss_index_path\n",
        "\n",
        "def append_to_faiss(urs_name, urs_section_id, block_count, conn, cursor, batch_id):\n",
        "    faiss_index, faiss_index_path = initialize_embedding_tools(urs_name)\n",
        "\n",
        "    # Fetch recently added content blocks\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT cb.id, cb.content, s.name\n",
        "        FROM content_blocks cb\n",
        "        JOIN urs_section_mapping usm ON cb.urs_section_id = usm.urs_section_id\n",
        "        JOIN sections s ON usm.section_id = s.id\n",
        "        WHERE cb.batch_run_id = %s AND cb.urs_section_id = %s AND cb.faiss_index_id IS NULL\n",
        "    \"\"\", (batch_id, urs_section_id))\n",
        "    rows = cursor.fetchall()\n",
        "\n",
        "    if rows:\n",
        "        query_strings = [f\"{row[2]}||{row[1]}\" for row in rows]\n",
        "        embeddings = sentence_model.encode(query_strings, convert_to_numpy=True)\n",
        "        faiss_index.add(embeddings)\n",
        "        index_positions = list(range(faiss_index.ntotal - len(rows), faiss_index.ntotal))\n",
        "\n",
        "        # Update faiss_index_id in content_blocks\n",
        "        update_values = [(row[0], idx) for idx, row in zip(index_positions, rows)]\n",
        "        execute_batch(cursor, \"\"\"\n",
        "            UPDATE content_blocks SET faiss_index_id = %s WHERE id = %s\n",
        "        \"\"\", update_values)\n",
        "        conn.commit()\n",
        "        faiss.write_index(faiss_index, str(faiss_index_path))\n",
        "        print(f\"Updated FAISS index for {urs_name} with {len(rows)} entries\")\n",
        "\n",
        "        del embeddings, query_strings\n",
        "    del rows"
      ],
      "metadata": {
        "id": "Yxh0dlGX1qbo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7: Main Processing Logic"
      ],
      "metadata": {
        "id": "6BZNO08C1rNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Main Processing Logic\n",
        "def convert_docx_to_pdf(docx_path, pdf_output_dir):\n",
        "    pdf_path = pdf_output_dir / f\"{Path(docx_path).stem}.pdf\"\n",
        "    try:\n",
        "        subprocess.run([\"libreoffice\", \"--headless\", \"--convert-to\", \"pdf\", str(docx_path), \"--outdir\", str(pdf_output_dir)], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        return pdf_path\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error converting {docx_path} to PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_images_from_pdf(pdf_path, image_output_dir):\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, output_folder=image_output_dir, fmt=\"png\")\n",
        "        image_paths = [Path(image.filename) for image in images]\n",
        "        return image_paths\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting images from {pdf_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def process_images():\n",
        "    conn, cursor, batch_id = setup_db_connection()\n",
        "    processed_files = 0\n",
        "\n",
        "    try:\n",
        "        source_files = [f for f in os.listdir(SOURCE_DIR) if f.endswith(('.docx', '.pdf'))]\n",
        "        for source_file in source_files:\n",
        "            source_file_path = SOURCE_DIR / source_file\n",
        "            urs_name = Path(source_file).stem\n",
        "            pdf_path = source_file_path if source_file.endswith('.pdf') else convert_docx_to_pdf(source_file_path, PDF_STAGING_DIR)\n",
        "\n",
        "            if pdf_path and pdf_path.exists():\n",
        "                image_paths = extract_images_from_pdf(pdf_path, IMG_STAGING_DIR)\n",
        "                for image_path in image_paths:\n",
        "                    print(f\"Processing {image_path} for URS {urs_name}\")\n",
        "                    urs_section_id, block_count = process_layout(str(image_path), urs_name, conn, cursor, batch_id)\n",
        "                    append_to_faiss(urs_name, urs_section_id, block_count, conn, cursor, batch_id)\n",
        "                    os.remove(image_path)\n",
        "                if source_file.endswith('.docx') and pdf_path.exists():\n",
        "                    os.remove(pdf_path)\n",
        "                processed_files += 1\n",
        "            else:\n",
        "                print(f\"Skipping {source_file} due to conversion failure\")\n",
        "\n",
        "        update_run_status(conn, cursor, batch_id, 'SUCCESS', processed_files)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        update_run_status(conn, cursor, batch_id, 'FAILED', processed_files)\n",
        "        raise\n",
        "    finally:\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "\n",
        "# Main Execution\n",
        "process_images()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncVXFBlQ1tPw",
        "outputId": "4c482ada-586b-441f-c4a8-1bcd40ad1839"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started batch run: batch_run_id=6\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/2bc4a9cc-0511-4e28-9439-4daaf78cb1fb-1.png for URS Pharma_URS_Enhanced\n",
            "OCR agent loaded\n",
            "Loaded model from /content/drive/My Drive/lifesciences/retrained_staging/models/model_final.pth\n",
            "SentenceTransformer loaded\n",
            "Initialized new FAISS index for Pharma_URS_Enhanced\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/2bc4a9cc-0511-4e28-9439-4daaf78cb1fb-2.png for URS Pharma_URS_Enhanced\n",
            "Initialized new FAISS index for Pharma_URS_Enhanced\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-01.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-02.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 21 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-03.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 33 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-04.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 33 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-05.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 33 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-06.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-07.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 19 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-08.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 33 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-09.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-10.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-11.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-12.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-13.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 19 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-14.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-15.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-16.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-17.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-18.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 18 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-19.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-20.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-21.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-22.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 19 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-23.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-24.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-25.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-26.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-27.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-28.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 20 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-29.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-30.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-31.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-32.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated FAISS index for ProtonGlow_Test_URS with 35 entries\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-33.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Processing /content/drive/My Drive/lifesciences/post_label/images/47fd018b-c6ce-490c-a8c9-6b15162f3c8f-34.png for URS ProtonGlow_Test_URS\n",
            "Loaded FAISS index for ProtonGlow_Test_URS from /content/drive/My Drive/lifesciences/faiss_indexes/28481597252c1a8c5171213add762432.index\n",
            "Updated batch run 6 to SUCCESS with 2 files processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8: Database Schema (Reference)"
      ],
      "metadata": {
        "id": "RiAcfFOX1xqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Database Schema (Reference)\n",
        "\"\"\"\n",
        "-- Drop existing tables and sequences\n",
        "DROP TABLE IF EXISTS skipped_block_items CASCADE;\n",
        "DROP TABLE IF EXISTS content_blocks CASCADE;\n",
        "DROP TABLE IF EXISTS urs_section_mapping CASCADE;\n",
        "DROP TABLE IF EXISTS sections CASCADE;\n",
        "DROP TABLE IF EXISTS batch_runs CASCADE;\n",
        "DROP TABLE IF EXISTS urs CASCADE;\n",
        "\n",
        "DROP SEQUENCE IF EXISTS skipped_block_items_id_seq;\n",
        "DROP SEQUENCE IF EXISTS content_blocks_id_seq;\n",
        "DROP SEQUENCE IF EXISTS urs_section_mapping_urs_section_id_seq;\n",
        "DROP SEQUENCE IF EXISTS sections_id_seq;\n",
        "DROP SEQUENCE IF EXISTS batch_runs_id_seq;\n",
        "DROP SEQUENCE IF EXISTS urs_id_seq;\n",
        "\n",
        "-- Create tables\n",
        "CREATE TABLE urs (\n",
        "    id SERIAL PRIMARY KEY,\n",
        "    name VARCHAR(255) NOT NULL UNIQUE,\n",
        "    urs_hash VARCHAR(32) NOT NULL\n",
        ");\n",
        "\n",
        "CREATE TABLE batch_runs (\n",
        "    id SERIAL PRIMARY KEY,\n",
        "    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "    completed_at TIMESTAMP,\n",
        "    processed_files INTEGER DEFAULT 0,\n",
        "    batch_status VARCHAR(20) DEFAULT 'RUNNING' CHECK (batch_status IN ('RUNNING', 'SUCCESS', 'FAILED'))\n",
        ");\n",
        "\n",
        "CREATE TABLE sections (\n",
        "    id SERIAL PRIMARY KEY,\n",
        "    name VARCHAR(255) NOT NULL,\n",
        "    section_hash VARCHAR(32) NOT NULL UNIQUE\n",
        ");\n",
        "\n",
        "CREATE TABLE urs_section_mapping (\n",
        "    urs_section_id SERIAL PRIMARY KEY,\n",
        "    urs_id INTEGER NOT NULL,\n",
        "    section_id INTEGER NOT NULL,\n",
        "    UNIQUE (urs_id, section_id),\n",
        "    FOREIGN KEY (urs_id) REFERENCES urs(id),\n",
        "    FOREIGN KEY (section_id) REFERENCES sections(id)\n",
        ");\n",
        "\n",
        "CREATE TABLE content_blocks (\n",
        "    id SERIAL PRIMARY KEY,\n",
        "    batch_run_id INTEGER NOT NULL,\n",
        "    urs_section_id INTEGER NOT NULL,\n",
        "    block_number INTEGER NOT NULL,\n",
        "    content_type VARCHAR(50) NOT NULL,\n",
        "    content TEXT,\n",
        "    coord_x1 INTEGER,\n",
        "    coord_y1 INTEGER,\n",
        "    coord_x2 INTEGER,\n",
        "    coord_y2 INTEGER,\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "    faiss_index_id INTEGER,\n",
        "    UNIQUE (batch_run_id, urs_section_id, block_number),\n",
        "    FOREIGN KEY (batch_run_id) REFERENCES batch_runs(id),\n",
        "    FOREIGN KEY (urs_section_id) REFERENCES urs_section_mapping(urs_section_id)\n",
        ");\n",
        "\n",
        "CREATE TABLE skipped_block_items (\n",
        "    id SERIAL PRIMARY KEY,\n",
        "    batch_run_id INTEGER NOT NULL,\n",
        "    parent_block_type VARCHAR(20) NOT NULL,\n",
        "    skipped_block_type VARCHAR(20) NOT NULL,\n",
        "    section_name VARCHAR(255),\n",
        "    parent_block_content VARCHAR(1000),\n",
        "    skipped_block_content VARCHAR(1000),\n",
        "    parent_block_coordinates INTEGER[],\n",
        "    skipped_block_coordinates INTEGER[],\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "    FOREIGN KEY (batch_run_id) REFERENCES batch_runs(id)\n",
        ");\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "YK8kxNPm12qe",
        "outputId": "34f8738c-6751-4d80-88d7-c431594c32ff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n-- Drop existing tables and sequences\\nDROP TABLE IF EXISTS skipped_block_items CASCADE;\\nDROP TABLE IF EXISTS content_blocks CASCADE;\\nDROP TABLE IF EXISTS urs_section_mapping CASCADE;\\nDROP TABLE IF EXISTS sections CASCADE;\\nDROP TABLE IF EXISTS batch_runs CASCADE;\\nDROP TABLE IF EXISTS urs CASCADE;\\n\\nDROP SEQUENCE IF EXISTS skipped_block_items_id_seq;\\nDROP SEQUENCE IF EXISTS content_blocks_id_seq;\\nDROP SEQUENCE IF EXISTS urs_section_mapping_urs_section_id_seq;\\nDROP SEQUENCE IF EXISTS sections_id_seq;\\nDROP SEQUENCE IF EXISTS batch_runs_id_seq;\\nDROP SEQUENCE IF EXISTS urs_id_seq;\\n\\n-- Create tables\\nCREATE TABLE urs (\\n    id SERIAL PRIMARY KEY,\\n    name VARCHAR(255) NOT NULL UNIQUE,\\n    urs_hash VARCHAR(32) NOT NULL\\n);\\n\\nCREATE TABLE batch_runs (\\n    id SERIAL PRIMARY KEY,\\n    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n    completed_at TIMESTAMP,\\n    processed_files INTEGER DEFAULT 0,\\n    batch_status VARCHAR(20) DEFAULT 'RUNNING' CHECK (batch_status IN ('RUNNING', 'SUCCESS', 'FAILED'))\\n);\\n\\nCREATE TABLE sections (\\n    id SERIAL PRIMARY KEY,\\n    name VARCHAR(255) NOT NULL,\\n    section_hash VARCHAR(32) NOT NULL UNIQUE\\n);\\n\\nCREATE TABLE urs_section_mapping (\\n    urs_section_id SERIAL PRIMARY KEY,\\n    urs_id INTEGER NOT NULL,\\n    section_id INTEGER NOT NULL,\\n    UNIQUE (urs_id, section_id),\\n    FOREIGN KEY (urs_id) REFERENCES urs(id),\\n    FOREIGN KEY (section_id) REFERENCES sections(id)\\n);\\n\\nCREATE TABLE content_blocks (\\n    id SERIAL PRIMARY KEY,\\n    batch_run_id INTEGER NOT NULL,\\n    urs_section_id INTEGER NOT NULL,\\n    block_number INTEGER NOT NULL,\\n    content_type VARCHAR(50) NOT NULL,\\n    content TEXT,\\n    coord_x1 INTEGER,\\n    coord_y1 INTEGER,\\n    coord_x2 INTEGER,\\n    coord_y2 INTEGER,\\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n    faiss_index_id INTEGER,\\n    UNIQUE (batch_run_id, urs_section_id, block_number),\\n    FOREIGN KEY (batch_run_id) REFERENCES batch_runs(id),\\n    FOREIGN KEY (urs_section_id) REFERENCES urs_section_mapping(urs_section_id)\\n);\\n\\nCREATE TABLE skipped_block_items (\\n    id SERIAL PRIMARY KEY,\\n    batch_run_id INTEGER NOT NULL,\\n    parent_block_type VARCHAR(20) NOT NULL,\\n    skipped_block_type VARCHAR(20) NOT NULL,\\n    section_name VARCHAR(255),\\n    parent_block_content VARCHAR(1000),\\n    skipped_block_content VARCHAR(1000),\\n    parent_block_coordinates INTEGER[],\\n    skipped_block_coordinates INTEGER[],\\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n    FOREIGN KEY (batch_run_id) REFERENCES batch_runs(id)\\n);\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}