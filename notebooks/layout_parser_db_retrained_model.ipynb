{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW1jWo24k6Kx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gy_aMFm5Fwox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwsXJyoPYCYr"
      },
      "source": [
        "0: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZxbFeAzDUnD",
        "outputId": "88cbfc37-954b-4f35-a2fc-d2699a6024c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.11/dist-packages (3.20.3)\n",
            "Requirement already satisfied: google-api-core==1.34.1 in /usr/local/lib/python3.11/dist-packages (1.34.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core==1.34.1) (1.69.2)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core==1.34.1) (2.38.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core==1.34.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core==1.34.1) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core==1.34.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core==1.34.1) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.34.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.34.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.34.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.34.1) (2025.1.31)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core==1.34.1) (0.6.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Torch version: 2.6.0+cu124\n",
            "Layoutparser version: 0.3.4\n",
            "Detectron2 version: 0.6\n",
            "FAISS version: 1.10.0\n"
          ]
        }
      ],
      "source": [
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Update package index and install system dependencies in one step\n",
        "!apt-get update -qq && apt-get install -y libreoffice poppler-utils tesseract-ocr tesseract-ocr-eng -qq\n",
        "# - libreoffice: For DOCX to PDF conversion\n",
        "# - poppler-utils: For pdf2image\n",
        "# - tesseract-ocr & tesseract-ocr-eng: For OCR functionality\n",
        "\n",
        "# Install core Python dependencies in a single command\n",
        "!pip install --quiet --no-cache-dir \\\n",
        "    torch torchvision \\\n",
        "    pdf2image opencv-python-headless \\\n",
        "    pytesseract PyPDF2 \\\n",
        "    python-docx \\\n",
        "    pillow \\\n",
        "    cython pycocotools fvcore \\\n",
        "    sentence-transformers \\\n",
        "    fastapi uvicorn pyngrok \\\n",
        "    psycopg2-binary\n",
        "\n",
        "!pip install protobuf==3.20.3 google-api-core==1.34.1\n",
        "\n",
        "!pip install faiss-cpu --upgrade\n",
        "# Install detectron2 from a specific commit for stability and compatibility\n",
        "!pip install --quiet -U 'git+https://github.com/facebookresearch/detectron2.git@ff53992b1985b63bd3262b5a36167098e3dada02'\n",
        "\n",
        "# Install layoutparser with OCR support\n",
        "!pip install --quiet \"layoutparser[ocr]\"\n",
        "\n",
        "# Verify installations (optional, for debugging)\n",
        "import torch\n",
        "import layoutparser\n",
        "import detectron2\n",
        "import faiss\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Layoutparser version: {layoutparser.__version__}\")\n",
        "print(f\"Detectron2 version: {detectron2.__version__}\")\n",
        "print(f\"FAISS version: {faiss.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4ckDByTUkmq"
      },
      "source": [
        "1: Imports and Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_5LFROk0Ug-C"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for deep learning, computer vision, OCR, and database operations\n",
        "import torch\n",
        "import torchvision\n",
        "import layoutparser as lp\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.model_zoo import model_zoo\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from pdf2image import convert_from_path\n",
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from docx import Document\n",
        "from docx.oxml.ns import qn\n",
        "from docx.oxml import OxmlElement\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import psycopg2\n",
        "from psycopg2.extras import execute_values\n",
        "from google.colab import drive\n",
        "\n",
        "# Configuration flag for database usage\n",
        "USE_POSTGRES = True\n",
        "\n",
        "# Neon PostgreSQL connection details\n",
        "DB_CONFIG = {\n",
        "    \"dbname\": \"neondb\",\n",
        "    \"user\": \"neondb_owner\",\n",
        "    \"password\": \"npg_DU3Vxoi6cCIu\",\n",
        "    \"host\": \"ep-purple-sound-a4z952yb-pooler.us-east-1.aws.neon.tech\",\n",
        "    \"port\": \"5432\",\n",
        "    \"sslmode\": \"require\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD8Pj1olUx6r"
      },
      "source": [
        "2: Device and Directory Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Q-6t0QUpUwxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e23d56-3673-454f-bcc0-89dc2e5b86c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Select device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive for file storage\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "# Define base directory and subdirectories for file organization\n",
        "BASE_DIR = \"/content/drive/My Drive/lifesciences\"\n",
        "SOURCE_DIR = Path(BASE_DIR) / \"training_documents\"\n",
        "PDF_STAGING_DIR = Path(BASE_DIR) / \"staging/pdf\"\n",
        "IMG_STAGING_DIR = Path(BASE_DIR) / \"post_label/images\"\n",
        "ANNOTATED_DIR = Path(BASE_DIR) / \"staging/annotated\"\n",
        "MODELS_DIR = Path(BASE_DIR) / \"models\"\n",
        "RETRAINED_STAGING_DIR = Path(BASE_DIR) / \"retrained_staging\"\n",
        "RETRAINED_ANNOTATED_DIR = RETRAINED_STAGING_DIR / \"annotated\"\n",
        "FAISS_INDEX_PATH = Path(BASE_DIR) / \"faiss_index_main.index\"  # Single main index\n",
        "FAISS_BACKUP_DIR = Path(BASE_DIR) / \"faiss_backups\"  # Directory for backups\n",
        "COCO_JSON_PATH = Path(BASE_DIR) / \"post_label/result.json\"\n",
        "RETRAINED_MODELS_DIR = Path(BASE_DIR) / \"retrained_staging/models\"\n",
        "# Create directories if they don’t exist\n",
        "for directory in [SOURCE_DIR, PDF_STAGING_DIR, IMG_STAGING_DIR, ANNOTATED_DIR, MODELS_DIR, RETRAINED_STAGING_DIR, RETRAINED_ANNOTATED_DIR, FAISS_BACKUP_DIR]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Increase maximum image pixel limit to handle large images\n",
        "Image.MAX_IMAGE_PIXELS = 100000000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRM-bTHKU2bI"
      },
      "source": [
        "3: Global Variables and Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "C0mgBvAdUiUv"
      },
      "outputs": [],
      "source": [
        "# Initialize global variables\n",
        "#id_seq = 1 removed this as we will use database sequence\n",
        "sentence_model = None\n",
        "faiss_index = None\n",
        "model = None\n",
        "ocr_agent = None\n",
        "db_conn = None\n",
        "db_cursor = None\n",
        "run_number = 1\n",
        "batch_run_id = None\n",
        "run_date_time = datetime.now().isoformat()\n",
        "logged_in_user = \"admin\"\n",
        "FAISS_INDEX_PATH = Path(BASE_DIR) / \"faiss_index_retrained.index\"\n",
        "# Define colors for different block types for visualization\n",
        "COLORS = {\n",
        "    \"title\": (255, 0, 0), \"text\": (0, 255, 0), \"list\": (0, 0, 255),\n",
        "    \"table\": (255, 255, 0), \"figure\": (255, 0, 255)\n",
        "}\n",
        "\n",
        "# Map numerical labels to block types\n",
        "LABEL_MAP = {0: \"figure\", 1: \"list\", 2: \"table\", 3: \"text\", 4: \"title\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ESvZedhU8h-"
      },
      "source": [
        "4: COCO JSON Path Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WfPWIC5tVEbB"
      },
      "outputs": [],
      "source": [
        "# Function to update COCO JSON file paths based on actual image directory\n",
        "def map_and_update_coco_paths(json_path, image_dir):\n",
        "    try:\n",
        "        with open(json_path, \"r\") as f:\n",
        "            coco_data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading JSON {json_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Create a mapping of actual image filenames\n",
        "    actual_images = {f: f for f in os.listdir(image_dir) if f.endswith(\".png\")}\n",
        "    print(f\"Actual images in {image_dir}: {list(actual_images.keys())}\")\n",
        "\n",
        "    updated_images = []\n",
        "    for image in coco_data[\"images\"]:\n",
        "        original_name = image[\"file_name\"]\n",
        "        cleaned_name = os.path.basename(original_name.replace(\"./images/\", \"\"))\n",
        "        if cleaned_name in actual_images:\n",
        "            image[\"file_name\"] = os.path.join(image_dir, cleaned_name)\n",
        "            updated_images.append(image)\n",
        "            print(f\"Mapped {original_name} to {image['file_name']}\")\n",
        "        else:\n",
        "            print(f\"No match found for {cleaned_name} in {image_dir}\")\n",
        "\n",
        "    coco_data[\"images\"] = updated_images\n",
        "    updated_json_path = RETRAINED_STAGING_DIR / \"results_mapped.json\"\n",
        "    with open(updated_json_path, \"w\") as f:\n",
        "        json.dump(coco_data, f, indent=2)\n",
        "    print(f\"Updated COCO JSON saved to {updated_json_path}\")\n",
        "    return updated_json_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1p8tY00VIeZ"
      },
      "source": [
        "5: Dataset Registration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CAY-8W5GVH5z"
      },
      "outputs": [],
      "source": [
        "# Function to register a COCO dataset with Detectron2\n",
        "def register_coco_dataset(json_path, image_dir):\n",
        "    dataset_name = \"custom_layout_dataset\"\n",
        "    try:\n",
        "        # Remove existing dataset if it exists\n",
        "        if dataset_name in DatasetCatalog:\n",
        "            DatasetCatalog.remove(dataset_name)\n",
        "            if dataset_name in MetadataCatalog:\n",
        "                MetadataCatalog.remove(dataset_name)\n",
        "            print(f\"Unregistered existing dataset: {dataset_name}\")\n",
        "\n",
        "        # Register the new dataset\n",
        "        register_coco_instances(dataset_name, {}, str(json_path), image_dir)\n",
        "        MetadataCatalog.get(dataset_name).thing_classes = list(LABEL_MAP.values())\n",
        "        print(f\"Registered dataset: {dataset_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error registering dataset: {e}\")\n",
        "        raise\n",
        "    return dataset_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1wAxFIFVRZJ"
      },
      "source": [
        "6: Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "S_z9aCHeVXMT"
      },
      "outputs": [],
      "source": [
        "# Function to train the LayoutParser model using Detectron2\n",
        "def train_layoutparser_model(json_path, image_dir):\n",
        "    global model\n",
        "    dataset_name = register_coco_dataset(json_path, image_dir)\n",
        "    cfg = get_cfg()\n",
        "\n",
        "    # Load pre-trained model if available, otherwise use model zoo weights\n",
        "    pretrained_model_path = os.path.join(MODELS_DIR, \"model_final.pth\")\n",
        "    if os.path.exists(pretrained_model_path):\n",
        "        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "        cfg.MODEL.WEIGHTS = pretrained_model_path\n",
        "        print(f\"Loaded pre-trained weights from {pretrained_model_path} (Faster R-CNN)\")\n",
        "    else:\n",
        "        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
        "        print(\"Using default Detectron2 Faster R-CNN model zoo weights\")\n",
        "\n",
        "    # Configure training parameters\n",
        "    cfg.MODEL.DEVICE = str(device)\n",
        "    cfg.DATASETS.TRAIN = (dataset_name,)\n",
        "    cfg.DATASETS.TEST = ()\n",
        "    cfg.DATALOADER.NUM_WORKERS = 2\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "    cfg.SOLVER.BASE_LR = 0.00025\n",
        "    cfg.SOLVER.MAX_ITER = 300\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(LABEL_MAP)\n",
        "    cfg.OUTPUT_DIR = str(RETRAINED_MODELS_DIR)\n",
        "\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "    trainer = DefaultTrainer(cfg)\n",
        "    trainer.resume_or_load(resume=False)\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the trained model and configuration\n",
        "    model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "    config_path = os.path.join(cfg.OUTPUT_DIR, \"config.yaml\")\n",
        "    with open(config_path, \"w\") as f:\n",
        "        f.write(cfg.dump())\n",
        "\n",
        "    model = lp.Detectron2LayoutModel(\n",
        "        config_path=config_path,\n",
        "        model_path=model_path,\n",
        "        label_map=LABEL_MAP,\n",
        "        extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.2],\n",
        "        device=str(device)\n",
        "    )\n",
        "    print(f\"Model retrained and saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQGGkQoZVaKS"
      },
      "source": [
        "7: Lazy Loading Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "DERyIgGvVgIC"
      },
      "outputs": [],
      "source": [
        "# Function to lazily load OCR and layout models\n",
        "def lazy_load_models():\n",
        "    global model, ocr_agent\n",
        "    if ocr_agent is None:\n",
        "        try:\n",
        "            ocr_agent = lp.TesseractAgent(languages='eng', config='--psm 6 --oem 1')\n",
        "            print(\"OCR agent loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading OCR agent: {e}\")\n",
        "            raise\n",
        "    if model is None:\n",
        "        model_path = os.path.join(RETRAINED_MODELS_DIR, \"model_final.pth\")\n",
        "        config_path = os.path.join(RETRAINED_MODELS_DIR, \"config.yaml\")\n",
        "        if os.path.exists(model_path) and os.path.exists(config_path):\n",
        "            try:\n",
        "                model = lp.Detectron2LayoutModel(\n",
        "                    config_path=config_path,\n",
        "                    model_path=model_path,\n",
        "                    label_map=LABEL_MAP,\n",
        "                    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.2],\n",
        "                    device=str(device)\n",
        "                )\n",
        "                print(f\"Loaded retrained model from {model_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(f\"No retrained model or config found at {model_path} or {config_path}. Training required.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1PwYmVFVg7-"
      },
      "source": [
        "8: Database Connection Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2CV8m2v6VkEJ"
      },
      "outputs": [],
      "source": [
        "# Function to set up PostgreSQL database connection\n",
        "def setup_db_connection():\n",
        "    global db_conn, db_cursor, run_number, batch_run_id\n",
        "    if db_conn is None:\n",
        "        try:\n",
        "            db_conn = psycopg2.connect(**DB_CONFIG)\n",
        "            db_cursor = db_conn.cursor()\n",
        "            print(\"Connected to Neon PostgreSQL database\")\n",
        "            insert_batch_query = \"\"\"\n",
        "                INSERT INTO batch_runs (run_number, started_at, processed_files, batch_status)\n",
        "                VALUES ((SELECT COALESCE(MAX(run_number), 0) + 1 FROM batch_runs), %s, 0, 'RUNNING')\n",
        "                RETURNING id, run_number\n",
        "            \"\"\"\n",
        "            db_cursor.execute(insert_batch_query, (datetime.now().isoformat(),))\n",
        "            batch_run_id, run_number = db_cursor.fetchone()\n",
        "            db_conn.commit()\n",
        "            print(f\"Started new batch run: run_number={run_number}, batch_run_id={batch_run_id}, status=RUNNING\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to connect to Neon PostgreSQL: {e}\")\n",
        "            raise\n",
        "\n",
        "# Function to update the run status in the database\n",
        "def update_run_status(status):\n",
        "    global db_conn, db_cursor, batch_run_id\n",
        "    if db_conn is not None:\n",
        "        try:\n",
        "            update_query = \"\"\"\n",
        "                UPDATE batch_runs\n",
        "                SET completed_at = %s,\n",
        "                    processed_files = (SELECT COUNT(DISTINCT source_file_id) FROM source_file_runs WHERE batch_run_id = %s),\n",
        "                    batch_status = %s\n",
        "                WHERE id = %s\n",
        "            \"\"\"\n",
        "            db_cursor.execute(update_query, (datetime.now().isoformat(), batch_run_id, status, batch_run_id))\n",
        "            db_conn.commit()\n",
        "            print(f\"Updated batch run {batch_run_id} status to {status} with processed files count\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to update run status: {e}\")\n",
        "            db_conn.rollback()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function populates documents, source_files, and source_file_runs for each file processed."
      ],
      "metadata": {
        "id": "RZURwolQcAJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_source_file_metadata(source_file_path):\n",
        "    global db_conn, db_cursor, batch_run_id  # Use batch_run_id instead of run_number\n",
        "    setup_db_connection()\n",
        "\n",
        "    try:\n",
        "        file_path = Path(source_file_path)\n",
        "        document_name = file_path.stem  # e.g., \"URS_System_A\"\n",
        "        file_name = file_path.stem  # e.g., \"URS_System_A_v1\"\n",
        "        file_type = file_path.suffix[1:] if file_path.suffix in ('.docx', '.pdf') else 'other'\n",
        "\n",
        "        # Insert or get document\n",
        "        db_cursor.execute(\"\"\"\n",
        "            INSERT INTO documents (name)\n",
        "            VALUES (%s)\n",
        "            ON CONFLICT (name) DO UPDATE SET name = EXCLUDED.name\n",
        "            RETURNING id\n",
        "        \"\"\", (document_name,))\n",
        "        document_id = db_cursor.fetchone()[0]\n",
        "\n",
        "        # Insert or get source file\n",
        "        db_cursor.execute(\"\"\"\n",
        "            INSERT INTO source_files (document_id, file_name, file_type)\n",
        "            VALUES (%s, %s, %s)\n",
        "            ON CONFLICT (document_id, file_name) DO UPDATE SET file_type = EXCLUDED.file_type\n",
        "            RETURNING id\n",
        "        \"\"\", (document_id, file_name, file_type))\n",
        "        source_file_id = db_cursor.fetchone()[0]\n",
        "\n",
        "        # Extract metadata from the source file\n",
        "        author = \"Unknown\"\n",
        "        created_dt = datetime.now().date()\n",
        "        last_modified_dt = datetime.now().date()\n",
        "        number_pages = 1\n",
        "\n",
        "        if file_type == 'pdf':\n",
        "            from pdf2image import convert_from_path\n",
        "            number_pages = len(convert_from_path(source_file_path))\n",
        "            try:\n",
        "                from PyPDF2 import PdfReader\n",
        "                pdf = PdfReader(source_file_path)\n",
        "                info = pdf.metadata\n",
        "                if info:\n",
        "                    author = info.get('/Author', 'Unknown') or 'Unknown'\n",
        "                    if info.get('/CreationDate'):\n",
        "                        created_dt = datetime.strptime(info['/CreationDate'][2:10], '%Y%m%d').date()\n",
        "                    if info.get('/ModDate'):\n",
        "                        last_modified_dt = datetime.strptime(info['/ModDate'][2:10], '%Y%m%d').date()\n",
        "            except ImportError:\n",
        "                print(\"PyPDF2 not installed; skipping detailed PDF metadata extraction\")\n",
        "        elif file_type == 'docx':\n",
        "            doc = Document(source_file_path)\n",
        "            number_pages = len(doc.sections)  # Rough estimate\n",
        "            props = doc.core_properties\n",
        "            author = props.author or 'Unknown'\n",
        "            created_dt = props.created.date() if props.created else datetime.now().date()\n",
        "            last_modified_dt = props.modified.date() if props.modified else datetime.now().date()\n",
        "\n",
        "        # Use batch_run_id (references batch_runs.id) instead of run_number\n",
        "        db_cursor.execute(\"\"\"\n",
        "            INSERT INTO source_file_runs (source_file_id, batch_run_id, author, created_dt, last_modified_dt, number_pages)\n",
        "            VALUES (%s, %s, %s, %s, %s, %s)\n",
        "            ON CONFLICT (source_file_id, batch_run_id) DO UPDATE\n",
        "                SET author = EXCLUDED.author,\n",
        "                    created_dt = EXCLUDED.created_dt,\n",
        "                    last_modified_dt = EXCLUDED.last_modified_dt,\n",
        "                    number_pages = EXCLUDED.number_pages\n",
        "            RETURNING id\n",
        "        \"\"\", (source_file_id, batch_run_id, author, created_dt, last_modified_dt, number_pages))\n",
        "        source_file_run_id = db_cursor.fetchone()[0]\n",
        "\n",
        "        db_conn.commit()\n",
        "        print(f\"Processed source file metadata for {source_file_path}: source_file_run_id = {source_file_run_id}\")\n",
        "        return source_file_run_id, document_name\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing source file metadata: {e}\")\n",
        "        db_conn.rollback()\n",
        "        raise"
      ],
      "metadata": {
        "id": "x3-E-Bvdb-cL"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_skipped_blocks_to_db(skipped_blocks):\n",
        "    if not skipped_blocks:\n",
        "        return\n",
        "\n",
        "    setup_db_connection()\n",
        "    try:\n",
        "        MAX_CONTENT_LENGTH = 1000\n",
        "        values = [\n",
        "            (\n",
        "                batch_run_id,  # Use batch_run_id instead of run_number\n",
        "                sb[\"parent_block_type\"],\n",
        "                sb[\"skipped_block_type\"],\n",
        "                sb[\"section_name\"][:255] if sb[\"section_name\"] else None,\n",
        "                sb[\"parent_block_content\"][:MAX_CONTENT_LENGTH-3] + \"...\" if len(sb[\"parent_block_content\"]) > MAX_CONTENT_LENGTH else sb[\"parent_block_content\"],\n",
        "                sb[\"skipped_block_content\"][:MAX_CONTENT_LENGTH-3] + \"...\" if len(sb[\"skipped_block_content\"]) > MAX_CONTENT_LENGTH else sb[\"skipped_block_content\"],\n",
        "                sb[\"parent_block_coordinates\"],\n",
        "                sb[\"skipped_block_coordinates\"]\n",
        "            )\n",
        "            for sb in skipped_blocks\n",
        "        ]\n",
        "        query = \"\"\"\n",
        "            INSERT INTO skipped_block_items (\n",
        "                batch_run_id, parent_block_type, skipped_block_type, section_name,\n",
        "                parent_block_content, skipped_block_content,\n",
        "                parent_block_coordinates, skipped_block_coordinates\n",
        "            ) VALUES %s\n",
        "        \"\"\"\n",
        "        execute_values(db_cursor, query, values)\n",
        "        db_conn.commit()\n",
        "        print(f\"Inserted {len(skipped_blocks)} skipped blocks into skipped_block_items\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipped blocks database error: {e}\")\n",
        "        db_conn.rollback()"
      ],
      "metadata": {
        "id": "AKWgXFF_d3pb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-BKXRTWAhKF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS-CEN79Vm4c"
      },
      "source": [
        "9: Layout Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QICOhIctVnl5"
      },
      "outputs": [],
      "source": [
        "# Function to check if two rectangles overlap\n",
        "def is_overlapping(rect1, rect2):\n",
        "    return not (rect1[2] <= rect2[0] or rect2[2] <= rect1[0] or\n",
        "                rect1[3] <= rect2[1] or rect2[3] <= rect1[1])\n",
        "\n",
        "def process_layout(image_path, original_file_path):\n",
        "    global model\n",
        "    lazy_load_models()\n",
        "    if model is None:\n",
        "        raise ValueError(\"Model not loaded. Train or load a model first.\")\n",
        "\n",
        "    source_file_run_id, document_name = process_source_file_metadata(original_file_path)\n",
        "    image = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "\n",
        "    layout = model.detect(image)\n",
        "    sorted_layout = sorted(layout, key=lambda x: x.coordinates[1])\n",
        "    results = []\n",
        "    skipped_blocks = []\n",
        "    list_table_regions = [(b.type.lower(), list(map(int, b.coordinates))) for b in sorted_layout if b.type.lower() in [\"list\", \"table\"]]\n",
        "    current_section = \"No Title\"\n",
        "    block_number = 1\n",
        "\n",
        "    list_table_contents = {}\n",
        "    for block_type, coords in list_table_regions:\n",
        "        cropped = image[coords[1]:coords[3], coords[0]:coords[2]]\n",
        "        content = ocr_agent.detect(cropped) or \"\"\n",
        "        if block_type == \"list\":\n",
        "            content = re.sub(r'[^a-zA-Z0-9\\s]', '', content)\n",
        "        elif block_type == \"table\":\n",
        "            content = '\\n'.join('-'.join(col.strip() for col in row.split() if col.strip()) for row in content.split('\\n') if row.strip()) or \"Table Content\"\n",
        "        list_table_contents[tuple(coords)] = content.strip()\n",
        "        del cropped\n",
        "\n",
        "    for block in sorted_layout:\n",
        "        coords = list(map(int, block.coordinates))\n",
        "        block_type = block.type.lower()\n",
        "        overlapping_region = next((r for r in list_table_regions if is_overlapping(coords, r[1])), None)\n",
        "        if block_type in [\"title\", \"text\"] and overlapping_region:\n",
        "            parent_type, parent_coords = overlapping_region\n",
        "            cropped = image[coords[1]:coords[3], coords[0]:coords[2]]\n",
        "            skipped_content = ocr_agent.detect(cropped) or \"\"\n",
        "            if block_type == \"title\":\n",
        "                skipped_content = skipped_content or \"Untitled\"\n",
        "            print(f\"Skipping {block_type} inside {parent_type} at coordinates: {coords}\")\n",
        "            skipped_blocks.append({\n",
        "                \"parent_block_type\": parent_type,\n",
        "                \"skipped_block_type\": block_type,\n",
        "                \"section_name\": current_section,\n",
        "                \"parent_block_content\": list_table_contents[tuple(parent_coords)],\n",
        "                \"skipped_block_content\": skipped_content.strip(),\n",
        "                \"parent_block_coordinates\": parent_coords,\n",
        "                \"skipped_block_coordinates\": coords\n",
        "            })\n",
        "            del cropped\n",
        "            continue\n",
        "\n",
        "        cropped = image[coords[1]:coords[3], coords[0]:coords[2]]\n",
        "        if block_type in [\"list\", \"table\"]:\n",
        "            content = list_table_contents[tuple(coords)]\n",
        "        elif block_type == \"title\":\n",
        "            content = ocr_agent.detect(cropped) or \"Untitled\"\n",
        "            current_section = content.lower()\n",
        "        else:\n",
        "            content = ocr_agent.detect(cropped) or \"\"\n",
        "\n",
        "        db_cursor.execute(\"\"\"\n",
        "            INSERT INTO sections (section_name)\n",
        "            VALUES (%s)\n",
        "            ON CONFLICT (section_name) DO UPDATE SET section_name = EXCLUDED.section_name\n",
        "            RETURNING id\n",
        "        \"\"\", (current_section,))\n",
        "        section_id = db_cursor.fetchone()[0]\n",
        "\n",
        "        db_cursor.execute(\"\"\"\n",
        "            INSERT INTO source_file_run_sections (source_file_run_id, section_id)\n",
        "            VALUES (%s, %s)\n",
        "            ON CONFLICT DO NOTHING\n",
        "        \"\"\", (source_file_run_id, section_id))\n",
        "\n",
        "        # Remove 'id' from result dict\n",
        "        result = {\n",
        "            \"source_file_run_id\": source_file_run_id,\n",
        "            \"section_id\": section_id,\n",
        "            \"block_number\": block_number,\n",
        "            \"content_type\": block_type,\n",
        "            \"content\": content.strip(),\n",
        "            \"coord_x1\": coords[0],\n",
        "            \"coord_y1\": coords[1],\n",
        "            \"coord_x2\": coords[2],\n",
        "            \"coord_y2\": coords[3],\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"urs_name\": document_name,\n",
        "            \"section_name\": current_section\n",
        "        }\n",
        "        results.append(result)\n",
        "        block_number += 1\n",
        "\n",
        "        x1, y1, x2, y2 = coords\n",
        "        color = COLORS.get(block_type, (255, 255, 255))\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(image, f\"{block_type} #{block_number-1}\", (x1, y1-10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "        del cropped\n",
        "\n",
        "    annotated_path = os.path.join(RETRAINED_ANNOTATED_DIR, f\"{Path(image_path).stem}_reannotated.png\")\n",
        "    cv2.imwrite(annotated_path, image, [cv2.IMWRITE_PNG_COMPRESSION, 9])\n",
        "    db_conn.commit()\n",
        "    del image\n",
        "    append_skipped_blocks_to_db(skipped_blocks)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eJcrWEDVp05"
      },
      "source": [
        "10: Embedding and Database Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9J_YrcQoVzFB"
      },
      "outputs": [],
      "source": [
        "# Function to initialize embedding tools (SentenceTransformer and FAISS)\n",
        "# In section \"10: Embedding and Database Storage\"\n",
        "def initialize_embedding_tools(device='cpu'):\n",
        "    global sentence_model, faiss_index\n",
        "    if sentence_model is None:\n",
        "        try:\n",
        "            sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "            print(\"SentenceTransformer model loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading SentenceTransformer: {e}\")\n",
        "            sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
        "            print(\"Fallback to CPU for SentenceTransformer\")\n",
        "\n",
        "    if faiss_index is None:\n",
        "        embedding_dim = sentence_model.encode(\"test\").shape[0]\n",
        "        faiss_index_path = FAISS_INDEX_PATH  # Use main index path\n",
        "        if faiss_index_path.exists():\n",
        "            try:\n",
        "                faiss_index = faiss.read_index(str(faiss_index_path))\n",
        "                print(f\"Loaded existing FAISS index from {faiss_index_path} with {faiss_index.ntotal} embeddings\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading FAISS index: {e}\")\n",
        "                faiss_index = None\n",
        "        if faiss_index is None:\n",
        "            faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "            print(f\"FAISS index initialized with dimension {embedding_dim}\")\n",
        "    return faiss_index_path\n",
        "\n",
        "# In section \"10: Embedding and Database Storage\"\n",
        "def append_to_db(data):\n",
        "    if not data:\n",
        "        return\n",
        "\n",
        "    setup_db_connection()\n",
        "    global batch_run_id\n",
        "    faiss_index_path = FAISS_INDEX_PATH  # Use main index path\n",
        "\n",
        "    # Backup the existing FAISS index before modifying\n",
        "    if faiss_index_path.exists():\n",
        "        backup_path = FAISS_BACKUP_DIR / f\"faiss_index_backup_{datetime.now().strftime('%Y%m%d')}.index\"\n",
        "        try:\n",
        "            shutil.copy(faiss_index_path, backup_path)\n",
        "            print(f\"Backed up FAISS index to {backup_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating FAISS index backup: {e}\")\n",
        "\n",
        "    # Initialize or load the FAISS index\n",
        "    if faiss_index is None:\n",
        "        faiss_index_path = initialize_embedding_tools()\n",
        "\n",
        "    try:\n",
        "        separator = \"|||\"\n",
        "        query_strings = [\n",
        "            f\"{d.get('urs_name', 'Unknown')}{separator}\"\n",
        "            f\"{d.get('section_name', d['section_name'])}{separator}\"\n",
        "            f\"{d['content_type']}{separator}\"\n",
        "            f\"{d['content']}\"\n",
        "            for d in data if d.get('content_type') != 'title'\n",
        "        ]\n",
        "        embeddings = sentence_model.encode(query_strings, convert_to_numpy=True)\n",
        "\n",
        "        faiss_index.add(embeddings)\n",
        "        index_positions = list(range(faiss_index.ntotal - len(data), faiss_index.ntotal))\n",
        "\n",
        "        values = [\n",
        "            (d[\"source_file_run_id\"], d[\"section_id\"], d[\"block_number\"], d[\"content_type\"], d[\"content\"],\n",
        "             d[\"coord_x1\"], d[\"coord_y1\"], d[\"coord_x2\"], d[\"coord_y2\"], d[\"created_at\"], idx)\n",
        "            for d, idx in zip(data, index_positions)\n",
        "        ]\n",
        "\n",
        "        query = \"\"\"\n",
        "            INSERT INTO content_blocks (\n",
        "                source_file_run_id, section_id, block_number, content_type, content,\n",
        "                coord_x1, coord_y1, coord_x2, coord_y2, created_at, faiss_index_id\n",
        "            ) VALUES %s\n",
        "            ON CONFLICT (source_file_run_id, block_number) DO UPDATE SET\n",
        "                section_id = EXCLUDED.section_id,\n",
        "                content_type = EXCLUDED.content_type,\n",
        "                content = EXCLUDED.content,\n",
        "                coord_x1 = EXCLUDED.coord_x1,\n",
        "                coord_y1 = EXCLUDED.coord_y1,\n",
        "                coord_x2 = EXCLUDED.coord_x2,\n",
        "                coord_y2 = EXCLUDED.coord_y2,\n",
        "                created_at = EXCLUDED.created_at,\n",
        "                faiss_index_id = EXCLUDED.faiss_index_id\n",
        "        \"\"\"\n",
        "        execute_values(db_cursor, query, values)\n",
        "        db_conn.commit()\n",
        "        print(f\"Inserted/Updated {len(data)} records into content_blocks for batch {batch_run_id}\")\n",
        "\n",
        "        faiss.write_index(faiss_index, str(faiss_index_path))\n",
        "        print(f\"Saved FAISS index to {faiss_index_path} with {faiss_index.ntotal} embeddings\")\n",
        "    except Exception as e:\n",
        "        print(f\"Database error: {e}\")\n",
        "        db_conn.rollback()\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmiuHRWiV2hu"
      },
      "source": [
        "11a: Update COCO JSON Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "I5UP_spgXMwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b66f705-39da-4241-9c94-5800ad537837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual images in /content/drive/My Drive/lifesciences/post_label/images: ['dba943c5-ProtonGlow_Test_URS_1_page_14.png', '32b16166-ProtonGlow_Test_URS_1_page_34.png', 'c77134ed-ProtonGlow_Test_URS_1_page_26.png', '2124bdf9-ProtonGlow_Test_URS_1_page_5.png', 'f7c3a644-ProtonGlow_Test_URS_1_page_16.png', 'f19c46f0-ProtonGlow_Test_URS_1_page_30.png', 'fe7aa29b-ProtonGlow_Test_URS_1_page_32.png', 'c8aa0be3-ProtonGlow_Test_URS_1_page_7.png', '042f030e-ProtonGlow_Test_URS_1_page_9.png', '3f57eebd-ProtonGlow_Test_URS_1_page_19.png', '7347d622-ProtonGlow_Test_URS_1_page_20.png', '245272e6-ProtonGlow_Test_URS_1_page_28.png', '426fb557-Pharma_URS_Enhanced_page_1.png', '5e5d7ccd-ProtonGlow_Test_URS_1_page_10.png', '68b58f6d-ProtonGlow_Test_URS_1_page_15.png', 'c9df1740-ProtonGlow_Test_URS_1_page_12.png', 'eddd6c38-ProtonGlow_Test_URS_1_page_29.png', '48759adc-Pharma_URS_Enhanced_page_2.png', '48ed3ccf-ProtonGlow_Test_URS_1_page_31.png', '148a3bf9-ProtonGlow_Test_URS_1_page_2.png', 'ea5776e7-ProtonGlow_Test_URS_1_page_6.png', '776ef5f2-ProtonGlow_Test_URS_1_page_8.png', '043f5478-ProtonGlow_Test_URS_1_page_21.png', '9751783c-ProtonGlow_Test_URS_1_page_18.png', 'd39b2cf1-ProtonGlow_Test_URS_1_page_33.png', '7226722b-ProtonGlow_Test_URS_1_page_25.png', '046cbd69-ProtonGlow_Test_URS_1_page_27.png', 'a83b124d-ProtonGlow_Test_URS_1_page_13.png', '58cd0367-ProtonGlow_Test_URS_1_page_11.png', 'b20e4d9c-ProtonGlow_Test_URS_1_page_24.png', 'e5433553-ProtonGlow_Test_URS_1_page_4.png', 'ee1322e1-ProtonGlow_Test_URS_1_page_22.png', 'bb38c851-ProtonGlow_Test_URS_1_page_3.png', 'aa67a5ad-ProtonGlow_Test_URS_1_page_23.png', 'f75fe9c4-ProtonGlow_Test_URS_1_page_17.png']\n",
            "Mapped ./images/426fb557-Pharma_URS_Enhanced_page_1.png to /content/drive/My Drive/lifesciences/post_label/images/426fb557-Pharma_URS_Enhanced_page_1.png\n",
            "Mapped ./images/48759adc-Pharma_URS_Enhanced_page_2.png to /content/drive/My Drive/lifesciences/post_label/images/48759adc-Pharma_URS_Enhanced_page_2.png\n",
            "Mapped ./images/148a3bf9-ProtonGlow_Test_URS_1_page_2.png to /content/drive/My Drive/lifesciences/post_label/images/148a3bf9-ProtonGlow_Test_URS_1_page_2.png\n",
            "Mapped ./images/bb38c851-ProtonGlow_Test_URS_1_page_3.png to /content/drive/My Drive/lifesciences/post_label/images/bb38c851-ProtonGlow_Test_URS_1_page_3.png\n",
            "Mapped ./images/e5433553-ProtonGlow_Test_URS_1_page_4.png to /content/drive/My Drive/lifesciences/post_label/images/e5433553-ProtonGlow_Test_URS_1_page_4.png\n",
            "Mapped ./images/2124bdf9-ProtonGlow_Test_URS_1_page_5.png to /content/drive/My Drive/lifesciences/post_label/images/2124bdf9-ProtonGlow_Test_URS_1_page_5.png\n",
            "Mapped ./images/ea5776e7-ProtonGlow_Test_URS_1_page_6.png to /content/drive/My Drive/lifesciences/post_label/images/ea5776e7-ProtonGlow_Test_URS_1_page_6.png\n",
            "Mapped ./images/c8aa0be3-ProtonGlow_Test_URS_1_page_7.png to /content/drive/My Drive/lifesciences/post_label/images/c8aa0be3-ProtonGlow_Test_URS_1_page_7.png\n",
            "Mapped ./images/776ef5f2-ProtonGlow_Test_URS_1_page_8.png to /content/drive/My Drive/lifesciences/post_label/images/776ef5f2-ProtonGlow_Test_URS_1_page_8.png\n",
            "Mapped ./images/042f030e-ProtonGlow_Test_URS_1_page_9.png to /content/drive/My Drive/lifesciences/post_label/images/042f030e-ProtonGlow_Test_URS_1_page_9.png\n",
            "Mapped ./images/5e5d7ccd-ProtonGlow_Test_URS_1_page_10.png to /content/drive/My Drive/lifesciences/post_label/images/5e5d7ccd-ProtonGlow_Test_URS_1_page_10.png\n",
            "Mapped ./images/58cd0367-ProtonGlow_Test_URS_1_page_11.png to /content/drive/My Drive/lifesciences/post_label/images/58cd0367-ProtonGlow_Test_URS_1_page_11.png\n",
            "Mapped ./images/c9df1740-ProtonGlow_Test_URS_1_page_12.png to /content/drive/My Drive/lifesciences/post_label/images/c9df1740-ProtonGlow_Test_URS_1_page_12.png\n",
            "Mapped ./images/a83b124d-ProtonGlow_Test_URS_1_page_13.png to /content/drive/My Drive/lifesciences/post_label/images/a83b124d-ProtonGlow_Test_URS_1_page_13.png\n",
            "Mapped ./images/dba943c5-ProtonGlow_Test_URS_1_page_14.png to /content/drive/My Drive/lifesciences/post_label/images/dba943c5-ProtonGlow_Test_URS_1_page_14.png\n",
            "Mapped ./images/68b58f6d-ProtonGlow_Test_URS_1_page_15.png to /content/drive/My Drive/lifesciences/post_label/images/68b58f6d-ProtonGlow_Test_URS_1_page_15.png\n",
            "Mapped ./images/f7c3a644-ProtonGlow_Test_URS_1_page_16.png to /content/drive/My Drive/lifesciences/post_label/images/f7c3a644-ProtonGlow_Test_URS_1_page_16.png\n",
            "Mapped ./images/f75fe9c4-ProtonGlow_Test_URS_1_page_17.png to /content/drive/My Drive/lifesciences/post_label/images/f75fe9c4-ProtonGlow_Test_URS_1_page_17.png\n",
            "Mapped ./images/9751783c-ProtonGlow_Test_URS_1_page_18.png to /content/drive/My Drive/lifesciences/post_label/images/9751783c-ProtonGlow_Test_URS_1_page_18.png\n",
            "Mapped ./images/3f57eebd-ProtonGlow_Test_URS_1_page_19.png to /content/drive/My Drive/lifesciences/post_label/images/3f57eebd-ProtonGlow_Test_URS_1_page_19.png\n",
            "Mapped ./images/7347d622-ProtonGlow_Test_URS_1_page_20.png to /content/drive/My Drive/lifesciences/post_label/images/7347d622-ProtonGlow_Test_URS_1_page_20.png\n",
            "Mapped ./images/043f5478-ProtonGlow_Test_URS_1_page_21.png to /content/drive/My Drive/lifesciences/post_label/images/043f5478-ProtonGlow_Test_URS_1_page_21.png\n",
            "Mapped ./images/ee1322e1-ProtonGlow_Test_URS_1_page_22.png to /content/drive/My Drive/lifesciences/post_label/images/ee1322e1-ProtonGlow_Test_URS_1_page_22.png\n",
            "Mapped ./images/aa67a5ad-ProtonGlow_Test_URS_1_page_23.png to /content/drive/My Drive/lifesciences/post_label/images/aa67a5ad-ProtonGlow_Test_URS_1_page_23.png\n",
            "Mapped ./images/b20e4d9c-ProtonGlow_Test_URS_1_page_24.png to /content/drive/My Drive/lifesciences/post_label/images/b20e4d9c-ProtonGlow_Test_URS_1_page_24.png\n",
            "Mapped ./images/7226722b-ProtonGlow_Test_URS_1_page_25.png to /content/drive/My Drive/lifesciences/post_label/images/7226722b-ProtonGlow_Test_URS_1_page_25.png\n",
            "Mapped ./images/c77134ed-ProtonGlow_Test_URS_1_page_26.png to /content/drive/My Drive/lifesciences/post_label/images/c77134ed-ProtonGlow_Test_URS_1_page_26.png\n",
            "Mapped ./images/046cbd69-ProtonGlow_Test_URS_1_page_27.png to /content/drive/My Drive/lifesciences/post_label/images/046cbd69-ProtonGlow_Test_URS_1_page_27.png\n",
            "Mapped ./images/245272e6-ProtonGlow_Test_URS_1_page_28.png to /content/drive/My Drive/lifesciences/post_label/images/245272e6-ProtonGlow_Test_URS_1_page_28.png\n",
            "Mapped ./images/eddd6c38-ProtonGlow_Test_URS_1_page_29.png to /content/drive/My Drive/lifesciences/post_label/images/eddd6c38-ProtonGlow_Test_URS_1_page_29.png\n",
            "Mapped ./images/f19c46f0-ProtonGlow_Test_URS_1_page_30.png to /content/drive/My Drive/lifesciences/post_label/images/f19c46f0-ProtonGlow_Test_URS_1_page_30.png\n",
            "Mapped ./images/48ed3ccf-ProtonGlow_Test_URS_1_page_31.png to /content/drive/My Drive/lifesciences/post_label/images/48ed3ccf-ProtonGlow_Test_URS_1_page_31.png\n",
            "Mapped ./images/fe7aa29b-ProtonGlow_Test_URS_1_page_32.png to /content/drive/My Drive/lifesciences/post_label/images/fe7aa29b-ProtonGlow_Test_URS_1_page_32.png\n",
            "Mapped ./images/d39b2cf1-ProtonGlow_Test_URS_1_page_33.png to /content/drive/My Drive/lifesciences/post_label/images/d39b2cf1-ProtonGlow_Test_URS_1_page_33.png\n",
            "Mapped ./images/32b16166-ProtonGlow_Test_URS_1_page_34.png to /content/drive/My Drive/lifesciences/post_label/images/32b16166-ProtonGlow_Test_URS_1_page_34.png\n",
            "Updated COCO JSON saved to /content/drive/My Drive/lifesciences/retrained_staging/results_mapped.json\n",
            "COCO JSON paths updated and saved to /content/drive/My Drive/lifesciences/retrained_staging/results_mapped.json\n"
          ]
        }
      ],
      "source": [
        "# Function to update COCO JSON paths and save the result\n",
        "def update_coco_paths(json_path, image_dir):\n",
        "    updated_json_path = map_and_update_coco_paths(json_path, image_dir)\n",
        "    print(f\"COCO JSON paths updated and saved to {updated_json_path}\")\n",
        "    return updated_json_path\n",
        "\n",
        "# Execute the COCO path update\n",
        "try:\n",
        "    updated_json_path = update_coco_paths(COCO_JSON_PATH, IMG_STAGING_DIR)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to update COCO JSON paths: {e}\")\n",
        "    print(traceback.format_exc())\n",
        "    update_run_status('FAILED')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P2NOzpDXN3f"
      },
      "source": [
        "11b: Train LayoutParser Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "-H48EewNXSAF"
      },
      "outputs": [],
      "source": [
        "# Function to train the model using updated COCO JSON\n",
        "def train_model(updated_json_path, image_dir):\n",
        "    train_layoutparser_model(updated_json_path, image_dir)\n",
        "    print(f\"Model training completed using {updated_json_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gFIA_zUXUfG"
      },
      "source": [
        "11c: Load COCO Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "vYM7JKB0XW1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442f79ae-e36a-4051-8908-64ce12131b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded COCO data from /content/drive/My Drive/lifesciences/retrained_staging/results_mapped.json\n"
          ]
        }
      ],
      "source": [
        "# Function to load the updated COCO JSON data\n",
        "def load_coco_data(updated_json_path):\n",
        "    with open(updated_json_path, \"r\") as f:\n",
        "        coco_data = json.load(f)\n",
        "    print(f\"Loaded COCO data from {updated_json_path}\")\n",
        "    return coco_data\n",
        "\n",
        "# Execute the COCO data loading\n",
        "try:\n",
        "    coco_data = load_coco_data(updated_json_path)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load COCO data: {e}\")\n",
        "    #print(traceback.format_exc())\n",
        "    update_run_status('FAILED')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ivpb73WXZtU"
      },
      "source": [
        "11d: Process Images and Annotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Eagv_ZqdXcmJ"
      },
      "outputs": [],
      "source": [
        "# Function to process and annotate images\n",
        "# Step 4: Process images and annotate\n",
        "\n",
        "# In section \"11d: Process Images and Annotate\"\n",
        "def process_images(coco_data):\n",
        "    results = []\n",
        "    source_files = [f for f in os.listdir(SOURCE_DIR) if f.endswith(('.docx', '.pdf'))]\n",
        "    image_to_source_map = {}\n",
        "    block_number_counter = {}  # Track block numbers per source_file_run_id\n",
        "\n",
        "    # Build image-to-source mapping\n",
        "    for source_file in source_files:\n",
        "        source_base_name = Path(source_file).stem\n",
        "        source_file_path = os.path.join(SOURCE_DIR, source_file)\n",
        "        for image_info in coco_data[\"images\"]:\n",
        "            image_path = image_info[\"file_name\"]\n",
        "            image_base_name = Path(image_path).stem\n",
        "            if source_base_name in image_base_name:\n",
        "                image_to_source_map[image_path] = source_file_path\n",
        "\n",
        "    # Process each image\n",
        "    for image_info in coco_data[\"images\"]:\n",
        "        image_path = image_info[\"file_name\"]\n",
        "        if os.path.exists(image_path):\n",
        "            original_file_path = image_to_source_map.get(image_path)\n",
        "            if original_file_path and os.path.exists(original_file_path):\n",
        "                print(f\"Reannotating {image_path} with source file {original_file_path}\")\n",
        "                # Process layout and adjust block numbers\n",
        "                layout_results = process_layout(image_path, original_file_path)\n",
        "                if layout_results:\n",
        "                    source_file_run_id = layout_results[0][\"source_file_run_id\"]\n",
        "                    # Initialize counter for this source_file_run_id if not present\n",
        "                    if source_file_run_id not in block_number_counter:\n",
        "                        block_number_counter[source_file_run_id] = 0\n",
        "                    # Adjust block numbers to be cumulative\n",
        "                    for result in layout_results:\n",
        "                        block_number_counter[source_file_run_id] += 1\n",
        "                        result[\"block_number\"] = block_number_counter[source_file_run_id]\n",
        "                    results.extend(layout_results)\n",
        "            else:\n",
        "                print(f\"No matching source file found for {image_path}, skipping\")\n",
        "        else:\n",
        "            print(f\"Image not found: {image_path}\")\n",
        "    print(f\"process_images generated {len(results)} results\")\n",
        "    return results\n",
        "\n",
        "# No change needed in process_layout, but ensure it uses block_number as assigned\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiUHwM1XXgXE"
      },
      "source": [
        "11e: Append Results to Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XVAsL4hbXhJ4"
      },
      "outputs": [],
      "source": [
        "# Function to append results to the database\n",
        "def save_results_to_db(results):\n",
        "    append_to_db(results)\n",
        "    print(f\"Results appended to database\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye5nH_KuXlYj"
      },
      "source": [
        "11f: Update Run Status to Success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "8zwsB95RXnSv"
      },
      "outputs": [],
      "source": [
        "# Function to mark the run as successful\n",
        "def mark_run_success():\n",
        "    update_run_status('SUCCESS')\n",
        "    print(\"Run status updated to SUCCESS\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a0JfsYuWDKP"
      },
      "source": [
        "12a: GPU Status Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7gkmmpJWWf4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e9fbe5-704e-41ac-f7f6-4754c3dee988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvidia-smi not found or error: [Errno 2] No such file or directory: 'nvidia-smi'\n"
          ]
        }
      ],
      "source": [
        "# Check GPU status using nvidia-smi\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(result.stdout)\n",
        "    else:\n",
        "        print(\"nvidia-smi failed:\", result.stderr)\n",
        "except Exception as e:\n",
        "    print(f\"nvidia-smi not found or error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7efQx2mWi4o"
      },
      "source": [
        "12b: Core Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqd-x3rGWkn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a1a166-51ad-4db1-f37d-13245f16ea2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Neon PostgreSQL database\n",
            "Started new batch run: run_number=3, batch_run_id=18, status=RUNNING\n",
            "Actual images in /content/drive/My Drive/lifesciences/post_label/images: ['dba943c5-ProtonGlow_Test_URS_1_page_14.png', '32b16166-ProtonGlow_Test_URS_1_page_34.png', 'c77134ed-ProtonGlow_Test_URS_1_page_26.png', '2124bdf9-ProtonGlow_Test_URS_1_page_5.png', 'f7c3a644-ProtonGlow_Test_URS_1_page_16.png', 'f19c46f0-ProtonGlow_Test_URS_1_page_30.png', 'fe7aa29b-ProtonGlow_Test_URS_1_page_32.png', 'c8aa0be3-ProtonGlow_Test_URS_1_page_7.png', '042f030e-ProtonGlow_Test_URS_1_page_9.png', '3f57eebd-ProtonGlow_Test_URS_1_page_19.png', '7347d622-ProtonGlow_Test_URS_1_page_20.png', '245272e6-ProtonGlow_Test_URS_1_page_28.png', '426fb557-Pharma_URS_Enhanced_page_1.png', '5e5d7ccd-ProtonGlow_Test_URS_1_page_10.png', '68b58f6d-ProtonGlow_Test_URS_1_page_15.png', 'c9df1740-ProtonGlow_Test_URS_1_page_12.png', 'eddd6c38-ProtonGlow_Test_URS_1_page_29.png', '48759adc-Pharma_URS_Enhanced_page_2.png', '48ed3ccf-ProtonGlow_Test_URS_1_page_31.png', '148a3bf9-ProtonGlow_Test_URS_1_page_2.png', 'ea5776e7-ProtonGlow_Test_URS_1_page_6.png', '776ef5f2-ProtonGlow_Test_URS_1_page_8.png', '043f5478-ProtonGlow_Test_URS_1_page_21.png', '9751783c-ProtonGlow_Test_URS_1_page_18.png', 'd39b2cf1-ProtonGlow_Test_URS_1_page_33.png', '7226722b-ProtonGlow_Test_URS_1_page_25.png', '046cbd69-ProtonGlow_Test_URS_1_page_27.png', 'a83b124d-ProtonGlow_Test_URS_1_page_13.png', '58cd0367-ProtonGlow_Test_URS_1_page_11.png', 'b20e4d9c-ProtonGlow_Test_URS_1_page_24.png', 'e5433553-ProtonGlow_Test_URS_1_page_4.png', 'ee1322e1-ProtonGlow_Test_URS_1_page_22.png', 'bb38c851-ProtonGlow_Test_URS_1_page_3.png', 'aa67a5ad-ProtonGlow_Test_URS_1_page_23.png', 'f75fe9c4-ProtonGlow_Test_URS_1_page_17.png']\n",
            "Mapped ./images/426fb557-Pharma_URS_Enhanced_page_1.png to /content/drive/My Drive/lifesciences/post_label/images/426fb557-Pharma_URS_Enhanced_page_1.png\n",
            "Mapped ./images/48759adc-Pharma_URS_Enhanced_page_2.png to /content/drive/My Drive/lifesciences/post_label/images/48759adc-Pharma_URS_Enhanced_page_2.png\n",
            "Mapped ./images/148a3bf9-ProtonGlow_Test_URS_1_page_2.png to /content/drive/My Drive/lifesciences/post_label/images/148a3bf9-ProtonGlow_Test_URS_1_page_2.png\n",
            "Mapped ./images/bb38c851-ProtonGlow_Test_URS_1_page_3.png to /content/drive/My Drive/lifesciences/post_label/images/bb38c851-ProtonGlow_Test_URS_1_page_3.png\n",
            "Mapped ./images/e5433553-ProtonGlow_Test_URS_1_page_4.png to /content/drive/My Drive/lifesciences/post_label/images/e5433553-ProtonGlow_Test_URS_1_page_4.png\n",
            "Mapped ./images/2124bdf9-ProtonGlow_Test_URS_1_page_5.png to /content/drive/My Drive/lifesciences/post_label/images/2124bdf9-ProtonGlow_Test_URS_1_page_5.png\n",
            "Mapped ./images/ea5776e7-ProtonGlow_Test_URS_1_page_6.png to /content/drive/My Drive/lifesciences/post_label/images/ea5776e7-ProtonGlow_Test_URS_1_page_6.png\n",
            "Mapped ./images/c8aa0be3-ProtonGlow_Test_URS_1_page_7.png to /content/drive/My Drive/lifesciences/post_label/images/c8aa0be3-ProtonGlow_Test_URS_1_page_7.png\n",
            "Mapped ./images/776ef5f2-ProtonGlow_Test_URS_1_page_8.png to /content/drive/My Drive/lifesciences/post_label/images/776ef5f2-ProtonGlow_Test_URS_1_page_8.png\n",
            "Mapped ./images/042f030e-ProtonGlow_Test_URS_1_page_9.png to /content/drive/My Drive/lifesciences/post_label/images/042f030e-ProtonGlow_Test_URS_1_page_9.png\n",
            "Mapped ./images/5e5d7ccd-ProtonGlow_Test_URS_1_page_10.png to /content/drive/My Drive/lifesciences/post_label/images/5e5d7ccd-ProtonGlow_Test_URS_1_page_10.png\n",
            "Mapped ./images/58cd0367-ProtonGlow_Test_URS_1_page_11.png to /content/drive/My Drive/lifesciences/post_label/images/58cd0367-ProtonGlow_Test_URS_1_page_11.png\n",
            "Mapped ./images/c9df1740-ProtonGlow_Test_URS_1_page_12.png to /content/drive/My Drive/lifesciences/post_label/images/c9df1740-ProtonGlow_Test_URS_1_page_12.png\n",
            "Mapped ./images/a83b124d-ProtonGlow_Test_URS_1_page_13.png to /content/drive/My Drive/lifesciences/post_label/images/a83b124d-ProtonGlow_Test_URS_1_page_13.png\n",
            "Mapped ./images/dba943c5-ProtonGlow_Test_URS_1_page_14.png to /content/drive/My Drive/lifesciences/post_label/images/dba943c5-ProtonGlow_Test_URS_1_page_14.png\n",
            "Mapped ./images/68b58f6d-ProtonGlow_Test_URS_1_page_15.png to /content/drive/My Drive/lifesciences/post_label/images/68b58f6d-ProtonGlow_Test_URS_1_page_15.png\n",
            "Mapped ./images/f7c3a644-ProtonGlow_Test_URS_1_page_16.png to /content/drive/My Drive/lifesciences/post_label/images/f7c3a644-ProtonGlow_Test_URS_1_page_16.png\n",
            "Mapped ./images/f75fe9c4-ProtonGlow_Test_URS_1_page_17.png to /content/drive/My Drive/lifesciences/post_label/images/f75fe9c4-ProtonGlow_Test_URS_1_page_17.png\n",
            "Mapped ./images/9751783c-ProtonGlow_Test_URS_1_page_18.png to /content/drive/My Drive/lifesciences/post_label/images/9751783c-ProtonGlow_Test_URS_1_page_18.png\n",
            "Mapped ./images/3f57eebd-ProtonGlow_Test_URS_1_page_19.png to /content/drive/My Drive/lifesciences/post_label/images/3f57eebd-ProtonGlow_Test_URS_1_page_19.png\n",
            "Mapped ./images/7347d622-ProtonGlow_Test_URS_1_page_20.png to /content/drive/My Drive/lifesciences/post_label/images/7347d622-ProtonGlow_Test_URS_1_page_20.png\n",
            "Mapped ./images/043f5478-ProtonGlow_Test_URS_1_page_21.png to /content/drive/My Drive/lifesciences/post_label/images/043f5478-ProtonGlow_Test_URS_1_page_21.png\n",
            "Mapped ./images/ee1322e1-ProtonGlow_Test_URS_1_page_22.png to /content/drive/My Drive/lifesciences/post_label/images/ee1322e1-ProtonGlow_Test_URS_1_page_22.png\n",
            "Mapped ./images/aa67a5ad-ProtonGlow_Test_URS_1_page_23.png to /content/drive/My Drive/lifesciences/post_label/images/aa67a5ad-ProtonGlow_Test_URS_1_page_23.png\n",
            "Mapped ./images/b20e4d9c-ProtonGlow_Test_URS_1_page_24.png to /content/drive/My Drive/lifesciences/post_label/images/b20e4d9c-ProtonGlow_Test_URS_1_page_24.png\n",
            "Mapped ./images/7226722b-ProtonGlow_Test_URS_1_page_25.png to /content/drive/My Drive/lifesciences/post_label/images/7226722b-ProtonGlow_Test_URS_1_page_25.png\n",
            "Mapped ./images/c77134ed-ProtonGlow_Test_URS_1_page_26.png to /content/drive/My Drive/lifesciences/post_label/images/c77134ed-ProtonGlow_Test_URS_1_page_26.png\n",
            "Mapped ./images/046cbd69-ProtonGlow_Test_URS_1_page_27.png to /content/drive/My Drive/lifesciences/post_label/images/046cbd69-ProtonGlow_Test_URS_1_page_27.png\n",
            "Mapped ./images/245272e6-ProtonGlow_Test_URS_1_page_28.png to /content/drive/My Drive/lifesciences/post_label/images/245272e6-ProtonGlow_Test_URS_1_page_28.png\n",
            "Mapped ./images/eddd6c38-ProtonGlow_Test_URS_1_page_29.png to /content/drive/My Drive/lifesciences/post_label/images/eddd6c38-ProtonGlow_Test_URS_1_page_29.png\n",
            "Mapped ./images/f19c46f0-ProtonGlow_Test_URS_1_page_30.png to /content/drive/My Drive/lifesciences/post_label/images/f19c46f0-ProtonGlow_Test_URS_1_page_30.png\n",
            "Mapped ./images/48ed3ccf-ProtonGlow_Test_URS_1_page_31.png to /content/drive/My Drive/lifesciences/post_label/images/48ed3ccf-ProtonGlow_Test_URS_1_page_31.png\n",
            "Mapped ./images/fe7aa29b-ProtonGlow_Test_URS_1_page_32.png to /content/drive/My Drive/lifesciences/post_label/images/fe7aa29b-ProtonGlow_Test_URS_1_page_32.png\n",
            "Mapped ./images/d39b2cf1-ProtonGlow_Test_URS_1_page_33.png to /content/drive/My Drive/lifesciences/post_label/images/d39b2cf1-ProtonGlow_Test_URS_1_page_33.png\n",
            "Mapped ./images/32b16166-ProtonGlow_Test_URS_1_page_34.png to /content/drive/My Drive/lifesciences/post_label/images/32b16166-ProtonGlow_Test_URS_1_page_34.png\n",
            "Updated COCO JSON saved to /content/drive/My Drive/lifesciences/retrained_staging/results_mapped.json\n",
            "COCO JSON paths updated and saved to /content/drive/My Drive/lifesciences/retrained_staging/results_mapped.json\n",
            "Loaded COCO data from /content/drive/My Drive/lifesciences/retrained_staging/results_mapped.json\n",
            "Reannotating /content/drive/My Drive/lifesciences/post_label/images/426fb557-Pharma_URS_Enhanced_page_1.png with source file /content/drive/My Drive/lifesciences/training_documents/Pharma_URS_Enhanced.docx\n",
            "OCR agent loaded\n",
            "Loaded retrained model from /content/drive/My Drive/lifesciences/retrained_staging/models/model_final.pth\n",
            "Processed source file metadata for /content/drive/My Drive/lifesciences/training_documents/Pharma_URS_Enhanced.docx: source_file_run_id = 311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping title inside list at coordinates: [354, 298, 2160, 572]\n",
            "Skipping title inside table at coordinates: [391, 954, 2145, 1062]\n",
            "Skipping title inside table at coordinates: [407, 1554, 1182, 1776]\n",
            "Skipping title inside list at coordinates: [418, 2490, 935, 2705]\n",
            "Inserted 4 skipped blocks into skipped_block_items\n",
            "Reannotating /content/drive/My Drive/lifesciences/post_label/images/48759adc-Pharma_URS_Enhanced_page_2.png with source file /content/drive/My Drive/lifesciences/training_documents/Pharma_URS_Enhanced.docx\n",
            "Processed source file metadata for /content/drive/My Drive/lifesciences/training_documents/Pharma_URS_Enhanced.docx: source_file_run_id = 311\n",
            "Skipping title inside table at coordinates: [307, 280, 2108, 562]\n",
            "Skipping title inside table at coordinates: [410, 1108, 1005, 1312]\n",
            "Skipping title inside table at coordinates: [311, 1670, 2237, 1903]\n",
            "Skipping text inside table at coordinates: [331, 2415, 2187, 2663]\n",
            "Inserted 4 skipped blocks into skipped_block_items\n",
            "Reannotating /content/drive/My Drive/lifesciences/post_label/images/148a3bf9-ProtonGlow_Test_URS_1_page_2.png with source file /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx\n",
            "Processed source file metadata for /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx: source_file_run_id = 313\n",
            "Skipping title inside list at coordinates: [433, 3143, 2206, 3212]\n",
            "Inserted 1 skipped blocks into skipped_block_items\n",
            "Reannotating /content/drive/My Drive/lifesciences/post_label/images/bb38c851-ProtonGlow_Test_URS_1_page_3.png with source file /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx\n",
            "Processed source file metadata for /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx: source_file_run_id = 313\n",
            "Reannotating /content/drive/My Drive/lifesciences/post_label/images/e5433553-ProtonGlow_Test_URS_1_page_4.png with source file /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx\n",
            "Processed source file metadata for /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx: source_file_run_id = 313\n",
            "Reannotating /content/drive/My Drive/lifesciences/post_label/images/2124bdf9-ProtonGlow_Test_URS_1_page_5.png with source file /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx\n",
            "Processed source file metadata for /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx: source_file_run_id = 313\n",
            "Reannotating /content/drive/My Drive/lifesciences/post_label/images/ea5776e7-ProtonGlow_Test_URS_1_page_6.png with source file /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx\n",
            "Processed source file metadata for /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx: source_file_run_id = 313\n",
            "Reannotating /content/drive/My Drive/lifesciences/post_label/images/c8aa0be3-ProtonGlow_Test_URS_1_page_7.png with source file /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx\n",
            "Processed source file metadata for /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx: source_file_run_id = 313\n",
            "Reannotating /content/drive/My Drive/lifesciences/post_label/images/776ef5f2-ProtonGlow_Test_URS_1_page_8.png with source file /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx\n",
            "Processed source file metadata for /content/drive/My Drive/lifesciences/training_documents/ProtonGlow_Test_URS.docx: source_file_run_id = 313\n"
          ]
        }
      ],
      "source": [
        "\"\"\"12b: Core Processing\"\"\"\n",
        "\n",
        "\"\"\"12b: Core Processing\"\"\"\n",
        "\n",
        "try:\n",
        "    # Step 0: Ensure database connection is set up (sets status to RUNNING)\n",
        "    setup_db_connection()\n",
        "\n",
        "    # Step 1: Update COCO JSON paths\n",
        "    updated_json_path = update_coco_paths(COCO_JSON_PATH, IMG_STAGING_DIR)\n",
        "\n",
        "    # Step 2: Train the model\n",
        "    # train_model(updated_json_path, IMG_STAGING_DIR)\n",
        "\n",
        "    # Step 3: Load COCO data\n",
        "    coco_data = load_coco_data(updated_json_path)\n",
        "\n",
        "    # Step 4: Process images and annotate\n",
        "    results = process_images(coco_data)\n",
        "\n",
        "    print(results)\n",
        "    # Step 5: Append results to database\n",
        "    save_results_to_db(results)\n",
        "\n",
        "    # Step 6: Mark run as successful\n",
        "    mark_run_success()  # Sets status to SUCCESS\n",
        "except Exception as e:\n",
        "    print(f\"Processing failed with unexpected error: {e}\")\n",
        "    print(traceback.format_exc())\n",
        "    update_run_status('FAILED')  # Changed from 'FAILURE' to 'FAILED'\n",
        "    raise"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}